[
    {
        "title": "Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo",
        "authorids": [
            "~Stephen_Zhao1",
            "~Rob_Brekelmans1",
            "~Alireza_Makhzani1",
            "~Roger_Baker_Grosse1"
        ],
        "abstract": "Numerous capability and safety techniques of Large Language Models (LLMs), including RLHF, automated red-teaming, prompt engineering, and infilling, can be cast as sampling from an unnormalized target distribution defined by a given reward or potential function over the full sequence. In this work, we leverage the rich toolkit of Sequential Monte Carlo (SMC) for these probabilistic inference problems. In particular, we use learned twist functions to estimate the expected future value of the potential at each timestep, which enables us to focus inference-time computation on promising partial sequences. We propose a novel contrastive method for learning the twist functions, and establish connections with the rich literature of soft reinforcement learning. As a complementary application of our twisted SMC framework, we present methods for evaluating the accuracy of language model inference techniques using novel bidirectional SMC bounds on the log partition function. These bounds can be used to estimate the KL divergence between the inference and target distributions in both directions. We apply our inference evaluation techniques to show that twisted SMC is effective for sampling undesirable outputs from a pretrained model (a useful component of harmlessness training and automated red-teaming), generating reviews with varied sentiment, and performing infilling tasks.",
        "_bibtex": "@inproceedings{\nzhao2024probabilistic,\ntitle={Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo},\nauthor={Stephen Zhao and Rob Brekelmans and Alireza Makhzani and Roger Baker Grosse},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=frA0NNBS1n}\n}"
    },
    {
        "title": "Position: Open-Endedness is Essential for Artificial Superhuman Intelligence",
        "authorids": [
            "~Edward_Hughes1",
            "~Michael_D_Dennis1",
            "~Jack_Parker-Holder1",
            "~Feryal_Behbahani1",
            "~Aditi_Mavalankar1",
            "~Yuge_Shi2",
            "~Tom_Schaul2",
            "~Tim_Rockt\u00e4schel1"
        ],
        "abstract": "In recent years there has been a tremendous surge in the general capabilities of AI systems, mainly fuelled by training foundation models on internet-scale data. Nevertheless, the creation of open-ended, ever self-improving AI remains elusive. **In this position paper, we argue that the ingredients are now in place to achieve *open-endedness* in AI systems with respect to a human observer. Furthermore, we claim that such open-endedness is an essential property of any artificial superhuman intelligence (ASI).** We begin by providing a concrete formal definition of open-endedness through the lens of novelty and learnability. We then illustrate a path towards ASI via open-ended systems built on top of foundation models, capable of making novel, human-relevant discoveries. We conclude by examining the safety implications of generally-capable open-ended AI. We expect that open-ended foundation models will prove to be an increasingly fertile and safety-critical area of research in the near future.",
        "_bibtex": "@inproceedings{\nhughes2024position,\ntitle={Position: Open-Endedness is Essential for Artificial Superhuman Intelligence},\nauthor={Edward Hughes and Michael D Dennis and Jack Parker-Holder and Feryal Behbahani and Aditi Mavalankar and Yuge Shi and Tom Schaul and Tim Rockt{\\\"a}schel},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=Bc4vZ2CX7E}\n}"
    },
    {
        "title": "Stop Regressing: Training Value Functions via Classification for Scalable Deep RL",
        "authorids": [
            "~Jesse_Farebrother1",
            "~Jordi_Orbay1",
            "~Quan_Vuong2",
            "~Adrien_Ali_Taiga1",
            "~Yevgen_Chebotar1",
            "~Ted_Xiao1",
            "~Alex_Irpan1",
            "~Sergey_Levine1",
            "~Pablo_Samuel_Castro1",
            "~Aleksandra_Faust1",
            "~Aviral_Kumar2",
            "~Rishabh_Agarwal2"
        ],
        "abstract": "Value functions are an essential component in deep reinforcement learning (RL), that are typically trained via mean squared error regression to match bootstrapped target values. However, scaling value-based RL methods to large networks has proven challenging. This difficulty is in stark contrast to supervised learning: by leveraging a cross-entropy classification loss, supervised methods have scaled reliably to massive networks. Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions. We show that training value functions with categorical cross-entropy significantly enhances performance and scalability across various domains, including single-task RL on Atari 2600 games, multi-task RL on Atari with large-scale ResNets, robotic manipulation with Q-transformers, playing Chess without search, and a language-agent Wordle task with high-capacity Transformers, achieving *state-of-the-art results* on these domains. Through careful analysis, we show that categorical cross-entropy mitigates issues inherent to value-based RL, such as noisy targets and non-stationarity. We argue that shifting to categorical cross-entropy for training value functions can substantially improve the scalability of deep RL at little-to-no cost.",
        "_bibtex": "@inproceedings{\nfarebrother2024stop,\ntitle={Stop Regressing: Training Value Functions via Classification for Scalable Deep {RL}},\nauthor={Jesse Farebrother and Jordi Orbay and Quan Vuong and Adrien Ali Taiga and Yevgen Chebotar and Ted Xiao and Alex Irpan and Sergey Levine and Pablo Samuel Castro and Aleksandra Faust and Aviral Kumar and Rishabh Agarwal},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=dVpFKfqF3R}\n}"
    },
    {
        "title": "Improving Transformers with Dynamically Composable Multi-Head Attention",
        "authorids": [
            "~Da_Xiao1",
            "~Qingye_Meng1",
            "~Shengping_Li2",
            "~xingyuan_yuan1"
        ],
        "abstract": "Multi-Head Attention (MHA) is a key component of Transformer. In MHA, attention heads work independently, causing problems such as low-rank bottleneck of attention score matrices and head redundancy. We propose Dynamically Composable Multi-Head Attention (DCMHA), a parameter and computation efficient attention architecture that tackles the shortcomings of MHA and increases the expressive power of the model by dynamically composing attention heads. At the core of DCMHA is a Compose function that transforms the attention score and weight matrices in an input-dependent way. DCMHA can be used as a drop-in replacement of MHA in any transformer architecture to obtain the corresponding DCFormer. DCFormer significantly outperforms Transformer on different architectures and model scales in language modeling, matching the performance of models with 1.7x-2.0x compute. For example, DCPythia-6.9B outperforms open source Pythia-12B on both pretraining perplexity and downstream task evaluation.",
        "_bibtex": "@inproceedings{\nxiao2024improving,\ntitle={Improving Transformers with Dynamically Composable Multi-Head Attention},\nauthor={Da Xiao and Qingye Meng and Shengping Li and xingyuan yuan},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=RbiBKPtuHp}\n}"
    },
    {
        "title": "Learning Useful Representations of Recurrent Neural Network Weight Matrices",
        "authorids": [
            "~Vincent_Herrmann1",
            "~Francesco_Faccio1",
            "~J\u00fcrgen_Schmidhuber1"
        ],
        "abstract": "Recurrent Neural Networks (RNNs) are general-purpose parallel-sequential computers. The program of an RNN is its weight matrix. How to learn useful representations of RNN weights that facilitate RNN analysis as well as downstream tasks? While the _mechanistic approach_ directly looks at some RNN's weights to predict its behavior, the _functionalist approach_ analyzes its overall functionality\u2013specifically, its input-output mapping. We consider several mechanistic approaches for RNN weights and adapt the permutation equivariant Deep Weight Space layer for RNNs. Our two novel functionalist approaches extract information from RNN weights by 'interrogating' the RNN through probing inputs. We develop a theoretical framework that demonstrates conditions under which the functionalist approach can generate rich representations that help determine RNN behavior. We create and release the first two 'model zoo' datasets for RNN weight representation learning. One consists of generative models of a class of formal languages, and the other one of classifiers of sequentially processed MNIST digits. With the help of an emulation-based self-supervised learning technique we compare and evaluate the different RNN weight encoding techniques on multiple downstream applications. On the most challenging one, namely predicting which exact task the RNN was trained on, functionalist approaches show clear superiority.",
        "_bibtex": "@inproceedings{\nherrmann2024learning,\ntitle={Learning Useful  Representations of Recurrent Neural Network Weight Matrices},\nauthor={Vincent Herrmann and Francesco Faccio and J{\\\"u}rgen Schmidhuber},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=QBj7Uurdwf}\n}"
    },
    {
        "title": "Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model",
        "authorids": [
            "~Fei_Liu14",
            "~Tong_Xialiang2",
            "~Mingxuan_Yuan1",
            "~Xi_Lin2",
            "~Fu_Luo1",
            "~Zhenkun_Wang1",
            "~Zhichao_Lu1",
            "~Qingfu_Zhang1"
        ],
        "abstract": "Heuristics are widely used for dealing with complex search and optimization problems. However, manual design of heuristics can be often very labour extensive and requires rich working experience and knowledge. This paper proposes Evolution of Heuristic (EoH), a novel evolutionary paradigm that leverages both Large Language Models (LLMs) and Evolutionary Computation (EC) methods for Automatic Heuristic Design (AHD). EoH represents the ideas of heuristics in natural language, termed thoughts. They are then translated into executable codes by LLMs. The evolution of both thoughts and codes in an evolutionary search framework makes it very effective and efficient for generating high-performance heuristics. Experiments on three widely studied combinatorial optimization benchmark problems demonstrate that EoH outperforms commonly used handcrafted heuristics and other recent AHD methods including FunSearch. Particularly, the heuristic produced by EoH with a low computational budget (in terms of the number of queries to LLMs) significantly outperforms widely-used human hand-crafted baseline algorithms for the online bin packing problem.",
        "_bibtex": "@inproceedings{\nliu2024evolution,\ntitle={Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model},\nauthor={Fei Liu and Tong Xialiang and Mingxuan Yuan and Xi Lin and Fu Luo and Zhenkun Wang and Zhichao Lu and Qingfu Zhang},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=BwAkaxqiLB}\n}"
    },
    {
        "title": "SceneCraft: An LLM Agent for Synthesizing 3D Scenes as Blender Code",
        "authorids": [
            "~Ziniu_Hu1",
            "~Ahmet_Iscen3",
            "~Aashi_Jain1",
            "~Thomas_Kipf2",
            "~Yisong_Yue1",
            "~David_A_Ross1",
            "~Cordelia_Schmid1",
            "~Alireza_Fathi1"
        ],
        "abstract": "This paper introduces SceneCraft, a Large Language Model (LLM) Agent converting text descriptions into Blender-executable Python scripts which render complex scenes with up to a hundred 3D assets. This process requires complex spatial planning and arrangement. We tackle these challenges through a combination of advanced abstraction, strategic planning, and library learning. SceneCraft first models a scene graph as a blueprint, detailing the spatial relationships among assets in the scene. SceneCraft then writes Python scripts based on this graph, translating relationships into numerical constraints for asset layout. Next, SceneCraft leverages the perceptual strengths of vision-language foundation models like GPT-V to analyze rendered images and iteratively refine the scene. On top of this process, SceneCraft features a library learning mechanism that compiles common script functions into a reusable library, facilitating continuous self-improvement without expensive LLM parameter tuning. Our evaluation demonstrates that SceneCraft surpasses existing LLM-based agents in rendering complex scenes, as shown by its adherence to constraints and favorable human assessments. We also showcase the broader application potential of SceneCraft by reconstructing detailed 3D scenes from the Sintel movie and guiding a video generative model with generated scenes as intermediary control signal.",
        "_bibtex": "@inproceedings{\nhu2024scenecraft,\ntitle={SceneCraft: An {LLM} Agent for Synthesizing 3D Scenes as Blender Code},\nauthor={Ziniu Hu and Ahmet Iscen and Aashi Jain and Thomas Kipf and Yisong Yue and David A Ross and Cordelia Schmid and Alireza Fathi},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=gAyzjHw2ml}\n}"
    },
    {
        "title": "Doubly Robust Causal Effect Estimation under Networked Interference via Targeted Learning",
        "authorids": [
            "~Weilin_Chen1",
            "~Ruichu_Cai1",
            "~Zeqin_Yang1",
            "~Jie_Qiao1",
            "~Yuguang_Yan1",
            "~Zijian_Li1",
            "~Zhifeng_Hao5"
        ],
        "abstract": "Causal effect estimation under networked interference is an important but challenging problem. Available parametric methods are limited in their model space, while previous semiparametric methods, e.g., leveraging neural networks to fit only one single nuisance function, may still encounter misspecification problems under networked interference without appropriate assumptions on the data generation process. To mitigate bias stemming from misspecification, we propose a novel doubly robust causal effect estimator under networked interference, by adapting the targeted learning technique to the training of neural networks. Specifically, we generalize the targeted learning technique into the networked interference setting and establish the condition under which an estimator achieves double robustness. Based on the condition, we devise an end-to-end causal effect estimator by transforming the identified theoretical condition into a targeted loss. Moreover, we provide a theoretical analysis of our designed estimator, revealing a faster convergence rate compared to a single nuisance model. Extensive experimental results on two real-world networks with semisynthetic data demonstrate the effectiveness of our proposed estimators.",
        "_bibtex": "@inproceedings{\nchen2024doubly,\ntitle={Doubly Robust Causal Effect Estimation under Networked Interference via Targeted Learning},\nauthor={Weilin Chen and Ruichu Cai and Zeqin Yang and Jie Qiao and Yuguang Yan and Zijian Li and Zhifeng Hao},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=5lI9wm4dws}\n}"
    },
    {
        "title": "Emergent Equivariance in Deep Ensembles",
        "authorids": [
            "~Jan_E_Gerken1",
            "~Pan_Kessel1"
        ],
        "abstract": "We show that deep ensembles become equivariant for all inputs and at all training times by simply using data augmentation. Crucially, equivariance holds off-manifold and for any architecture in the infinite width limit. The equivariance is emergent in the sense that predictions of individual ensemble members are not equivariant but their collective prediction is. Neural tangent kernel theory is used to derive this result and we verify our theoretical insights using detailed numerical experiments.",
        "_bibtex": "@inproceedings{\ngerken2024emergent,\ntitle={Emergent Equivariance in Deep Ensembles},\nauthor={Jan E Gerken and Pan Kessel},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=plXXbXjvQ9}\n}"
    },
    {
        "title": "Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks",
        "authorids": [
            "~Linyuan_Gong1",
            "~Sida_Wang2",
            "~Mostafa_Elhoushi1",
            "~Alvin_Cheung2"
        ],
        "abstract": "We introduce **S**yntax-**A**ware **F**ill-**i**n-the-**M**iddle (SAFIM), a new benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across LLMs. Our comprehensive evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future research in effective pretraining strategies for code LLMs. The evaluation toolkit and dataset are available at https://github.com/gonglinyuan/safim, and the leaderboard is available at https://safimbenchmark.com.",
        "_bibtex": "@inproceedings{\ngong2024evaluation,\ntitle={Evaluation of {LLM}s on Syntax-Aware Code Fill-in-the-Middle Tasks},\nauthor={Linyuan Gong and Sida Wang and Mostafa Elhoushi and Alvin Cheung},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=jKYyFbH8ap}\n}"
    },
    {
        "title": "Position: Automatic Environment Shaping is the Next Frontier in RL",
        "authorids": [
            "~Younghyo_Park1",
            "~Gabriel_B._Margolis1",
            "~Pulkit_Agrawal1"
        ],
        "abstract": "Many roboticists dream of presenting a robot with a task in the evening and returning the next morning to find the robot capable of solving the task. What is preventing us from achieving this? Sim-to-real reinforcement learning (RL) has achieved impressive performance on challenging robotics tasks, but requires substantial human effort to set up the task in a way that is amenable to RL. It's our position that algorithmic improvements in policy optimization and other ideas should be guided towards resolving the primary bottleneck of shaping the training environment, i.e., designing observations, actions, rewards and simulation dynamics. Most practitioners don't tune the RL algorithm, but other environment parameters to obtain a desirable controller. We posit that scaling RL to diverse robotic tasks will only be achieved if the community focuses on automating environment shaping procedures.",
        "_bibtex": "@inproceedings{\npark2024position,\ntitle={Position: Automatic Environment Shaping is the Next Frontier in {RL}},\nauthor={Younghyo Park and Gabriel B. Margolis and Pulkit Agrawal},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=dslUyy1rN4}\n}"
    },
    {
        "title": "Online Matching with Stochastic Rewards: Provable Better Bound via Adversarial Reinforcement Learning",
        "authorids": [
            "~Qiankun_Zhang1",
            "~Aocheng_Shen1",
            "~Boyu_Zhang6",
            "~Hanrui_Jiang1",
            "~Bingqian_Du1"
        ],
        "abstract": "For a specific online optimization problem, for example, online bipartite matching (OBM), research efforts could be made in two directions before it is finally closed, i.e., the optimal competitive online algorithm is found. One is to continuously design algorithms with better performance. To this end, reinforcement learning (RL) has demonstrated great success in literature. However, little is known on the other direction: whether RL helps explore how hard an online problem is. In this paper, we study a generalized model of OBM, named online matching with stochastic rewards (OMSR, FOCS 2012), for which the optimal competitive ratio is still unknown. We adopt an adversarial RL approach that trains two RL agents adversarially and iteratively: the algorithm agent learns for algorithms with larger competitive ratios, while the adversarial agent learns to produce a family of hard instances. Through such a framework, agents converge at the end with a robust algorithm, which empirically outperforms the state of the art (STOC 2020). Much more significantly, it allows to track how the hard instances are generated. We succeed in distilling two structural properties from the learned graph patterns, which remarkably reduce the action space, and further enable theoretical improvement on the best-known hardness result of OMSR, from $0.621$ (FOCS 2012) to $0.597$. To the best of our knowledge, this gives the first evidence that RL can help enhance the theoretical understanding of an online problem.",
        "_bibtex": "@inproceedings{\nzhang2024online,\ntitle={Online Matching with Stochastic Rewards: Provable Better Bound via Adversarial Reinforcement Learning},\nauthor={Qiankun Zhang and Aocheng Shen and Boyu Zhang and Hanrui Jiang and Bingqian Du},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=TujtZgdRxB}\n}"
    },
    {
        "title": "Position: Beyond Personhood: Agency, Accountability, and the Limits of Anthropomorphic Ethical Analysis",
        "authorids": [
            "~Jessica_Dai1"
        ],
        "abstract": "What is *agency,* and why does it matter? In this work, we draw from the political science and philosophy literature and give two competing visions of what it means to be an (ethical) agent. The first view, which we term *mechanistic*, is commonly\u2014 and implicitly\u2014assumed in AI research, yet it is a fundamentally limited means to understand the ethical characteristics of AI. Under the second view, which we term volitional, AI can no longer be considered an ethical agent. We discuss the implications of each of these views for two critical questions: first, what the ideal system \u201cought\u201d to look like, and second, how accountability may be achieved. In light of this discussion, we ultimately argue that, in the context of ethically-significant behavior, AI should be viewed not as an agent but as the outcome of political processes.",
        "_bibtex": "@inproceedings{\ndai2024position,\ntitle={Position: Beyond Personhood: Agency, Accountability, and the Limits of Anthropomorphic Ethical Analysis},\nauthor={Jessica Dai},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=4XlGXIh2BB}\n}"
    },
    {
        "title": "Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape",
        "authorids": [
            "~Juno_Kim1",
            "~Taiji_Suzuki1"
        ],
        "abstract": "Large language models based on the Transformer architecture have demonstrated impressive capabilities to learn in context. However, existing theoretical studies on how this phenomenon arises are limited to the dynamics of a single layer of attention trained on linear regression tasks. In this paper, we study the optimization of a Transformer consisting of a fully connected layer followed by a linear attention layer. The MLP acts as a common nonlinear representation or feature map, greatly enhancing the power of in-context learning. We prove in the mean-field and two-timescale limit that the infinite-dimensional loss landscape for the distribution of parameters, while highly nonconvex, becomes quite benign. We also analyze the second-order stability of mean-field dynamics and show that Wasserstein gradient flow almost always avoids saddle points. Furthermore, we establish novel methods for obtaining concrete improvement rates both away from and near critical points. This represents the first saddle point analysis of mean-field dynamics in general and the techniques are of independent interest.",
        "_bibtex": "@inproceedings{\nkim2024transformers,\ntitle={Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape},\nauthor={Juno Kim and Taiji Suzuki},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=xm2lU7tteQ}\n}"
    },
    {
        "title": "Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews",
        "authorids": [
            "~Weixin_Liang1",
            "~Zachary_Izzo1",
            "~Yaohui_Zhang2",
            "~Haley_Lepp1",
            "~Hancheng_Cao1",
            "~Xuandong_Zhao1",
            "~Lingjiao_Chen1",
            "~Haotian_Ye1",
            "~Sheng_Liu2",
            "zhihuang@stanford.edu",
            "~Daniel_McFarland1",
            "~James_Y._Zou1"
        ],
        "abstract": "We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: *ICLR* 2024, *NeurIPS* 2023, *CoRL* 2023 and *EMNLP* 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from reviewers who are less likely to respond to author rebuttals. We also observe corpus-level trends in generated text which may be too subtle to detect at the individual level, and discuss the implications of such trends on peer review. We call for future interdisciplinary work to examine how LLM use is changing our information and knowledge practices.",
        "_bibtex": "@inproceedings{\nliang2024monitoring,\ntitle={Monitoring {AI}-Modified Content at Scale: A Case Study on the Impact of Chat{GPT} on {AI} Conference Peer Reviews},\nauthor={Weixin Liang and Zachary Izzo and Yaohui Zhang and Haley Lepp and Hancheng Cao and Xuandong Zhao and Lingjiao Chen and Haotian Ye and Sheng Liu and Zhi Huang and Daniel McFarland and James Y. Zou},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=bX3J7ho18S}\n}"
    },
    {
        "title": "Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics",
        "authorids": [
            "~Siqi_Miao1",
            "~Zhiyuan_Lu1",
            "~Mia_Liu1",
            "~Javier_Duarte1",
            "~Pan_Li2"
        ],
        "abstract": "This study introduces a novel transformer model optimized for large-scale point cloud processing in scientific domains such as high-energy physics (HEP) and astrophysics. Addressing the limitations of graph neural networks and standard transformers, our model integrates local inductive bias and achieves near-linear complexity with hardware-friendly regular operations. One contribution of this work is the quantitative analysis of the error-complexity tradeoff of various sparsification techniques for building efficient transformers. Our findings highlight the superiority of using locality-sensitive hashing (LSH), especially OR & AND-construction LSH, in kernel approximation for large-scale point cloud data with local inductive bias. Based on this finding, we propose LSH-based Efficient Point Transformer (**HEPT**), which combines E$^2$LSH with OR & AND constructions and is built upon regular computations. HEPT demonstrates remarkable performance on two critical yet time-consuming HEP tasks, significantly outperforming existing GNNs and transformers in accuracy and computational speed, marking a significant advancement in geometric deep learning and large-scale scientific data processing. Our code is available at https://github.com/Graph-COM/HEPT.",
        "_bibtex": "@inproceedings{\nmiao2024localitysensitive,\ntitle={Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics},\nauthor={Siqi Miao and Zhiyuan Lu and Mia Liu and Javier Duarte and Pan Li},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=vJx6fld6l0}\n}"
    },
    {
        "title": "Unified Training of Universal Time Series Forecasting Transformers",
        "authorids": [
            "~Gerald_Woo1",
            "~Chenghao_Liu1",
            "~Akshat_Kumar2",
            "~Caiming_Xiong1",
            "~Silvio_Savarese1",
            "~Doyen_Sahoo1"
        ],
        "abstract": "Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of *universal forecasting*, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: (i) cross-frequency learning, (ii) accommodating an arbitrary number of variates for multivariate time series, and (iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed **M**asked Enc**o**der-based Un**i**ve**r**s**a**l T**i**me Series Forecasting Transformer (**Moirai**). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) featuring over 27B observations across nine domains, Moirai achieves competitive or superior performance as a zero-shot forecaster when compared to full-shot models. Code, data, and model weights can be found at https://github.com/SalesforceAIResearch/uni2ts.",
        "_bibtex": "@inproceedings{\nwoo2024unified,\ntitle={Unified Training of Universal Time Series Forecasting Transformers},\nauthor={Gerald Woo and Chenghao Liu and Akshat Kumar and Caiming Xiong and Silvio Savarese and Doyen Sahoo},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=Yd8eHMY1wz}\n}"
    },
    {
        "title": "Position: Opportunities Exist for Machine Learning in Magnetic Fusion Energy",
        "authorids": [
            "~Lucas_Spangher1",
            "~Allen_M._Wang1",
            "maris@psfc.mit.edu",
            "myless@psfc.mit.edu",
            "~Viraj_Mehta1",
            "saperstein@psfc.mit.edu",
            "slwalsh@psfc.mit.edu",
            "~Akshata_Kishore_Moharir2",
            "alessandro.pau@epfl.ch",
            "crea@psfc.mit.edu"
        ],
        "abstract": "Magnetic confinement fusion may one day provide reliable, carbon-free energy, but the field currently faces technical hurdles. In this position paper, we highlight six key research challenges in the field of fusion energy that we believe should be research priorities for the Machine Learning (ML) community because they are especially ripe for ML applications: (1) disruption prediction, (2) simulation and dynamics modeling (3) resolving partially observed data, (4) improving controls, (5) guiding experiments with optimal design, and (6) enhancing materials discovery. For each problem, we give background, review past ML work, suggest features of future models, and list challenges and idiosyncrasies facing ML development. We also discuss ongoing efforts to update the fusion data ecosystem and identify opportunities further down the line that will be enabled as fusion and its data infrastructure advance. It is our position that fusion energy offers especially exciting opportunities for ML practitioners to impact decarbonization and the future of energy.",
        "_bibtex": "@inproceedings{\nspangher2024position,\ntitle={Position: Opportunities Exist for Machine Learning in Magnetic Fusion Energy},\nauthor={Lucas Spangher and Allen M. Wang and Andrew Maris and Myles Stapelberg and Viraj Mehta and Alex Saperstein and Stephen Lane-Walsh and Akshata Kishore Moharir and Alessandro Pau and Cristina Rea},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=arwP5FA2dO}\n}"
    },
    {
        "title": "Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models",
        "authorids": [
            "~Christian_Schlarmann1",
            "~Naman_Deep_Singh1",
            "~Francesco_Croce1",
            "~Matthias_Hein2"
        ],
        "abstract": "Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many large vision-language models (LVLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (LVLMs, zero-shot classification) that rely on CLIP. In particular, we show that stealth-attacks on users of LVLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP model with our robust one. No retraining or fine-tuning of the down-stream LVLMs is required. The code and robust models are available on GitHub.",
        "_bibtex": "@inproceedings{\nschlarmann2024robust,\ntitle={Robust {CLIP}: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models},\nauthor={Christian Schlarmann and Naman Deep Singh and Francesco Croce and Matthias Hein},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=WLPhywf1si}\n}"
    },
    {
        "title": "Less is More: on the Over-Globalizing Problem in Graph Transformers",
        "authorids": [
            "~Yujie_Xing2",
            "~Xiao_Wang2",
            "~Yibo_Li2",
            "~Hai_Huang9",
            "~Chuan_Shi1"
        ],
        "abstract": "Graph Transformer, due to its global attention mechanism, has emerged as a new tool in dealing with graph-structured data. It is well recognized that the global attention mechanism considers a wider receptive field in a fully connected graph, leading many to believe that useful information can be extracted from all the nodes. In this paper, we challenge this belief: does the globalizing property always benefit Graph Transformers? We reveal the over-globalizing problem in Graph Transformer by presenting both empirical evidence and theoretical analysis, i.e., the current attention mechanism overly focuses on those distant nodes, while the near nodes, which actually contain most of the useful information, are relatively weakened. Then we propose a novel Bi-Level Global Graph Transformer with Collaborative Training (CoBFormer), including the inter-cluster and intra-cluster Transformers, to prevent the over-globalizing problem while keeping the ability to extract valuable information from distant nodes. Moreover, the collaborative training is proposed to improve the model's generalization ability with a theoretical guarantee. Extensive experiments on various graphs well validate the effectiveness of our proposed CoBFormer.",
        "_bibtex": "@inproceedings{\nxing2024less,\ntitle={Less is More: on the Over-Globalizing Problem in Graph Transformers},\nauthor={Yujie Xing and Xiao Wang and Yibo Li and Hai Huang and Chuan Shi},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=uKmcyyrZae}\n}"
    },
    {
        "title": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection",
        "authorids": [
            "~Jiawei_Zhao2",
            "~Zhenyu_Zhang4",
            "~Beidi_Chen1",
            "~Zhangyang_Wang1",
            "~Anima_Anandkumar1",
            "~Yuandong_Tian1"
        ],
        "abstract": "Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore further reduces optimizer memory by up to 82.5% and total training memory by 63.3%, compared to a BF16 baseline. Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies.",
        "_bibtex": "@inproceedings{\nzhao2024galore,\ntitle={GaLore: Memory-Efficient {LLM} Training by Gradient Low-Rank Projection},\nauthor={Jiawei Zhao and Zhenyu Zhang and Beidi Chen and Zhangyang Wang and Anima Anandkumar and Yuandong Tian},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=hYHsrKDiX7}\n}"
    },
    {
        "title": "PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs",
        "authorids": [
            "~Charlie_Hou1",
            "~Akshat_Shrivastava1",
            "~Hongyuan_Zhan2",
            "~Rylan_Conway1",
            "~Trang_Le1",
            "adithyasagar@meta.com",
            "~Giulia_Fanti1",
            "~Daniel_Lazar1"
        ],
        "abstract": "On-device training is currently the most common approach for training machine learning (ML) models on private, distributed user data. Despite this, on-device training has several drawbacks: (1) most user devices are too small to train large models on-device, (2) on-device training is communication- and computation-intensive, and (3) on-device training can be difficult to debug and deploy. To address these problems, we propose Private Evolution-Text (PrE-Text), a method for generating differentially private (DP) synthetic textual data. First, we show that across multiple datasets, training small models (models that fit on user devices) with PrE-Text synthetic data outperforms small models trained on-device under practical privacy regimes ($\\epsilon=1.29$, $\\epsilon=7.58$). We achieve these results while using 9$\\times$ fewer rounds, 6$\\times$ less client computation per round, and 100$\\times$ less communication per round. Second, finetuning large models on PrE-Text's DP synthetic data improves large language model (LLM) performance on private data across the same range of privacy budgets. Altogether, these results suggest that training on DP synthetic data can be a better option than training a model on-device on private distributed data. Code is available at https://github.com/houcharlie/PrE-Text.",
        "_bibtex": "@inproceedings{\nhou2024pretext,\ntitle={PrE-Text: Training Language Models on Private Federated Data in the Age of {LLM}s},\nauthor={Charlie Hou and Akshat Shrivastava and Hongyuan Zhan and Rylan Conway and Trang Le and Adithya Sagar and Giulia Fanti and Daniel Lazar},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=3WCvnkHnxV}\n}"
    },
    {
        "title": "Compressible Dynamics in Deep Overparameterized Low-Rank Learning & Adaptation",
        "authorids": [
            "~Can_Yaras1",
            "~Peng_Wang23",
            "~Laura_Balzano1",
            "~Qing_Qu2"
        ],
        "abstract": "While overparameterization in machine learning models offers great benefits in terms of optimization and generalization, it also leads to increased computational requirements as model sizes grow. In this work, we show that by leveraging the inherent low-dimensional structures of data and compressible dynamics within the model parameters, we can reap the benefits of overparameterization without the computational burdens. In practice, we demonstrate the effectiveness of this approach for deep low-rank matrix completion as well as fine-tuning language models. Our approach is grounded in theoretical findings for deep overparameterized low-rank matrix recovery, where we show that the learning dynamics of each weight matrix are confined to an invariant low-dimensional subspace. Consequently, we can construct and train compact, highly compressed factorizations possessing the same benefits as their overparameterized counterparts. In the context of deep matrix completion, our technique substantially improves training efficiency while retaining the advantages of overparameterization. For language model fine-tuning, we propose a method called \"Deep LoRA\", which improves the existing low-rank adaptation (LoRA) technique, leading to reduced overfitting and a simplified hyperparameter setup, while maintaining comparable efficiency. We validate the effectiveness of Deep LoRA on natural language tasks, particularly when fine-tuning with limited data.",
        "_bibtex": "@inproceedings{\nyaras2024compressible,\ntitle={Compressible Dynamics in Deep Overparameterized Low-Rank Learning \\& Adaptation},\nauthor={Can Yaras and Peng Wang and Laura Balzano and Qing Qu},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=uDkXoZMzBv}\n}"
    },
    {
        "title": "Position: Technical Research and Talent is Needed for Effective AI Governance",
        "authorids": [
            "~Anka_Reuel1",
            "~Lisa_Soder1",
            "~Benjamin_Bucknall1",
            "trondun@stanford.edu"
        ],
        "abstract": "In light of recent advancements in AI capabilities and the increasingly widespread integration of AI systems into society, governments worldwide are actively seeking to mitigate the potential harms and risks associated with these technologies through regulation and other governance tools. However, there exist significant gaps between governance aspirations and the current state of the technical tooling necessary for their realisation. In this position paper, we survey policy documents published by public-sector institutions in the EU, US, and China to highlight specific areas of disconnect between the technical requirements necessary for enacting proposed policy actions, and the current technical state of the art. Our analysis motivates a call for tighter integration of the AI/ML research community within AI governance in order to i) catalyse technical research aimed at bridging the gap between current and supposed technical underpinnings of regulatory action, as well as ii) increase the level of technical expertise within governing institutions so as to inform and guide effective governance of AI.",
        "_bibtex": "@inproceedings{\nreuel2024position,\ntitle={Position: Technical Research and Talent is Needed for Effective {AI} Governance},\nauthor={Anka Reuel and Lisa Soder and Benjamin Bucknall and Trond Arne Undheim},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=Be2B6f0ps1}\n}"
    },
    {
        "title": "Bottleneck-Minimal Indexing for Generative Document Retrieval",
        "authorids": [
            "~Xin_Du4",
            "~Lixin_Xiu1",
            "~Kumiko_Tanaka-Ishii2"
        ],
        "abstract": "We apply an information-theoretic perspective to reconsider generative document retrieval (GDR), in which a document $x \\in \\mathcal{X}$ is indexed by $t \\in \\mathcal{T}$, and a neural autoregressive model is trained to map queries $\\mathcal{Q}$ to $\\mathcal{T}$. GDR can be considered to involve information transmission from documents $\\mathcal{X}$ to queries $\\mathcal{Q}$, with the requirement to transmit more bits via the indexes $\\mathcal{T}$. By applying Shannon's rate-distortion theory, the optimality of indexing can be analyzed in terms of the mutual information, and the design of the indexes $\\mathcal{T}$ can then be regarded as a *bottleneck* in GDR. After reformulating GDR from this perspective, we empirically quantify the bottleneck underlying GDR. Finally, using the NQ320K and MARCO datasets, we evaluate our proposed bottleneck-minimal indexing method in comparison with various previous indexing methods, and we show that it outperforms those methods.",
        "_bibtex": "@inproceedings{\ndu2024bottleneckminimal,\ntitle={Bottleneck-Minimal Indexing for Generative Document Retrieval},\nauthor={Xin Du and Lixin Xiu and Kumiko Tanaka-Ishii},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=MFPYCvWsNR}\n}"
    },
    {
        "title": "Hybrid$^2$ Neural ODE Causal Modeling and an Application to Glycemic Response",
        "authorids": [
            "~Bob_Junyi_Zou1",
            "~Matthew_E_Levine1",
            "~Dessi_P._Zaharieva1",
            "~Ramesh_Johari1",
            "~Emily_Fox2"
        ],
        "abstract": "Hybrid models composing mechanistic ODE-based dynamics with flexible and expressive neural network components have grown rapidly in popularity, especially in scientific domains where such ODE-based modeling offers important interpretability and validated causal grounding (e.g., for counterfactual reasoning). The incorporation of mechanistic models also provides inductive bias in standard blackbox modeling approaches, critical when learning from small datasets or partially observed, complex systems. Unfortunately, as the hybrid models become more flexible, the causal grounding provided by the mechanistic model can quickly be lost. We address this problem by leveraging another common source of domain knowledge: *ranking* of treatment effects for a set of interventions, even if the precise treatment effect is unknown. We encode this information in a *causal loss* that we combine with the standard predictive loss to arrive at a *hybrid loss* that biases our learning towards causally valid hybrid models. We demonstrate our ability to achieve a win-win, state-of-the-art predictive performance *and* causal validity, in the challenging task of modeling glucose dynamics post-exercise in individuals with type 1 diabetes.",
        "_bibtex": "@inproceedings{\nzou2024hybrid,\ntitle={Hybrid\\${\\textasciicircum}2\\$ Neural {ODE} Causal Modeling and an Application to Glycemic Response},\nauthor={Bob Junyi Zou and Matthew E Levine and Dessi P. Zaharieva and Ramesh Johari and Emily Fox},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=GHZVjmaGQM}\n}"
    },
    {
        "title": "FedMBridge: Bridgeable Multimodal Federated Learning",
        "authorids": [
            "~Jiayi_Chen4",
            "~Aidong_Zhang2"
        ],
        "abstract": "Multimodal Federated Learning (MFL) addresses the setup of multiple clients with diversified modality types (e.g. image, text, video, and audio) working together to improve their local personal models in a data-privacy manner. Prior MFL works rely on restrictive compositional neural architecture designs to ensure inter-client information sharing via blockwise model aggregation, limiting their applicability in the real-world **Architecture-personalized MFL (AMFL)** scenarios, where clients may have distinguished multimodal interaction strategies and there is no restriction on local architecture design. The key challenge in AMFL is how to automatically and efficiently tackle the two heterogeneity patterns--statistical and architecture heterogeneity--while maximizing the beneficial information sharing among clients. To solve this challenge, we propose **FedMBridge**, which leverages a topology-aware hypernetwork to act as a bridge that can automatically balance and digest the two heterogeneity patterns in a communication-efficient manner. Our experiments on four AMFL simulations demonstrate the efficiency and effectiveness of our proposed approach.",
        "_bibtex": "@inproceedings{\nchen2024fedmbridge,\ntitle={Fed{MB}ridge: Bridgeable Multimodal Federated Learning},\nauthor={Jiayi Chen and Aidong Zhang},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=jrHUbftLd6}\n}"
    },
    {
        "title": "How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?",
        "authorids": [
            "~Ryan_Liu1",
            "~Theodore_Sumers1",
            "~Ishita_Dasgupta1",
            "~Thomas_L._Griffiths1"
        ],
        "abstract": "In day-to-day communication, people often approximate the truth --- for example, rounding the time or omitting details --- in order to be maximally helpful to the listener. How do large language models (LLMs) handle such nuanced trade-offs? To address this question, we use psychological models and experiments designed to characterize human behavior to analyze LLMs. We test a range of LLMs and explore how optimization for human preferences or inference-time reasoning affects these trade-offs. We find that reinforcement learning from human feedback improves both honesty and helpfulness, while chain-of-thought prompting skews LLMs towards helpfulness over honesty. Finally, GPT-4 Turbo demonstrates human-like response patterns including sensitivity to the conversational framing and listener's decision context. Our findings reveal the conversational values internalized by LLMs and suggest that even these abstract values can, to a degree, be steered by zero-shot prompting.",
        "_bibtex": "@inproceedings{\nliu2024how,\ntitle={How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?},\nauthor={Ryan Liu and Theodore Sumers and Ishita Dasgupta and Thomas L. Griffiths},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=685vj0lC9z}\n}"
    },
    {
        "title": "Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision",
        "authorids": [
            "~Collin_Burns1",
            "~Pavel_Izmailov1",
            "~Jan_Hendrik_Kirchner1",
            "~Bowen_Baker2",
            "~Leo_Gao1",
            "leopold@openai.com",
            "~Yining_Chen1",
            "~Adrien_Ecoffet1",
            "manas@openai.com",
            "~Jan_Leike1",
            "~Ilya_Sutskever2",
            "~Jeffrey_Wu1"
        ],
        "abstract": "Widely used alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on the ability of humans to supervise model behavior---for example, to evaluate whether a model faithfully followed instructions or generated safe outputs. However, future superhuman models will behave in complex ways too difficult for humans to reliably evaluate; humans will only be able to *weakly supervise* superhuman models. We study an analogy to this problem: can weak model supervision elicit the full capabilities of a much stronger model? We test this using a range of pretrained language models in the GPT-4 family on natural language processing (NLP), chess, and reward modeling tasks. We find that when we naively finetune strong pretrained models on labels generated by a weak model, they consistently perform better than their weak supervisors, a phenomenon we call *weak-to-strong generalization*. However, we are still far from recovering the full capabilities of strong models with naive finetuning alone, suggesting that techniques like RLHF may scale poorly to superhuman models without further work. We find that simple methods can often significantly improve weak-to-strong generalization: for example, when finetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence loss, we can recover close to GPT-3.5-level performance on NLP tasks. Our results suggest that it is feasible to make empirical progress today on a fundamental challenge of aligning superhuman models.",
        "_bibtex": "@inproceedings{\nburns2024weaktostrong,\ntitle={Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision},\nauthor={Collin Burns and Pavel Izmailov and Jan Hendrik Kirchner and Bowen Baker and Leo Gao and Leopold Aschenbrenner and Yining Chen and Adrien Ecoffet and Manas Joglekar and Jan Leike and Ilya Sutskever and Jeffrey Wu},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=ghNRg2mEgN}\n}"
    },
    {
        "title": "ACE: Off-Policy Actor-Critic with Causality-Aware Entropy Regularization",
        "authorids": [
            "~Tianying_Ji2",
            "~Yongyuan_Liang1",
            "~Yan_Zeng2",
            "~Yu_Luo5",
            "~Guowei_Xu2",
            "~Jiawei_Guo4",
            "~Ruijie_Zheng1",
            "~Furong_Huang1",
            "~Fuchun_Sun1",
            "~Huazhe_Xu1"
        ],
        "abstract": "The varying significance of distinct primitive behaviors during the policy learning process has been overlooked by prior model-free RL algorithms. Leveraging this insight, we explore the causal relationship between different action dimensions and rewards to evaluate the significance of various primitive behaviors during training. We introduce a causality-aware entropy term that effectively identifies and prioritizes actions with high potential impacts for efficient exploration. Furthermore, to prevent excessive focus on specific primitive behaviors, we analyze the gradient dormancy phenomenon and introduce a dormancy-guided reset mechanism to further enhance the efficacy of our method. Our proposed algorithm, **ACE**: Off-policy **A**ctor-critic with **C**ausality-aware **E**ntropy regularization, demonstrates a substantial performance advantage across 29 diverse continuous control tasks spanning 7 domains compared to model-free RL baselines, which underscores the effectiveness, versatility, and efficient sample efficiency of our approach. Benchmark results and videos are available at https://ace-rl.github.io/.",
        "_bibtex": "@inproceedings{\nji2024ace,\ntitle={{ACE}: Off-Policy Actor-Critic with Causality-Aware Entropy Regularization},\nauthor={Tianying Ji and Yongyuan Liang and Yan Zeng and Yu Luo and Guowei Xu and Jiawei Guo and Ruijie Zheng and Furong Huang and Fuchun Sun and Huazhe Xu},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=1puvYh729M}\n}"
    },
    {
        "title": "Expressivity and Generalization: Fragment-Biases for Molecular GNNs",
        "authorids": [
            "~Tom_Wollschl\u00e4ger1",
            "~Niklas_Kemper1",
            "~Leon_Hetzel1",
            "~Johanna_Sommer1",
            "~Stephan_G\u00fcnnemann1"
        ],
        "abstract": "Although recent advances in higher-order Graph Neural Networks (GNNs) improve the theoretical expressiveness and molecular property predictive performance, they often fall short of the empirical performance of models that explicitly use fragment information as inductive bias. However, for these approaches, there exists no theoretic expressivity study. In this work, we propose the *Fragment-WL* test, an extension to the well-known Weisfeiler & Leman (WL) test, which enables the theoretic analysis of these fragment-biased GNNs. Building on the insights gained from the Fragment-WL test, we develop a new GNN architecture and a fragmentation with infinite vocabulary that significantly boosts expressiveness. We show the effectiveness of our model on synthetic and real-world data where we outperform all GNNs on Peptides and have $12$% lower error than all GNNs on ZINC and $34$% lower error than other fragment-biased models. Furthermore, we show that our model exhibits superior generalization capabilities compared to the latest transformer-based architectures, positioning it as a robust solution for a range of molecular modeling tasks.",
        "_bibtex": "@inproceedings{\nwollschl{\\\"a}ger2024expressivity,\ntitle={Expressivity and Generalization: Fragment-Biases for Molecular {GNN}s},\nauthor={Tom Wollschl{\\\"a}ger and Niklas Kemper and Leon Hetzel and Johanna Sommer and Stephan G{\\\"u}nnemann},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=rPm5cKb1VB}\n}"
    },
    {
        "title": "Theoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability",
        "authorids": [
            "~Sepanta_Zeighami2",
            "~Cyrus_Shahabi1"
        ],
        "abstract": "Use of machine learning to perform database operations, such as indexing, cardinality estimation, and sorting, is shown to provide substantial performance benefits. However, when datasets change and data distribution shifts, empirical results also show performance degradation for learned models, possibly to worse than non-learned alternatives. This, together with a lack of theoretical understanding of learned methods undermines their practical applicability, since there are no guarantees on how well the models will perform after deployment. In this paper, we present the first known theoretical characterization of the performance of learned models in dynamic datasets, for the aforementioned operations. Our results show novel theoretical characteristics achievable by learned models and provide bounds on the performance of the models that characterize their advantages over non-learned methods, showing why and when learned models can outperform the alternatives. Our analysis develops the *distribution learnability* framework and novel theoretical tools which build the foundation for the analysis of learned database operations in the future.",
        "_bibtex": "@inproceedings{\nzeighami2024theoretical,\ntitle={Theoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability},\nauthor={Sepanta Zeighami and Cyrus Shahabi},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=oowQ8LPA12}\n}"
    },
    {
        "title": "Position: A Safe Harbor for AI Evaluation and Red Teaming",
        "authorids": [
            "~Shayne_Longpre1",
            "~Sayash_Kapoor2",
            "~Kevin_Klyman1",
            "aramaswamis@gmail.com",
            "~Rishi_Bommasani1",
            "~Borhane_Blili-Hamelin1",
            "~Yangsibo_Huang2",
            "~Aviya_Skowron1",
            "~Zheng_Xin_Yong1",
            "~Suhas_Kotha1",
            "~Yi_Zeng3",
            "~Weiyan_Shi2",
            "~Xianjun_Yang1",
            "~Reid_Southen1",
            "~Alexander_Robey1",
            "~Patrick_Chao1",
            "~Diyi_Yang2",
            "~Ruoxi_Jia1",
            "~Daniel_Kang1",
            "~Alex_Pentland1",
            "~Arvind_Narayanan1",
            "~Percy_Liang1",
            "~Peter_Henderson1"
        ],
        "abstract": "Independent evaluation and red teaming are critical for identifying the risks posed by generative AI systems. However, the terms of service and enforcement strategies used by prominent AI companies to deter model misuse have disincentives on good faith safety evaluations. This causes some researchers to fear that conducting such research or releasing their findings will result in account suspensions or legal reprisal. Although some companies offer researcher access programs, they are an inadequate substitute for independent research access, as they have limited community representation, receive inadequate funding, and lack independence from corporate incentives. We propose that major generative AI developers commit to providing a legal and technical safe harbor, protecting public interest safety research and removing the threat of account suspensions or legal reprisal. These proposals emerged from our collective experience conducting safety, privacy, and trustworthiness research on generative AI systems, where norms and incentives could be better aligned with public interests, without exacerbating model misuse. We believe these commitments are a necessary step towards more inclusive and unimpeded community efforts to tackle the risks of generative AI.",
        "_bibtex": "@inproceedings{\nlongpre2024position,\ntitle={Position Paper: A Safe Harbor for {AI} Evaluation and Red Teaming},\nauthor={Shayne Longpre and Sayash Kapoor and Kevin Klyman and Ashwin Ramaswami and Rishi Bommasani and Borhane Blili-Hamelin and Yangsibo Huang and Aviya Skowron and Zheng Xin Yong and Suhas Kotha and Yi Zeng and Weiyan Shi and Xianjun Yang and Reid Southen and Alexander Robey and Patrick Chao and Diyi Yang and Ruoxi Jia and Daniel Kang and Alex Pentland and Arvind Narayanan and Percy Liang and Peter Henderson},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=dLojMSgSFW}\n}"
    },
    {
        "title": "A Dynamic Algorithm for Weighted Submodular Cover Problem",
        "authorids": [
            "~Kiarash_Banihashem1",
            "~Samira_Goudarzi1",
            "~MohammadTaghi_Hajiaghayi1",
            "~Peyman_Jabbarzade1",
            "~Morteza_Monemizadeh1"
        ],
        "abstract": "We initiate the study of the submodular cover problem in a dynamic setting where the elements of the ground set are inserted and deleted. In the classical submodular cover problem, we are given a monotone submodular function $f : 2^{V} \\to \\mathbb{R}^{\\ge 0}$ and the goal is to obtain a set $S \\subseteq V$ that minimizes the cost subject to the constraint $f(S) = f(V)$. This is a classical problem in computer science and generalizes the Set Cover problem, 2-Set Cover, and dominating set problem among others. We consider this problem in a dynamic setting where there are updates to our set $V$, in the form of insertions and deletions of elements from a ground set $\\mathcal{V}$, and the goal is to maintain an approximately optimal solution with low query complexity per update. For this problem, we propose a randomized algorithm that, in expectation, obtains a $(1-O(\\epsilon), O(\\epsilon^{-1}))$-bicriteria approximation using polylogarithmic query complexity per update.",
        "_bibtex": "@inproceedings{\nbanihashem2024a,\ntitle={A Dynamic Algorithm for Weighted Submodular Cover Problem},\nauthor={Kiarash Banihashem and Samira Goudarzi and MohammadTaghi Hajiaghayi and Peyman Jabbarzade and Morteza Monemizadeh},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=uUeXaKLE1I}\n}"
    },
    {
        "title": "Rethinking Data Shapley for Data Selection Tasks: Misleads and Merits",
        "authorids": [
            "~Jiachen_T._Wang1",
            "~Tianji_Yang1",
            "~James_Zou1",
            "~Yongchan_Kwon1",
            "~Ruoxi_Jia1"
        ],
        "abstract": "Data Shapley provides a principled approach to data valuation and plays a crucial role in data-centric machine learning (ML) research. Data selection is considered a standard application of Data Shapley. However, its data selection performance has shown to be inconsistent across settings in the literature. This study aims to deepen our understanding of this phenomenon. We introduce a hypothesis testing framework and show that Data Shapley's performance can be no better than random selection without specific constraints on utility functions. We identify a class of utility functions, monotonically transformed modular functions, within which Data Shapley optimally selects data. Based on this insight, we propose a heuristic for predicting Data Shapley\u2019s effectiveness in data selection tasks. Our experiments corroborate these findings, adding new insights into when Data Shapley may or may not succeed.",
        "_bibtex": "@inproceedings{\nwang2024rethinking,\ntitle={Rethinking Data Shapley for Data Selection Tasks: Misleads and Merits},\nauthor={Jiachen T. Wang and Tianji Yang and James Zou and Yongchan Kwon and Ruoxi Jia},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=mKYBMf1hHG}\n}"
    },
    {
        "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study",
        "authorids": [
            "~Shusheng_Xu1",
            "~Wei_Fu1",
            "~Jiaxuan_Gao1",
            "~Wenjie_Ye1",
            "~Weilin_Liu1",
            "~Zhiyu_Mei1",
            "~Guangju_Wang1",
            "~Chao_Yu1",
            "~Yi_Wu1"
        ],
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) is currently the most widely used method to align large language models (LLMs) with human preferences. Existing RLHF methods can be roughly categorized as either reward-based or reward-free. Novel applications such as ChatGPT and Claude leverage reward-based methods that first learn a reward model and apply actor-critic algorithms, such as Proximal Policy Optimization (PPO). However, in academic benchmarks, state-of-the-art results are often achieved via reward-free methods, such as Direct Preference Optimization (DPO). Is DPO truly superior to PPO? Why does PPO perform poorly on these benchmarks? In this paper, we first conduct both theoretical and empirical studies on the algorithmic properties of DPO and show that DPO may have fundamental limitations. Moreover, we also comprehensively examine PPO and reveal the key factors for the best performances of PPO in fine-tuning LLMs. Finally, we benchmark DPO and PPO across a collection of RLHF testbeds, ranging from dialogue to code generation. Experiment results demonstrate that PPO is able to surpass other alignment methods in all cases and achieve state-of-the-art results in challenging code competitions.",
        "_bibtex": "@inproceedings{\nxu2024is,\ntitle={Is {DPO} Superior to {PPO} for {LLM} Alignment? A Comprehensive Study},\nauthor={Shusheng Xu and Wei Fu and Jiaxuan Gao and Wenjie Ye and Weilin Liu and Zhiyu Mei and Guangju Wang and Chao Yu and Yi Wu},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=6XH8R7YrSk}\n}"
    },
    {
        "title": "Trained Random Forests Completely Reveal your Dataset",
        "authorids": [
            "~Julien_Ferry1",
            "~Ricardo_Fukasawa1",
            "~Timoth\u00e9e_Pascal1",
            "~Thibaut_Vidal1"
        ],
        "abstract": "We introduce an optimization-based reconstruction attack capable of completely or near-completely reconstructing a dataset utilized for training a random forest. Notably, our approach relies solely on information readily available in commonly used libraries such as scikit-learn. To achieve this, we formulate the reconstruction problem as a combinatorial problem under a maximum likelihood objective. We demonstrate that this problem is NP-hard, though solvable at scale using constraint programming - an approach rooted in constraint propagation and solution-domain reduction. Through an extensive computational investigation, we demonstrate that random forests trained without bootstrap aggregation but with feature randomization are susceptible to a complete reconstruction. This holds true even with a small number of trees. Even with bootstrap aggregation, the majority of the data can also be reconstructed. These findings underscore a critical vulnerability inherent in widely adopted ensemble methods, warranting attention and mitigation. Although the potential for such reconstruction attacks has been discussed in privacy research, our study provides clear empirical evidence of their practicability.",
        "_bibtex": "@inproceedings{\nferry2024trained,\ntitle={Trained Random Forests Completely Reveal your Dataset},\nauthor={Julien Ferry and Ricardo Fukasawa and Timoth{\\'e}e Pascal and Thibaut Vidal},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=cc72Vnfvoc}\n}"
    },
    {
        "title": "LoRA Training in the NTK Regime has No Spurious Local Minima",
        "authorids": [
            "~Uijeong_Jang1",
            "~Jason_D._Lee1",
            "~Ernest_K._Ryu1"
        ],
        "abstract": "Low-rank adaptation (LoRA) has become the standard approach for parameter-efficient fine-tuning of large language models (LLM), but our theoretical understanding of LoRA has been limited. In this work, we theoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK) regime with $N$ data points, showing: (i) full fine-tuning (without LoRA) admits a low-rank solution of rank $r\\lesssim \\sqrt{N}$; (ii) using LoRA with rank $r\\gtrsim \\sqrt{N}$ eliminates spurious local minima, allowing gradient descent to find the low-rank solutions; (iii) the low-rank solution found using LoRA generalizes well.",
        "_bibtex": "@inproceedings{\njang2024lora,\ntitle={Lo{RA} Training in the {NTK} Regime has No Spurious Local Minima},\nauthor={Uijeong Jang and Jason D. Lee and Ernest K. Ryu},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=s1sdx6vNsU}\n}"
    },
    {
        "title": "SAPG: Split and Aggregate Policy Gradients",
        "authorids": [
            "~Jayesh_Singla1",
            "~Ananye_Agarwal1",
            "~Deepak_Pathak1"
        ],
        "abstract": "Despite extreme sample inefficiency, on-policy reinforcement learning, aka policy gradients, has become a fundamental tool in decision-making problems. With the recent advances in GPU-driven simulation, the ability to collect large amounts of data for RL training has scaled exponentially. However, we show that current RL methods, e.g. PPO, fail to ingest the benefit of parallelized environments beyond a certain point and their performance saturates. To address this, we propose a new on-policy RL algorithm that can effectively leverage large-scale environments by splitting them into chunks and fusing them back together via importance sampling. Our algorithm, termed SAPG, shows significantly higher performance across a variety of challenging environments where vanilla PPO and other strong baselines fail to achieve high performance. Webpage at https://sapg-rl.github.io/.",
        "_bibtex": "@inproceedings{\nsingla2024sapg,\ntitle={{SAPG}: Split and Aggregate Policy Gradients},\nauthor={Jayesh Singla and Ananye Agarwal and Deepak Pathak},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=4dOJAfXhNV}\n}"
    },
    {
        "title": "How Private are DP-SGD Implementations?",
        "authorids": [
            "~Lynn_Chua1",
            "~Badih_Ghazi1",
            "~Pritish_Kamath2",
            "~Ravi_Kumar1",
            "~Pasin_Manurangsi2",
            "~Amer_Sinha1",
            "~Chiyuan_Zhang1"
        ],
        "abstract": "We demonstrate a substantial gap between the privacy guarantees of the Adaptive Batch Linear Queries (ABLQ) mechanism under different types of batch sampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis of Differentially Private Stochastic Gradient Descent (DP-SGD) follows by interpreting it as a post-processing of ABLQ. While shuffling-based DP-SGD is more commonly used in practical implementations, it has not been amenable to easy privacy analysis, either analytically or even numerically. On the other hand, Poisson subsampling-based DP-SGD is challenging to scalably implement, but has a well-understood privacy analysis, with multiple open-source numerically tight privacy accountants available. This has led to a common practice of using shuffling-based DP-SGD in practice, but using the privacy analysis for the corresponding Poisson subsampling version. Our result shows that there can be a substantial gap between the privacy analysis when using the two types of batch sampling, and thus advises caution in reporting privacy parameters for DP-SGD.",
        "_bibtex": "@inproceedings{\nchua2024how,\ntitle={How Private are {DP}-{SGD} Implementations?},\nauthor={Lynn Chua and Badih Ghazi and Pritish Kamath and Ravi Kumar and Pasin Manurangsi and Amer Sinha and Chiyuan Zhang},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=xWI0MKwJSS}\n}"
    },
    {
        "title": "From Coarse to Fine: Enable Comprehensive Graph Self-supervised Learning with Multi-granular Semantic Ensemble",
        "authorids": [
            "~Qianlong_Wen1",
            "~Mingxuan_Ju1",
            "~Zhongyu_Ouyang1",
            "~Chuxu_Zhang2",
            "~Yanfang_Ye1"
        ],
        "abstract": "Self-supervised learning (SSL) has gained increasing attention in the graph learning community, owing to its capability of enabling powerful models pre-trained on large unlabeled graphs for general purposes, facilitating quick adaptation to specific domains. Though promising, existing graph SSL frameworks often struggle to capture both high-level abstract features and fine-grained features simultaneously, leading to sub-optimal generalization abilities across different downstream tasks. To bridge this gap, we present Multi-granularity Graph Semantic Ensemble via Knowledge Distillation, namely MGSE, a plug-and-play graph knowledge distillation framework that can be applied to any existing graph SSL framework to enhance its performance by incorporating the concept of multi-granularity. Specifically, MGSE captures multi-granular knowledge by employing multiple student models to learn from a single teacher model, conditioned by probability distributions with different granularities. We apply it to six state-of-the-art graph SSL frameworks and evaluate their performances over multiple graph datasets across different domains, the experimental results show that MGSE can consistently boost the performance of these existing graph SSL frameworks with up to 9.2% improvement.",
        "_bibtex": "@inproceedings{\nwen2024from,\ntitle={From Coarse to Fine: Enable Comprehensive Graph Self-supervised Learning with Multi-granular Semantic Ensemble},\nauthor={Qianlong Wen and Mingxuan Ju and Zhongyu Ouyang and Chuxu Zhang and Yanfang Ye},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=JnA9IveEwg}\n}"
    },
    {
        "title": "Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution",
        "authorids": [
            "~Aaron_Lou1",
            "~Chenlin_Meng1",
            "~Stefano_Ermon1"
        ],
        "abstract": "Despite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. In this work, we bridge this gap by proposing score entropy, a novel loss that naturally extends score matching to discrete spaces, integrates seamlessly to build discrete diffusion models, and significantly boosts performance. Experimentally, we test our Score Entropy Discrete Diffusion models (SEDD) on standard language modeling tasks. For comparable model sizes, SEDD beats existing language diffusion paradigms (reducing perplexity by $25$-$75$%) and is competitive with autoregressive models, in particular outperforming GPT-2. Furthermore, compared to autoregressive mdoels, SEDD generates faithful text without requiring distribution annealing techniques like temperature scaling (around $6$-$8\\times$ better generative perplexity than un-annealed GPT-2), can trade compute and quality (similar quality with $32\\times$ fewer network evaluations), and enables controllable infilling (matching nucleus sampling quality while enabling other strategies besides left to right prompting).",
        "_bibtex": "@inproceedings{\nlou2024discrete,\ntitle={Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution},\nauthor={Aaron Lou and Chenlin Meng and Stefano Ermon},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=CNicRIVIPA}\n}"
    },
    {
        "title": "Discovering Environments with XRM",
        "authorids": [
            "~Mohammad_Pezeshki1",
            "~Diane_Bouchacourt3",
            "~Mark_Ibrahim1",
            "~Nicolas_Ballas1",
            "~Pascal_Vincent1",
            "~David_Lopez-Paz2"
        ],
        "abstract": "Environment annotations are essential for the success of many out-of-distribution (OOD) generalization methods. Unfortunately, these are costly to obtain and often limited by human annotators' biases. To achieve robust generalization, it is essential to develop algorithms for automatic environment discovery within datasets. Current proposals, which divide examples based on their training error, suffer from one fundamental problem. These methods introduce hyper-parameters and early-stopping criteria, which require a validation set with human-annotated environments, the very information subject to discovery. In this paper, we propose Cross-Risk Minimization (XRM) to address this issue. XRM trains twin networks, each learning from one random half of the training data, while imitating confident held-out mistakes made by its sibling. XRM provides a recipe for hyper-parameter tuning, does not require early-stopping, and can discover environments for all training and validation data. Algorithms built on top of XRM environments achieve oracle worst-group-accuracy, addressing a long-standing challenge in OOD generalization.",
        "_bibtex": "@inproceedings{\npezeshki2024discovering,\ntitle={Discovering Environments with {XRM}},\nauthor={Mohammad Pezeshki and Diane Bouchacourt and Mark Ibrahim and Nicolas Ballas and Pascal Vincent and David Lopez-Paz},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=gPStP3FSY9}\n}"
    },
    {
        "title": "Fast Co-Training under Weak Dependence via Stream-Based Active Learning",
        "authorids": [
            "~Ilias_Diakonikolas1",
            "~Mingchen_Ma1",
            "~Lisheng_Ren1",
            "~Christos_Tzamos1"
        ],
        "abstract": "Co-training is a classical semi-supervised learning method which only requires a small number of labeled examples for learning, under reasonable assumptions. Despite extensive literature on the topic, very few hypothesis classes are known to be provably efficiently learnable via co-training, even under very strong distributional assumptions. In this work, we study the co-training problem in the stream-based active learning model. We show that a range of natural concept classes are efficiently learnable via co-training, in terms of both label efficiency and computational efficiency. We provide an efficient reduction of co-training under the standard assumption of weak dependence, in the stream-based active model, to online classification. As a corollary, we obtain efficient co-training algorithms with error independent label complexity for every concept class class efficiently learnable in the mistake bound online model. Our framework also gives co-training algorithms with label complexity $\\tilde{O}(d\\log (1/\\epsilon))$ for any concept class with VC dimension $d$, though in general this reduction is not computationally efficient. Finally, using additional ideas from online learning, we design the first efficient co-training algorithms with label complexity $\\tilde{O}(d^2\\log (1/\\epsilon))$ for several concept classes, including unions of intervals and homogeneous halfspaces.",
        "_bibtex": "@inproceedings{\ndiakonikolas2024fast,\ntitle={Fast Co-Training under Weak Dependence via Stream-Based Active Learning},\nauthor={Ilias Diakonikolas and Mingchen Ma and Lisheng Ren and Christos Tzamos},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=GqWy1wZKeE}\n}"
    },
    {
        "title": "Active Adaptive Experimental Design for Treatment Effect Estimation with Covariate Choice",
        "authorids": [
            "~Masahiro_Kato1",
            "akihiro-oga@fintec.co.jp",
            "wataru-komatsubara@fintec.co.jp",
            "ryo-inokuchi@fintec.co.jp"
        ],
        "abstract": "This study designs an adaptive experiment for efficiently estimating *average treatment effects* (ATEs). In each round of our adaptive experiment, an experimenter sequentially samples an experimental unit, assigns a treatment, and observes the corresponding outcome immediately. At the end of the experiment, the experimenter estimates an ATE using the gathered samples. The objective is to estimate the ATE with a smaller asymptotic variance. Existing studies have designed experiments that adaptively optimize the propensity score (treatment-assignment probability). As a generalization of such an approach, we propose optimizing the covariate density as well as the propensity score. First, we derive the efficient covariate density and propensity score that minimize the semiparametric efficiency bound and find that optimizing both covariate density and propensity score minimizes the semiparametric efficiency bound more effectively than optimizing only the propensity score. Next, we design an adaptive experiment using the efficient covariate density and propensity score sequentially estimated during the experiment. Lastly, we propose an ATE estimator whose asymptotic variance aligns with the minimized semiparametric efficiency bound.",
        "_bibtex": "@inproceedings{\nkato2024active,\ntitle={Active Adaptive Experimental Design for Treatment Effect Estimation with Covariate Choice},\nauthor={Masahiro Kato and Akihiro Oga and Wataru Komatsubara and Ryo Inokuchi},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=K6HpbvkrwO}\n}"
    },
    {
        "title": "Listenable Maps for Audio Classifiers",
        "authorids": [
            "~Francesco_Paissan1",
            "~Mirco_Ravanelli1",
            "~Cem_Subakan1"
        ],
        "abstract": "Despite the impressive performance of deep learning models across diverse tasks, their complexity poses challenges for interpretation. This challenge is particularly evident for audio signals, where conveying interpretations becomes inherently difficult. To address this issue, we introduce Listenable Maps for Audio Classifiers (L-MAC), a posthoc interpretation method that generates faithful and listenable interpretations. L-MAC utilizes a decoder on top of a pretrained classifier to generate binary masks that highlight relevant portions of the input audio. We train the decoder with a loss function that maximizes the confidence of the classifier decision on the masked-in portion of the audio while minimizing the probability of model output for the masked-out portion. Quantitative evaluations on both in-domain and out-of-domain data demonstrate that L-MAC consistently produces more faithful interpretations than several gradient and masking-based methodologies. Furthermore, a user study confirms that, on average, users prefer the interpretations generated by the proposed technique.",
        "_bibtex": "@inproceedings{\npaissan2024listenable,\ntitle={Listenable Maps for Audio Classifiers},\nauthor={Francesco Paissan and Mirco Ravanelli and Cem Subakan},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=kAfYYg6PX8}\n}"
    },
    {
        "title": "I/O Complexity of Attention, or How Optimal is FlashAttention?",
        "authorids": [
            "~Barna_Saha3",
            "~Christopher_Ye1"
        ],
        "abstract": "Attention is at the heart of the popular Transformer architecture, yet suffers from quadratic time and memory complexity. In a recent significant development, FlashAttention shows that the I/O complexity of attention is the true bottleneck in scaling Transformers. Given two levels of memory hierarchy, a fast cache (e.g. GPU on-chip SRAM) where computation happens and a slow memory (e.g. GPU high-bandwidth memory) where the data resides, the I/O complexity measures the number of accesses to the slow memory. FlashAttention is an I/O-aware algorithm for self-attention that requires $\\frac{N^2d^2}{M}$ I/O operations where $N$ is the dimension of the attention matrix, $d$ is the head-dimension and $M$ is the size of cache. Naturally, to further reduce the computational costs of Attention, the authors ask the question: is FlashAttention's I/O complexity optimal for every value of $M$? We resolve the above question in its full generality by showing an I/O complexity lower bound that matches the upper bound provided by FlashAttention for any values of $M \\geq d^2$ within any constant factors. Moreover, our lower bounds do not rely on using combinatorial matrix multiplication for computing the attention matrix: even if one uses fast matrix multiplication, the above I/O complexity bounds cannot be improved. Further, we give a better algorithm with lower I/O complexity for $M < d^2$, and show that it is optimal for combinatorial algorithms. We do so by introducing a new communication complexity protocol for matrix compression, and connecting communication complexity to I/O complexity. We believe this connection could be of independent interest and will find more applications in proving I/O complexity lower bounds in future.",
        "_bibtex": "@inproceedings{\nsaha2024io,\ntitle={I/O Complexity of Attention, or How Optimal is FlashAttention?},\nauthor={Barna Saha and Christopher Ye},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=MdPBVWTfwG}\n}"
    },
    {
        "title": "Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling",
        "authorids": [
            "~Bairu_Hou2",
            "~Yujian_Liu1",
            "~Kaizhi_Qian1",
            "~Jacob_Andreas1",
            "~Shiyu_Chang2",
            "~Yang_Zhang3"
        ],
        "abstract": "Uncertainty decomposition refers to the task of decomposing the total uncertainty of a predictive model into aleatoric (data) uncertainty, resulting from inherent randomness in the data-generating process, and epistemic (model) uncertainty, resulting from missing information in the model's training data. In large language models (LLMs) specifically, identifying sources of uncertainty is an important step toward improving reliability, trustworthiness, and interpretability, but remains an important open research question. In this paper, we introduce an uncertainty decomposition framework for LLMs, called input clarification ensembling, which can be applied to any pre-trained LLM. Our approach generates a set of clarifications for the input, feeds them into an LLM, and ensembles the corresponding predictions. We show that, when aleatoric uncertainty arises from ambiguity or under-specification in LLM inputs, this approach makes it possible to factor an (un-clarified) LLM's predictions into separate aleatoric and epistemic terms, using a decomposition similar to the one employed by Bayesian neural networks. Empirical evaluations demonstrate that input clarification ensembling provides accurate and reliable uncertainty quantification on several language processing tasks. Code and data are available at https://github.com/UCSB-NLP-Chang/llm_uncertainty.",
        "_bibtex": "@inproceedings{\nhou2024decomposing,\ntitle={Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling},\nauthor={Bairu Hou and Yujian Liu and Kaizhi Qian and Jacob Andreas and Shiyu Chang and Yang Zhang},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=byxXa99PtF}\n}"
    },
    {
        "title": "Inferring the Long-Term Causal Effects of Long-Term Treatments from Short-Term Experiments",
        "authorids": [
            "~Allen_Tran1",
            "~Aurelien_Bibaut1",
            "~Nathan_Kallus1"
        ],
        "abstract": "We study inference on the long-term causal effect of a continual exposure to a novel intervention, which we term a long-term treatment, based on an experiment involving only short-term observations. Key examples include the long-term health effects of regularly-taken medicine or of environmental hazards and the long-term effects on users of changes to an online platform. This stands in contrast to short-term treatments or \"shocks,\" whose long-term effect can reasonably be mediated by short-term observations, enabling the use of surrogate methods. Long-term treatments by definition have direct effects on long-term outcomes via continual exposure, so surrogacy conditions cannot reasonably hold. We connect the problem with offline reinforcement learning, leveraging doubly-robust estimators to estimate long-term causal effects for long-term treatments and construct confidence intervals.",
        "_bibtex": "@inproceedings{\ntran2024inferring,\ntitle={Inferring the Long-Term Causal Effects of Long-Term Treatments from Short-Term Experiments},\nauthor={Allen Tran and Aurelien Bibaut and Nathan Kallus},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=lQ2o7JteMO}\n}"
    },
    {
        "title": "Multiplicative Weights Update, Area Convexity and Random Coordinate Descent for Densest Subgraph Problems",
        "authorids": [
            "~Ta_Duy_Nguyen1",
            "~Alina_Ene1"
        ],
        "abstract": "We study the densest subgraph problem and give algorithms via multiplicative weights update and area convexity that converge in $O\\left(\\frac{\\log m}{\\epsilon^{2}}\\right)$ and $O\\left(\\frac{\\log m}{\\epsilon}\\right)$ iterations, respectively, both with nearly-linear time per iteration. Compared with the work by Bahmani et al. (2014), our MWU algorithm uses a very different and much simpler procedure for recovering the dense subgraph from the fractional solution and does not employ a binary search. Compared with the work by Boob et al. (2019), our algorithm via area convexity improves the iteration complexity by a factor $\\Delta$---the maximum degree in the graph, and matches the fastest theoretical runtime currently known via flows (Chekuri et al., 2022) in total time. Next, we study the dense subgraph decomposition problem and give the first practical iterative algorithm with linear convergence rate $O\\left(mn\\log\\frac{1}{\\epsilon}\\right)$ via accelerated random coordinate descent. This significantly improves over $O\\left(\\frac{m\\sqrt{mn\\Delta}}{\\epsilon}\\right)$ time of the FISTA-based algorithm by Harb et al. (2022). In the high precision regime $\\epsilon\\ll\\frac{1}{n}$ where we can even recover the exact solution, our algorithm has a total runtime of $O\\left(mn\\log n\\right)$, matching the state of the art exact algorithm via parametric flows (Gallo et al., 1989). Empirically, we show that this algorithm is very practical and scales to very large graphs, and its performance is competitive with widely used methods that have significantly weaker theoretical guarantees.",
        "_bibtex": "@inproceedings{\nnguyen2024multiplicative,\ntitle={Multiplicative Weights Update, Area Convexity and Random Coordinate Descent for Densest Subgraph Problems},\nauthor={Ta Duy Nguyen and Alina Ene},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=d2E2i5rJ4x}\n}"
    },
    {
        "title": "All-in-one simulation-based inference",
        "authorids": [
            "~Manuel_Gloeckler1",
            "~Michael_Deistler1",
            "~Christian_Dietrich_Weilbach1",
            "~Frank_Wood2",
            "~Jakob_H._Macke1"
        ],
        "abstract": "Amortized Bayesian inference trains neural networks to solve stochastic inference problems using model simulations, thereby making it possible to rapidly perform Bayesian inference for any newly observed data. However, current simulation-based amortized inference methods are simulation-hungry and inflexible: They require the specification of a fixed parametric prior, simulator, and inference tasks ahead of time. Here, we present a new amortized inference method---the Simformer---which overcomes these limitations. By training a probabilistic diffusion model with transformer architectures, the Simformer outperforms current state-of-the-art amortized inference approaches on benchmark tasks and is substantially more flexible: It can be applied to models with function-valued parameters, it can handle inference scenarios with missing or unstructured data, and it can sample arbitrary conditionals of the joint distribution of parameters and data, including both posterior and likelihood. We showcase the performance and flexibility of the Simformer on simulators from ecology, epidemiology, and neuroscience, and demonstrate that it opens up new possibilities and application domains for amortized Bayesian inference on simulation-based models.",
        "_bibtex": "@inproceedings{\ngloeckler2024allinone,\ntitle={All-in-one simulation-based inference},\nauthor={Manuel Gloeckler and Michael Deistler and Christian Dietrich Weilbach and Frank Wood and Jakob H. Macke},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=DL79HYCFFq}\n}"
    },
    {
        "title": "Environment Design for Inverse Reinforcement Learning",
        "authorids": [
            "~Thomas_Kleine_Buening1",
            "~Victor_Villin1",
            "~Christos_Dimitrakakis1"
        ],
        "abstract": "Learning a reward function from demonstrations suffers from low sample-efficiency. Even with abundant data, current inverse reinforcement learning methods that focus on learning from a single environment can fail to handle slight changes in the environment dynamics. We tackle these challenges through adaptive environment design. In our framework, the learner repeatedly interacts with the expert, with the former selecting environments to identify the reward function as quickly as possible from the expert\u2019s demonstrations in said environments. This results in improvements in both sample-efficiency and robustness, as we show experimentally, for both exact and approximate inference.",
        "_bibtex": "@inproceedings{\nbuening2024environment,\ntitle={Environment Design for Inverse Reinforcement Learning},\nauthor={Thomas Kleine Buening and Victor Villin and Christos Dimitrakakis},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=Ar0dsOMStE}\n}"
    },
    {
        "title": "Scalable AI Safety via Doubly-Efficient Debate",
        "authorids": [
            "~Jonah_Brown-Cohen1",
            "~Geoffrey_Irving2",
            "~Georgios_Piliouras1"
        ],
        "abstract": "The emergence of pre-trained AI systems with powerful capabilities across a diverse and ever-increasing set of complex domains has raised a critical challenge for AI safety as tasks can become too complicated for humans to judge directly. Irving et al (2018). proposed a debate method in this direction with the goal of pitting the power of such AI models against each other until the problem of identifying (mis)-alignment is broken down into a manageable subtask. While the promise of this approach is clear, the original framework was based on the assumption that the honest strategy is able to simulate *deterministic* AI systems for an *exponential* number of steps, limiting its applicability. In this paper, we show how to address these challenges by designing a new set of debate protocols where the honest strategy can always succeed using a simulation of a *polynomial* number of steps, whilst being able to verify the alignment of *stochastic* AI systems, even when the dishonest strategy is allowed to use exponentially many simulation steps.",
        "_bibtex": "@inproceedings{\nbrown-cohen2024scalable,\ntitle={Scalable {AI} Safety via Doubly-Efficient Debate},\nauthor={Jonah Brown-Cohen and Geoffrey Irving and Georgios Piliouras},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=6jmdOTRMIO}\n}"
    },
    {
        "title": "OMPO: A Unified Framework for RL under Policy and Dynamics Shifts",
        "authorids": [
            "~Yu_Luo5",
            "~Tianying_Ji2",
            "~Fuchun_Sun1",
            "~Jianwei_Zhang2",
            "~Huazhe_Xu1",
            "~Xianyuan_Zhan1"
        ],
        "abstract": "Training reinforcement learning policies using environment interaction data collected from varying policies or dynamics presents a fundamental challenge. Existing works often overlook the distribution discrepancies induced by policy or dynamics shifts, or rely on specialized algorithms with task priors, thus often resulting in suboptimal policy performances and high learning variances. In this paper, we identify a unified strategy for online RL policy learning under diverse settings of policy and dynamics shifts: transition occupancy matching. In light of this, we introduce a surrogate policy learning objective by considering the transition occupancy discrepancies and then cast it into a tractable min-max optimization problem through dual reformulation. Our method, dubbed Occupancy-Matching Policy Optimization (OMPO), features a specialized actor-critic structure equipped with a distribution discriminator and a small-size local buffer. We conduct extensive experiments based on the OpenAI Gym, Meta-World, and Panda Robots environments, encompassing policy shifts under stationary and non-stationary dynamics, as well as domain adaption. The results demonstrate that OMPO outperforms the specialized baselines from different categories in all settings. We also find that OMPO exhibits particularly strong performance when combined with domain randomization, highlighting its potential in RL-based robotics applications.",
        "_bibtex": "@inproceedings{\nluo2024ompo,\ntitle={{OMPO}: A Unified Framework for {RL} under Policy and Dynamics Shifts},\nauthor={Yu Luo and Tianying Ji and Fuchun Sun and Jianwei Zhang and Huazhe Xu and Xianyuan Zhan},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=R83VIZtHXA}\n}"
    },
    {
        "title": "Neural Collapse meets Differential Privacy: Curious behaviors of NoisyGD with Near-Perfect Representation Learning",
        "authorids": [
            "~Chendi_Wang2",
            "~Yuqing_Zhu1",
            "~Weijie_J_Su1",
            "~Yu-Xiang_Wang1"
        ],
        "abstract": "A recent study by De et al. (2022) shows that large-scale representation learning through pre-training on a public dataset significantly enhances differentially private (DP) learning in downstream tasks. To explain this, we consider a layer-peeled model in representation learning, resulting in Neural Collapse (NC) phenomena. Within NC, we establish that the misclassification error is independent of dimension when the distance between actual and ideal features is below a threshold. We empirically evaluate feature quality in the last layer under different pre-trained models, showing that a more powerful pre-trained model improves feature representation. Moreover, we show that DP fine-tuning is less robust compared to non-DP fine-tuning, especially with perturbations. Supported by theoretical analyses and experiments, we suggest strategies like feature normalization and dimension reduction methods such as PCA to enhance DP fine-tuning robustness. Conducting PCA on last-layer features significantly improves testing accuracy.",
        "_bibtex": "@inproceedings{\nwang2024neural,\ntitle={Neural Collapse meets Differential Privacy: Curious behaviors of Noisy{GD} with Near-Perfect Representation Learning},\nauthor={Chendi Wang and Yuqing Zhu and Weijie J Su and Yu-Xiang Wang},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=7rrN6E4KU0}\n}"
    },
    {
        "title": "Differentiable Mapper for Topological Optimization of Data Representation",
        "authorids": [
            "ziyad.oulhaj@ec-nantes.fr",
            "~Mathieu_Carri\u00e8re1",
            "~Bertrand_Michel2"
        ],
        "abstract": "Unsupervised data representation and visualization using tools from topology is an active and growing field of Topological Data Analysis (TDA) and data science. Its most prominent line of work is based on the so-called Mapper graph, which is a combinatorial graph whose topological structures (connected components, branches, loops) are in correspondence with those of the data itself. While highly generic and applicable, its use has been hampered so far by the manual tuning of its many parameters\u2014among these, a crucial one is the so-called filter: it is a continuous function whose variations on the data set are the main ingredient for both building the Mapper representation and assessing the presence and sizes of its topological structures. However, while a few parameter tuning methods have already been investigated for the other Mapper parameters (i.e., resolution, gain, clustering), there is currently no method for tuning the filter itself. In this work, we build on a recently proposed optimization framework incorporating topology to provide the first filter optimization scheme for Mapper graphs. In order to achieve this, we propose a relaxed and more general version of the Mapper graph, whose convergence properties are investigated. Finally, we demonstrate the usefulness of our approach by optimizing Mapper graph representations on several datasets, and showcasing the superiority of the optimized representation over arbitrary ones.",
        "_bibtex": "@inproceedings{\noulhaj2024differentiable,\ntitle={Differentiable Mapper for Topological Optimization of Data Representation},\nauthor={Ziyad Oulhaj and Mathieu Carri{\\`e}re and Bertrand Michel},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=QZ1DVzr6N9}\n}"
    },
    {
        "title": "ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum Joint Shrinking",
        "authorids": [
            "~Wenshuo_Li2",
            "~Xinghao_Chen1",
            "~Han_Shu1",
            "~Yehui_Tang1",
            "~Yunhe_Wang1"
        ],
        "abstract": "Large language models (LLM) have recently attracted significant attention in the field of artificial intelligence. However, the training process of these models poses significant challenges in terms of computational and storage capacities, thus compressing checkpoints has become an urgent problem. In this paper, we propose a novel Extreme Checkpoint Compression (ExCP) framework, which significantly reduces the required storage of training checkpoints while achieving nearly lossless performance. We first calculate the residuals of adjacent checkpoints to obtain the essential but sparse information for higher compression ratio. To further excavate the redundancy parameters in checkpoints, we then propose a weight-momentum joint shrinking method to utilize another important information during the model optimization, i.e., momentum. In particular, we exploit the information of both model and optimizer to discard as many parameters as possible while preserving critical information to ensure optimal performance. Furthermore, we utilize non-uniform quantization to further compress the storage of checkpoints. We extensively evaluate our proposed ExCP framework on several models ranging from 410M to 7B parameters and demonstrate significant storage reduction while maintaining strong performance. For instance, we achieve approximately $70\\times$ compression for the Pythia-410M model, with the final performance being as accurate as the original model on various downstream tasks. Codes will be available at https://github.com/Gaffey/ExCP.",
        "_bibtex": "@inproceedings{\nli2024excp,\ntitle={Ex{CP}: Extreme {LLM} Checkpoint Compression via Weight-Momentum Joint Shrinking},\nauthor={Wenshuo Li and Xinghao Chen and Han Shu and Yehui Tang and Yunhe Wang},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=hlvKd7Vdxm}\n}"
    },
    {
        "title": "Towards Optimal Adversarial Robust Q-learning with Bellman Infinity-error",
        "authorids": [
            "~Haoran_Li17",
            "~Zicheng_Zhang3",
            "~Wang_Luo1",
            "~Congying_Han1",
            "~Yudong_Hu1",
            "~Tiande_Guo1",
            "~Shichen_Liao1"
        ],
        "abstract": "Establishing robust policies is essential to counter attacks or disturbances affecting deep reinforcement learning (DRL) agents. Recent studies explore state-adversarial robustness and suggest the potential lack of an optimal robust policy (ORP), posing challenges in setting strict robustness constraints. This work further investigates ORP: At first, we introduce a consistency assumption of policy (CAP) stating that optimal actions in the Markov decision process remain consistent with minor perturbations, supported by empirical and theoretical evidence. Building upon CAP, we crucially prove the existence of a deterministic and stationary ORP that aligns with the Bellman optimal policy. Furthermore, we illustrate the necessity of $L^{\\infty}$-norm when minimizing Bellman error to attain ORP. This finding clarifies the vulnerability of prior DRL algorithms that target the Bellman optimal policy with $L^{1}$-norm and motivates us to train a Consistent Adversarial Robust Deep Q-Network (CAR-DQN) by minimizing a surrogate of Bellman Infinity-error. The top-tier performance of CAR-DQN across various benchmarks validates its practical effectiveness and reinforces the soundness of our theoretical analysis.",
        "_bibtex": "@inproceedings{\nli2024towards,\ntitle={Towards Optimal Adversarial Robust Q-learning with Bellman Infinity-error},\nauthor={Haoran Li and Zicheng Zhang and Wang Luo and Congying Han and Yudong Hu and Tiande Guo and Shichen Liao},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=pgI9inG2Ny}\n}"
    },
    {
        "title": "Pruned Pivot: Correlation Clustering Algorithm for Dynamic, Parallel, and Local Computation Models",
        "authorids": [
            "~Mina_Dalirrooyfard1",
            "~Konstantin_Makarychev1",
            "~Slobodan_Mitrovic1"
        ],
        "abstract": "Given a graph with positive and negative edge labels, the correlation clustering problem aims to cluster the nodes so to minimize the total number of between-cluster positive and within-cluster negative edges. This problem has many applications in data mining, particularly in unsupervised learning. Inspired by the prevalence of large graphs and constantly changing data in modern applications, we study correlation clustering in dynamic, parallel (MPC), and local computation (LCA) settings. We design an approach that improves state-of-the-art runtime complexities in all these settings. In particular, we provide the first fully dynamic algorithm that runs in an expected amortized constant time, without any dependence on the graph size. Moreover, our algorithm essentially matches the approximation guarantee of the celebrated Pivot algorithm.",
        "_bibtex": "@inproceedings{\ndalirrooyfard2024pruned,\ntitle={Pruned Pivot: Correlation Clustering Algorithm for Dynamic, Parallel, and Local Computation Models},\nauthor={Mina Dalirrooyfard and Konstantin Makarychev and Slobodan Mitrovic},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=saP7s0ZgYE}\n}"
    },
    {
        "title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
        "authorids": [
            "~Dongping_Chen1",
            "~Ruoxi_Chen1",
            "~Shilin_Zhang2",
            "~Yaochen_Wang1",
            "~Yinuo_Liu1",
            "~Huichi_Zhou1",
            "~Qihui_Zhang1",
            "~Yao_Wan2",
            "~Pan_Zhou5",
            "~Lichao_Sun1"
        ],
        "abstract": "Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence multimodal benchmarks that align with human preferences. Drawing inspiration from the concept of LLM-as-a-Judge within LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges across diverse modalities, encompassing three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparisons, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking tasks. Furthermore, a closer examination reveals persistent challenges in the evaluative capacities of LLMs, including diverse biases, hallucinatory responses, and inconsistencies in judgment, even in advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further research efforts to be undertaken before regarding MLLMs as fully reliable evaluators. In light of this, we advocate for additional efforts dedicated to supporting the continuous development within the domain of MLLM functioning as judges. The code and dataset are publicly available at our project homepage: https://mllm-judge.github.io/.",
        "_bibtex": "@inproceedings{\nchen2024mllmasajudge,\ntitle={{MLLM}-as-a-Judge: Assessing Multimodal {LLM}-as-a-Judge with Vision-Language Benchmark},\nauthor={Dongping Chen and Ruoxi Chen and Shilin Zhang and Yaochen Wang and Yinuo Liu and Huichi Zhou and Qihui Zhang and Yao Wan and Pan Zhou and Lichao Sun},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=dbFEFHAD79}\n}"
    },
    {
        "title": "CompeteAI: Understanding the Competition Dynamics of Large Language Model-based Agents",
        "authorids": [
            "~Qinlin_Zhao1",
            "~Jindong_Wang1",
            "~Yixuan_Zhang7",
            "~Yiqiao_Jin1",
            "~Kaijie_Zhu1",
            "~Hao_Chen15",
            "~Xing_Xie3"
        ],
        "abstract": "Large language models (LLMs) have been widely used as agents to complete different tasks, such as personal assistance or event planning. Although most of the work has focused on cooperation and collaboration between agents, little work explores *competition*, another important mechanism that promotes the development of society and economy. In this paper, we seek to examine the competition dynamics in LLM-based agents. We first propose a general framework for studying the competition between agents. Then, we implement a practical competitive environment using GPT-4 to simulate a virtual town with two types of agents, including restaurant agents and customer agents. Specifically, the restaurant agents compete with each other to attract more customers, where competition encourages them to transform, such as cultivating new operating strategies. Simulation experiments reveal several interesting findings at the micro and macro levels, which align well with existing market and sociological theories. We hope that the framework and environment can be a promising testbed to study the competition that fosters understanding of society. Code is available at: https://github.com/microsoft/competeai.",
        "_bibtex": "@inproceedings{\nzhao2024competeai,\ntitle={Compete{AI}: Understanding the Competition Dynamics of Large Language Model-based Agents},\nauthor={Qinlin Zhao and Jindong Wang and Yixuan Zhang and Yiqiao Jin and Kaijie Zhu and Hao Chen and Xing Xie},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=wGtzp4ZT1n}\n}"
    },
    {
        "title": "Robustness of Nonlinear Representation Learning",
        "authorids": [
            "~Simon_Buchholz1",
            "~Bernhard_Sch\u00f6lkopf1"
        ],
        "abstract": "We study the problem of unsupervised representation learning in slightly misspecified settings, and thus formalize the study of robustness of nonlinear representation learning. We focus on the case where the mixing is close to a local isometry in a suitable distance and show based on existing rigidity results that the mixing can be identified up to linear transformations and small errors. In a second step, we investigate Independent Component Analysis (ICA) with observations generated according to $x=f(s)=As+h(s)$ where $A$ is an invertible mixing matrix and $h$ a small perturbation. We show that we can approximately recover the matrix $A$ and the independent components. Together, these two results show approximate identifiability of nonlinear ICA with almost isometric mixing functions. Those results are a step towards identifiability results for unsupervised representation learning for real-world data that do not follow restrictive model classes.",
        "_bibtex": "@inproceedings{\nbuchholz2024robustness,\ntitle={Robustness of Nonlinear Representation Learning},\nauthor={Simon Buchholz and Bernhard Sch{\\\"o}lkopf},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=GyV33H5Uuk}\n}"
    },
    {
        "title": "S$\\Omega$I: Score-based O-INFORMATION Estimation",
        "authorids": [
            "~Mustapha_BOUNOUA1",
            "~Giulio_Franzese1",
            "~Pietro_Michiardi1"
        ],
        "abstract": "The analysis of scientific data and complex multivariate systems requires information quantities that capture relationships among multiple random variables. Recently, new information-theoretic measures have been developed to overcome the shortcomings of classical ones, such as mutual information, that are restricted to considering pairwise interactions. Among them, the concept of information synergy and redundancy is crucial for understanding the high-order dependencies between variables. One of the most prominent and versatile measures based on this concept is *O-information*, which provides a clear and scalable way to quantify the synergy-redundancy balance in multivariate systems. However, its practical application is limited to simplified cases. In this work, we introduce **S$\\Omega$I**, which allows to compute *O-information* without restrictive assumptions about the system while leveraging a unique model. Our experiments validate our approach on synthetic data, and demonstrate the effectiveness of **S$\\Omega$I** in the context of a real-world use case.",
        "_bibtex": "@inproceedings{\nbounoua2024somegai,\ntitle={S\\${\\textbackslash}Omega\\$I: Score-based O-{INFORMATION} Estimation},\nauthor={Mustapha BOUNOUA and Giulio Franzese and Pietro Michiardi},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=LuhWZ2oJ5L}\n}"
    },
    {
        "title": "Rate-Optimal Policy Optimization for Linear Markov Decision Processes",
        "authorids": [
            "~Uri_Sherman1",
            "~Alon_Cohen1",
            "~Tomer_Koren1",
            "~Yishay_Mansour2"
        ],
        "abstract": "We study regret minimization in online episodic linear Markov Decision Processes, and propose a policy optimization algorithm that is computationally efficient, and obtains rate optimal $\\widetilde O (\\sqrt K)$ regret where $K$ denotes the number of episodes. Our work is the first to establish the optimal rate (in terms of $K$) of convergence in the stochastic setting with bandit feedback using a policy optimization based approach, and the first to establish the optimal rate in the adversarial setup with full information feedback, for which no algorithm with an optimal rate guarantee was previously known.",
        "_bibtex": "@inproceedings{\nsherman2024rateoptimal,\ntitle={Rate-Optimal Policy Optimization for Linear Markov Decision Processes},\nauthor={Uri Sherman and Alon Cohen and Tomer Koren and Yishay Mansour},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=VJwsDwuiuH}\n}"
    },
    {
        "title": "SAM as the Guide: Mastering Pseudo-Label Refinement in Semi-Supervised Referring Expression Segmentation",
        "authorids": [
            "~Danni_Yang1",
            "~Jiayi_Ji1",
            "~Yiwei_Ma1",
            "~Tianyu_Guo3",
            "~Haowei_Wang1",
            "~Xiaoshuai_Sun3",
            "~Rongrong_Ji5"
        ],
        "abstract": "In this paper, we introduce SemiRES, a semi-supervised framework that effectively leverages a combination of labeled and unlabeled data to perform RES. A significant hurdle in applying semi-supervised techniques to RES is the prevalence of noisy pseudo-labels, particularly at the boundaries of objects. SemiRES incorporates the Segment Anything Model (SAM), renowned for its precise boundary demarcation, to improve the accuracy of these pseudo-labels. Within SemiRES, we offer two alternative matching strategies: IoU-based Optimal Matching (IOM) and Composite Parts Integration (CPI). These strategies are designed to extract the most accurate masks from SAM's output, thus guiding the training of the student model with enhanced precision. In instances where a precise mask cannot be matched from the available candidates, we develop the Pixel-Wise Adjustment (PWA) strategy, guiding the student model's training directly by the pseudo-labels. Extensive experiments on three RES benchmarks\u2014RefCOCO, RefCOCO+, and G-Ref reveal its superior performance compared to fully supervised methods, especially in low-data scenarios. Remarkably, with only 1% labeled data, our SemiRES outperforms the supervised baseline by a large margin, e.g. +18.64% gains on RefCOCO val set.",
        "_bibtex": "@inproceedings{\nyang2024sam,\ntitle={{SAM} as the Guide: Mastering Pseudo-Label Refinement in Semi-Supervised Referring Expression Segmentation},\nauthor={Danni Yang and Jiayi Ji and Yiwei Ma and Tianyu Guo and Haowei Wang and Xiaoshuai Sun and Rongrong Ji},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=M5kn9NKIs4}\n}"
    },
    {
        "title": "Position: Near to Mid-term Risks and Opportunities of Open-Source Generative AI",
        "authorids": [
            "~Francisco_Eiras1",
            "~Aleksandar_Petrov1",
            "~Bertie_Vidgen1",
            "~Christian_Schroeder_de_Witt1",
            "~Fabio_Pizzati1",
            "~Katherine_Elkins1",
            "~Supratik_Mukhopadhyay2",
            "~Adel_Bibi1",
            "~Botos_Csaba1",
            "ofabro@itsrio.org",
            "~Fazl_Barez1",
            "~Genevieve_Smith1",
            "~Gianluca_Guadagni1",
            "~Jon_Chun1",
            "~Jordi_Cabot1",
            "~Joseph_Marvin_Imperial1",
            "~Juan_A._Nolazco-Flores1",
            "~Lori_Landay1",
            "~Matthew_Thomas_Jackson1",
            "~Paul_Rottger1",
            "~Philip_Torr1",
            "~Trevor_Darrell2",
            "~Yong_Suk_Lee1",
            "~Jakob_Nicolaus_Foerster1"
        ],
        "abstract": "In the next few years, applications of Generative AI are expected to revolutionize a number of different areas, ranging from science & medicine to education. The potential for these seismic changes has triggered a lively debate about potential risks and resulted in calls for tighter regulation, in particular from some of the major tech companies who are leading in AI development. While regulation is important, it is key that it does not put at risk the budding field of open-source Generative AI. We argue for the responsible open sourcing of generative AI models in the near and medium term. To set the stage, we first introduce an AI openness taxonomy system and apply it to 40 current large language models. We then outline differential benefits and risks of open versus closed source AI and present potential risk mitigation, ranging from best practices to calls for technical and scientific contributions. We hope that this report will add a much needed missing voice to the current public discourse on near to mid-term AI safety and other societal impact.",
        "_bibtex": "@inproceedings{\neiras2024position,\ntitle={Position: Near to Mid-term Risks and Opportunities of Open-Source Generative {AI}},\nauthor={Francisco Eiras and Aleksandar Petrov and Bertie Vidgen and Christian Schroeder de Witt and Fabio Pizzati and Katherine Elkins and Supratik Mukhopadhyay and Adel Bibi and Botos Csaba and Fabro Steibel and Fazl Barez and Genevieve Smith and Gianluca Guadagni and Jon Chun and Jordi Cabot and Joseph Marvin Imperial and Juan A. Nolazco-Flores and Lori Landay and Matthew Thomas Jackson and Paul Rottger and Philip Torr and Trevor Darrell and Yong Suk Lee and Jakob Nicolaus Foerster},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=8q4EPdjTLE}\n}"
    },
    {
        "title": "Repoformer: Selective Retrieval for Repository-Level Code Completion",
        "authorids": [
            "~Di_Wu14",
            "~Wasi_Uddin_Ahmad1",
            "~Dejiao_Zhang1",
            "~Murali_Krishna_Ramanathan1",
            "~Xiaofei_Ma1"
        ],
        "abstract": "Recent advances in retrieval-augmented generation (RAG) have initiated a new era in repository-level code completion. However, the invariable use of retrieval in existing methods exposes issues in both efficiency and robustness, with a large proportion of the retrieved contexts proving unhelpful or harmful to code language models (code LMs). In this paper, we propose a selective RAG framework to avoid retrieval when unnecessary. To power this framework, we design a self-supervised learning approach to enable a code LM to accurately self-evaluate whether retrieval can improve its output quality and robustly leverage the potentially noisy retrieved contexts. Using this LM as both the selective RAG policy and the generation model, our framework achieves state-of-the-art repository-level code completion performance on diverse benchmarks including RepoEval, CrossCodeEval, and CrossCodeLongEval, a new long-form code completion benchmark. Meanwhile, our analyses show that selectively retrieving brings as much as 70% inference speedup in the online serving setting without harming the performance. We further demonstrate that our framework is able to accommodate different generation models, retrievers, and programming languages. These advancements position our framework as an important step towards more accurate and efficient repository-level code completion.",
        "_bibtex": "@inproceedings{\nwu2024repoformer,\ntitle={Repoformer: Selective Retrieval for Repository-Level Code Completion},\nauthor={Di Wu and Wasi Uddin Ahmad and Dejiao Zhang and Murali Krishna Ramanathan and Xiaofei Ma},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=moyG54Okrj}\n}"
    },
    {
        "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
        "authorids": [
            "~Patrick_Esser1",
            "~Sumith_Kulal1",
            "~Andreas_Blattmann1",
            "~Rahim_Entezari1",
            "~Jonas_M\u00fcller1",
            "~Harry_Saini1",
            "~Yam_Levi1",
            "~Dominik_Lorenz1",
            "~Axel_Sauer1",
            "~Frederic_Boesel1",
            "~Dustin_Podell1",
            "~Tim_Dockhorn1",
            "~Zion_English1",
            "~Robin_Rombach1"
        ],
        "abstract": "Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models. Stability AI is considering making experimental data, code, and model weights publicly available.",
        "_bibtex": "@inproceedings{\nesser2024scaling,\ntitle={Scaling Rectified Flow Transformers for High-Resolution Image Synthesis},\nauthor={Patrick Esser and Sumith Kulal and Andreas Blattmann and Rahim Entezari and Jonas M{\\\"u}ller and Harry Saini and Yam Levi and Dominik Lorenz and Axel Sauer and Frederic Boesel and Dustin Podell and Tim Dockhorn and Zion English and Robin Rombach},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=FPnUhsQJ5B}\n}"
    },
    {
        "title": "Position: On the Societal Impact of Open Foundation Models",
        "authorids": [
            "~Sayash_Kapoor2",
            "~Rishi_Bommasani1",
            "~Kevin_Klyman1",
            "~Shayne_Longpre1",
            "aramaswamis@gmail.com",
            "pcihon@github.com",
            "~Aspen_K_Hopkins1",
            "kbankston@cdt.org",
            "~Stella_Biderman1",
            "~Miranda_Bogen1",
            "~Rumman_Chowdhury1",
            "alexcengler@gmail.com",
            "~Peter_Henderson1",
            "~Yacine_Jernite1",
            "~Seth_Lazar1",
            "stefano@opensource.org",
            "anelson@ias.edu",
            "~Joelle_Pineau1",
            "~Aviya_Skowron1",
            "~Dawn_Song1",
            "~Victor_Storchan1",
            "dzhang105@stanford.edu",
            "~Daniel_E._Ho1",
            "~Percy_Liang1",
            "~Arvind_Narayanan1"
        ],
        "abstract": "Foundation models are powerful technologies: how they are released publicly directly shapes their societal impact. In this position paper, we focus on *open* foundation models, defined here as those with broadly available model weights (e.g., Llama 3, Stable Diffusion XL). We identify five distinctive properties (e.g., greater customizability, poor monitoring) that mediate their benefits and risks. Open foundation models present significant benefits, with some caveats, that span innovation, competition, the distribution of decision-making power, and transparency. To understand their risks of misuse, we design a risk assessment framework for analyzing their *marginal risk*. Across several misuse vectors (e.g., cyberattacks, bioweapons), we find that current research is insufficient to effectively characterize the marginal risk of open foundation models relative to pre-existing technologies. The framework helps explain why the marginal risk is low in some cases, clarifies disagreements about misuse risks by revealing that past work has focused on different subsets of the framework with different assumptions, and articulates a way forward for more constructive debate. Overall, our work helps support a more grounded assessment of the societal impact of open foundation models by outlining what research is needed to empirically validate their theoretical benefits and risks.",
        "_bibtex": "@inproceedings{\nkapoor2024position,\ntitle={Position Paper: On the Societal Impact of Open Foundation Models},\nauthor={Sayash Kapoor and Rishi Bommasani and Kevin Klyman and Shayne Longpre and Ashwin Ramaswami and Peter Cihon and Aspen K Hopkins and Kevin Bankston and Stella Biderman and Miranda Bogen and Rumman Chowdhury and Alex Engler and Peter Henderson and Yacine Jernite and Seth Lazar and Stefano Maffulli and Alondra Nelson and Joelle Pineau and Aviya Skowron and Dawn Song and Victor Storchan and Daniel Zhang and Daniel E. Ho and Percy Liang and Arvind Narayanan},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=jRX6yCxFhx}\n}"
    },
    {
        "title": "DITTO: Diffusion Inference-Time T-Optimization for Music Generation",
        "authorids": [
            "~Zachary_Novack1",
            "~Julian_McAuley1",
            "~Taylor_Berg-Kirkpatrick1",
            "~Nicholas_J._Bryan1"
        ],
        "abstract": "We propose Diffusion Inference-Time T-Optimization (DITTO), a general-purpose framework for controlling pre-trained text-to-music diffusion models at inference-time via optimizing initial noise latents. Our method can be used to optimize through any differentiable feature matching loss to achieve a target (stylized) output and leverages gradient checkpointing for memory efficiency. We demonstrate a surprisingly wide-range of applications for music generation including inpainting, outpainting, and looping as well as intensity, melody, and musical structure control \u2013 all without ever fine-tuning the underlying model. When we compare our approach against related training, guidance, and optimization-based methods, we find DITTO achieves state-of-the-art performance on nearly all tasks, including outperforming comparable approaches on controllability, audio quality, and computational efficiency, thus opening the door for high-quality, flexible, training-free control of diffusion models. Sound examples can be found at https://ditto-music.github.io/web/.",
        "_bibtex": "@inproceedings{\nnovack2024ditto,\ntitle={{DITTO}: Diffusion Inference-Time T-Optimization for Music Generation},\nauthor={Zachary Novack and Julian McAuley and Taylor Berg-Kirkpatrick and Nicholas J. Bryan},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=z5Ux2u6t7U}\n}"
    },
    {
        "title": "Parameterized Physics-informed Neural Networks for Parameterized PDEs",
        "authorids": [
            "~Woojin_Cho1",
            "~Minju_Jo1",
            "~Haksoo_Lim1",
            "~Kookjin_Lee1",
            "~Dongeun_Lee1",
            "~Sanghyun_Hong1",
            "~Noseong_Park1"
        ],
        "abstract": "Complex physical systems are often described by partial differential equations (PDEs) that depend on parameters such as the Raynolds number in fluid mechanics. In applications such as design optimization or uncertainty quantification, solutions of those PDEs need to be evaluated at numerous points in the parameter space. While physics-informed neural networks (PINNs) have emerged as a new strong competitor as a surrogate, their usage in this scenario remains underexplored due to the inherent need for repetitive and time-consuming training. In this paper, we address this problem by proposing a novel extension, parameterized physics-informed neural networks (P$^2$INNs). P$^2$INNs enable modeling the solutions of parameterized PDEs via explicitly encoding a latent representation of PDE parameters. With the extensive empirical evaluation, we demonstrate that P$^2$INNs outperform the baselines both in accuracy and parameter efficiency on benchmark 1D and 2D parameterized PDEs and are also effective in overcoming the known \u201cfailure modes\u201d.",
        "_bibtex": "@inproceedings{\ncho2024parameterized,\ntitle={Parameterized Physics-informed Neural Networks for Parameterized {PDE}s},\nauthor={Woojin Cho and Minju Jo and Haksoo Lim and Kookjin Lee and Dongeun Lee and Sanghyun Hong and Noseong Park},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=n3yYrtt9U7}\n}"
    },
    {
        "title": "Position: Rethinking Post-Hoc Search-Based Neural Approaches for Solving Large-Scale Traveling Salesman Problems",
        "authorids": [
            "~Yifan_Xia2",
            "~Xianliang_Yang1",
            "~Zichuan_Liu3",
            "~Zhihao_Liu3",
            "~Lei_Song3",
            "~Jiang_Bian1"
        ],
        "abstract": "Recent advancements in solving large-scale traveling salesman problems (TSP) utilize the heatmap-guided Monte Carlo tree search (MCTS) paradigm, where machine learning (ML) models generate heatmaps, indicating the probability distribution of each edge being part of the optimal solution, to guide MCTS in solution finding. However, our theoretical and experimental analysis raises doubts about the effectiveness of ML-based heatmap generation. In support of this, we demonstrate that a simple baseline method can outperform complex ML approaches in heatmap generation. Furthermore, we question the practical value of the heatmap-guided MCTS paradigm. To substantiate this, our findings show its inferiority to the LKH-3 heuristic despite the paradigm's reliance on problem-specific, hand-crafted strategies. For the future, we suggest research directions focused on developing more theoretically sound heatmap generation methods and exploring autonomous, generalizable ML approaches for combinatorial problems. The code is available for review: https://github.com/xyfffff/rethink_mcts_for_tsp.",
        "_bibtex": "@inproceedings{\nxia2024position,\ntitle={Position: Rethinking Post-Hoc Search-Based Neural Approaches for Solving Large-Scale Traveling Salesman Problems},\nauthor={Yifan Xia and Xianliang Yang and Zichuan Liu and Zhihao Liu and Lei Song and Jiang Bian},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=cEJ9jNJuJP}\n}"
    },
    {
        "title": "Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific Exam Generation",
        "authorids": [
            "~Gauthier_Guinet1",
            "omidvart@amazon.com",
            "~Anoop_Deoras1",
            "~Laurent_Callot1"
        ],
        "abstract": "We propose a new method to measure the task-specific accuracy of Retrieval-Augmented Large Language Models (RAG). Evaluation is performed by scoring the RAG on an automatically-generated synthetic exam composed of multiple choice questions based on the corpus of documents associated with the task. Our method is an automated, cost-efficient, interpretable, and robust strategy to select the optimal components for a RAG system. We leverage Item Response Theory (IRT) to estimate the quality of an exam and its informativeness on task-specific accuracy. IRT also provides a natural way to iteratively improve the exam by eliminating the exam questions that are not sufficiently informative about a model's ability. We demonstrate our approach on four new open-ended Question-Answering tasks based on Arxiv abstracts, StackExchange questions, AWS DevOps troubleshooting guides, and SEC filings. In addition, our experiments reveal more general insights into factors impacting RAG performance like size, retrieval mechanism, prompting and fine-tuning. Most notably, our findings show that choosing the right retrieval algorithms often leads to bigger performance gains than simply using a larger language model.",
        "_bibtex": "@inproceedings{\nguinet2024automated,\ntitle={Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific Exam Generation},\nauthor={Gauthier Guinet and Behrooz Omidvar-Tehrani and Anoop Deoras and Laurent Callot},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=4jqOV6NlUz}\n}"
    },
    {
        "title": "PRISE: LLM-Style Sequence Compression for Learning Temporal Action Abstractions in Control",
        "authorids": [
            "~Ruijie_Zheng1",
            "~Ching-An_Cheng1",
            "~Hal_Daum\u00e9_III1",
            "~Furong_Huang1",
            "~Andrey_Kolobov1"
        ],
        "abstract": "Temporal action abstractions, along with belief state representations, are a powerful knowledge sharing mechanism for sequential decision making. In this work, we propose a novel view that treats inducing temporal action abstractions as a sequence compression problem. To do so, we bring a subtle but critical component of LLM training pipelines -- input tokenization via byte pair encoding (BPE) -- to bear on the seemingly distant task of learning skills of variable time span in continuous control domains. We introduce an approach called Primitive Sequence Encoding (PRISE) that combines continuous action quantization with BPE to learn powerful action abstractions. We empirically show that high-level skills discovered by PRISE from a multitask set of robotic manipulation demonstrations significantly boost the learning performance of behavior cloning on downstream tasks.",
        "_bibtex": "@inproceedings{\nzheng2024prise,\ntitle={{PRISE}: {LLM}-Style Sequence Compression for Learning Temporal Action Abstractions in Control},\nauthor={Ruijie Zheng and Ching-An Cheng and Hal Daum{\\'e} III and Furong Huang and Andrey Kolobov},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=p225Od0aYt}\n}"
    },
    {
        "title": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies",
        "authorids": [
            "~Jia_Shi2",
            "~Gautam_Rajendrakumar_Gare1",
            "~Jinjin_Tian1",
            "~Siqi_Chai1",
            "~Zhiqiu_Lin1",
            "~Arun_Balajee_Vasudevan1",
            "~Di_Feng1",
            "~Francesco_Ferroni1",
            "~Shu_Kong1"
        ],
        "abstract": "We tackle the challenge of predicting models' Out-of-Distribution (OOD) performance using in-distribution (ID) measurements without requiring OOD data. Existing evaluations with ``Effective robustness'', which use ID accuracy as an indicator of OOD accuracy, encounter limitations when models are trained with diverse supervision and distributions, such as class labels (*Vision Models, VMs, on ImageNet*) and textual descriptions (*Visual-Language Models, VLMs, on LAION*). VLMs often generalize better to OOD data than VMs despite having similar or lower ID performance. To improve the prediction of models' OOD performance from ID measurements, we introduce the *Lowest Common Ancestor (LCA)-on-the-Line* framework. This approach revisits the established concept of LCA distance, which measures the hierarchical distance between labels and predictions within a predefined class hierarchy, such as WordNet. We assess 75 models using ImageNet as the ID dataset and five significantly shifted OOD variants, uncovering a strong linear correlation between ID LCA distance and OOD top-1 accuracy. Our method provides a compelling alternative for understanding why VLMs tend to generalize better. Additionally, we propose a technique to construct a taxonomic hierarchy on any dataset using $K$-means clustering, demonstrating that LCA distance is robust to the constructed taxonomic hierarchy. Moreover, we demonstrate that aligning model predictions with class taxonomies, through soft labels or prompt engineering, can enhance model generalization. Open source code in our [Project Page](https://elvishelvis.github.io/papers/lca/).",
        "_bibtex": "@inproceedings{\nshi2024lcaontheline,\ntitle={{LCA}-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies},\nauthor={Jia Shi and Gautam Rajendrakumar Gare and Jinjin Tian and Siqi Chai and Zhiqiu Lin and Arun Balajee Vasudevan and Di Feng and Francesco Ferroni and Shu Kong},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=HPXRzM9BYZ}\n}"
    },
    {
        "title": "Active Statistical Inference",
        "authorids": [
            "~Tijana_Zrnic1",
            "~Emmanuel_Candes1"
        ],
        "abstract": "Inspired by the concept of active learning, we propose active inference---a methodology for statistical inference with machine-learning-assisted data collection. Assuming a budget on the number of labels that can be collected, the methodology uses a machine learning model to identify which data points would be most beneficial to label, thus effectively utilizing the budget. It operates on a simple yet powerful intuition: prioritize the collection of labels for data points where the model exhibits uncertainty, and rely on the model's predictions where it is confident. Active inference constructs valid confidence intervals and hypothesis tests while leveraging any black-box machine learning model and handling any data distribution. The key point is that it achieves the same level of accuracy with far fewer samples than existing baselines relying on non-adaptively-collected data. This means that for the same number of collected samples, active inference enables smaller confidence intervals and more powerful tests. We evaluate active inference on datasets from public opinion research, census analysis, and proteomics.",
        "_bibtex": "@inproceedings{\nzrnic2024active,\ntitle={Active Statistical Inference},\nauthor={Tijana Zrnic and Emmanuel Candes},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=GKMcCtWC7H}\n}"
    },
    {
        "title": "Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion",
        "authorids": [
            "~Yujia_Huang1",
            "adishree@caltech.edu",
            "liuy72@rpi.edu",
            "~Ziniu_Hu1",
            "~Qinsheng_Zhang1",
            "~Chandramouli_Shama_Sastry1",
            "~Siddharth_Gururani1",
            "~Sageev_Oore1",
            "~Yisong_Yue1"
        ],
        "abstract": "We study the problem of symbolic music generation (e.g., generating piano rolls), with a technical focus on non-differentiable rule guidance. Musical rules are often expressed in symbolic form on note characteristics, such as note density or chord progression, many of which are non-differentiable which pose a challenge when using them for guided diffusion. We propose Stochastic Control Guidance (SCG), a novel guidance method that only requires forward evaluation of rule functions that can work with pre-trained diffusion models in a plug-and-play way, thus achieving training-free guidance for non-differentiable rules for the first time. Additionally, we introduce a latent diffusion architecture for symbolic music generation with high time resolution, which can be composed with SCG in a plug-and-play fashion. Compared to standard strong baselines in symbolic music generation, this framework demonstrates marked advancements in music quality and rule-based controllability, outperforming current state-of-the-art generators in a variety of settings. For detailed demonstrations, code and model checkpoints, please visit our [project website](https://scg-rule-guided-music.github.io/).",
        "_bibtex": "@inproceedings{\nhuang2024symbolic,\ntitle={Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion},\nauthor={Yujia Huang and Adishree Ghatare and Yuanzhe Liu and Ziniu Hu and Qinsheng Zhang and Chandramouli Shama Sastry and Siddharth Gururani and Sageev Oore and Yisong Yue},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=g8AigOTNXL}\n}"
    },
    {
        "title": "NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models",
        "authorids": [
            "~Zeqian_Ju1",
            "~Yuancheng_Wang1",
            "~Kai_Shen2",
            "~Xu_Tan1",
            "~Detai_Xin1",
            "~Dongchao_Yang1",
            "~Eric_Liu1",
            "~Yichong_Leng1",
            "~Kaitao_Song1",
            "~Siliang_Tang1",
            "~Zhizheng_Wu1",
            "~Tao_Qin1",
            "~Xiangyang_Li4",
            "~Wei_Ye2",
            "~Shikun_Zhang2",
            "~Jiang_Bian1",
            "~Lei_He6",
            "~Jinyu_Li1",
            "~sheng_zhao1"
        ],
        "abstract": "While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall shorts in speech quality, similarity, and prosody. Considering that speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model, which generates attributes in each subspace following its corresponding prompt. With this factorization design, our method can effectively and efficiently model the intricate speech with disentangled subspaces in a divide-and-conquer way. Experimental results show that our method outperforms the state-of-the-art TTS systems on quality, similarity, prosody, and intelligibility.",
        "_bibtex": "@inproceedings{\nju2024naturalspeech,\ntitle={NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models},\nauthor={Zeqian Ju and Yuancheng Wang and Kai Shen and Xu Tan and Detai Xin and Dongchao Yang and Eric Liu and Yichong Leng and Kaitao Song and Siliang Tang and Zhizheng Wu and Tao Qin and Xiangyang Li and Wei Ye and Shikun Zhang and Jiang Bian and Lei He and Jinyu Li and sheng zhao},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=dVhrnjZJad}\n}"
    },
    {
        "title": "Privacy Preserving Adaptive Experiment Design",
        "authorids": [
            "~Jiachun_Li1",
            "~Kaining_Shi1",
            "~David_Simchi-Levi2"
        ],
        "abstract": "Adaptive experiment is widely adopted to estimate conditional average treatment effect (CATE) in clinical trials and many other scenarios. While the primary goal in experiment is to maximize estimation accuracy, due to the imperative of social welfare, it's also crucial to provide treatment with superior outcomes to patients, which is measured by regret in contextual bandit framework. Furthermore, privacy concerns arise in clinical scenarios containing sensitive data like patients health records. Therefore, it's essential for the treatment allocation mechanism to incorporate robust privacy protection measures. In this paper, we investigate the tradeoff between loss of social welfare and statistical power of CATE estimation in contextual bandit experiment. We propose a matched upper and lower bound for the multi-objective optimization problem, and then adopt the concept of Pareto optimality to mathematically characterize the optimality condition. Furthermore, we propose differentially private algorithms which still matches the lower bound, showing that privacy is \"almost free\". Additionally, we derive the asymptotic normality of the estimator, which is essential in statistical inference and hypothesis testing.",
        "_bibtex": "@inproceedings{\nli2024privacy,\ntitle={Privacy Preserving Adaptive Experiment Design},\nauthor={Jiachun Li and Kaining Shi and David Simchi-Levi},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=1QmFKwVwwI}\n}"
    },
    {
        "title": "Position: Do pretrained Transformers Learn In-Context by Gradient Descent?",
        "authorids": [
            "~Lingfeng_Shen1",
            "~Aayush_Mishra1",
            "~Daniel_Khashabi2"
        ],
        "abstract": "The emergence of In-Context Learning (ICL) in LLMs remains a remarkable phenomenon that is partially understood. To explain ICL, recent studies have created theoretical connections to Gradient Descent (GD). We ask, do such connections hold up in actual pre-trained language models? We highlight the limiting assumptions in prior works that make their setup considerably different from the practical setup in which language models are trained. For example, their experimental verification uses *ICL objective* (training models explicitly for ICL), which differs from the emergent ICL in the wild. Furthermore, the theoretical hand-constructed weights used in these studies have properties that don't match those of real LLMs. We also look for evidence in real models. We observe that ICL and GD have different sensitivity to the order in which they observe demonstrations. Finally, we probe and compare the ICL vs. GD hypothesis in a natural setting. We conduct comprehensive empirical analyses on language models pre-trained on natural data (LLaMa-7B). Our comparisons of three performance metrics highlight the inconsistent behavior of ICL and GD as a function of various factors such as datasets, models, and the number of demonstrations. We observe that ICL and GD modify the output distribution of language models differently. These results indicate that *the equivalence between ICL and GD remains an open hypothesis* and calls for further studies.",
        "_bibtex": "@inproceedings{\nshen2024position,\ntitle={Position: Do pretrained Transformers Learn In-Context by Gradient Descent?},\nauthor={Lingfeng Shen and Aayush Mishra and Daniel Khashabi},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=WsawczEqO6}\n}"
    },
    {
        "title": "Learning to Model the World With Language",
        "authorids": [
            "~Jessy_Lin1",
            "~Yuqing_Du1",
            "~Olivia_Watkins1",
            "~Danijar_Hafner1",
            "~Pieter_Abbeel2",
            "~Dan_Klein1",
            "~Anca_Dragan1"
        ],
        "abstract": "To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents can learn to execute simple language instructions, we aim to build agents that leverage diverse language---language like \"this button turns on the TV\" or \"I put the bowls away\"---that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that *agents should interpret such diverse language as a signal that helps them predict the future*: what they will observe, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts. While current methods that learn language-conditioned policies degrade in performance with more diverse types of language, we show that Dynalang learns to leverage environment descriptions, game rules, and instructions to excel on tasks ranging from game-playing to navigating photorealistic home scans. Finally, we show that our method enables additional capabilities due to learning a generative model: Dynalang can be pretrained on text-only data, enabling learning from offline datasets, and generate language grounded in an environment.",
        "_bibtex": "@inproceedings{\nlin2024learning,\ntitle={Learning to Model the World With Language},\nauthor={Jessy Lin and Yuqing Du and Olivia Watkins and Danijar Hafner and Pieter Abbeel and Dan Klein and Anca Dragan},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=7dP6Yq9Uwv}\n}"
    },
    {
        "title": "Position: AI-Powered Autonomous Weapons Risk Geopolitical Instability and Threaten AI Research",
        "authorids": [
            "~Riley_Simmons-Edler1",
            "~Ryan_Paul_Badman1",
            "~Shayne_Longpre1",
            "~Kanaka_Rajan1"
        ],
        "abstract": "The recent embrace of machine learning (ML) in the development of autonomous weapons systems (AWS) creates serious risks to geopolitical stability and the free exchange of ideas in AI research. This topic has received comparatively little attention of late compared to risks stemming from superintelligent artificial general intelligence (AGI), but requires fewer assumptions about the course of technological development and is thus a nearer-future issue. ML is already enabling the substitution of AWS for human soldiers in many battlefield roles, reducing the upfront human cost, and thus political cost, of waging offensive war. In the case of peer adversaries, this increases the likelihood of \"low intensity\" conflicts which risk escalation to broader warfare. In the case of non-peer adversaries, it reduces the domestic blowback to wars of aggression. This effect can occur regardless of other ethical issues around the use of military AI such as the risk of civilian casualties, and does not require any superhuman AI capabilities. Further, the military value of AWS raises the specter of an AI-powered arms race and the misguided imposition of national security restrictions on AI research. Our goal in this paper is to raise awareness among the public and ML researchers on the near-future risks posed by full or near-full autonomy in military technology, and we provide regulatory suggestions to mitigate these risks. We call upon AI policy experts and the defense AI community in particular to embrace transparency and caution in their development and deployment of AWS to avoid the negative effects on global stability and AI research that we highlight here.",
        "_bibtex": "@inproceedings{\nsimmons-edler2024position,\ntitle={Position: {AI}-Powered Autonomous Weapons Risk Geopolitical Instability and Threaten {AI} Research},\nauthor={Riley Simmons-Edler and Ryan Paul Badman and Shayne Longpre and Kanaka Rajan},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=ZwUThOE7Zc}\n}"
    },
    {
        "title": "Arrows of Time for Large Language Models",
        "authorids": [
            "~Vassilis_Papadopoulos1",
            "~J\u00e9r\u00e9mie_Wenger1",
            "~Cl\u00e9ment_Hongler1"
        ],
        "abstract": "We study the probabilistic modeling performed by Autoregressive Large Language Models (LLMs) through the angle of time directionality, addressing a question first raised in (Shannon, 1951). For large enough models, we empirically find a time asymmetry in their ability to learn natural language: a difference in the average log-perplexity when trying to predict the next token versus when trying to predict the previous one. This difference is at the same time subtle and very consistent across various modalities (language, model size, training time, ...). Theoretically, this is surprising: from an information-theoretic point of view, there should be no such difference. We provide a theoretical framework to explain how such an asymmetry can appear from sparsity and computational complexity considerations, and outline a number of perspectives opened by our results.",
        "_bibtex": "@inproceedings{\npapadopoulos2024arrows,\ntitle={Arrows of Time for Large Language Models},\nauthor={Vassilis Papadopoulos and J{\\'e}r{\\'e}mie Wenger and Cl{\\'e}ment Hongler},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=UpSe7ag34v}\n}"
    },
    {
        "title": "Chain of Code: Reasoning with a Language Model-Augmented Code Emulator",
        "authorids": [
            "~Chengshu_Li1",
            "~Jacky_Liang1",
            "~Andy_Zeng3",
            "~Xinyun_Chen1",
            "~Karol_Hausman2",
            "~Dorsa_Sadigh1",
            "~Sergey_Levine1",
            "~Li_Fei-Fei1",
            "~Fei_Xia1",
            "~brian_ichter1"
        ],
        "abstract": "Code provides a general syntactic structure to build complex programs and perform precise computations when paired with a code interpreter \u2013 we hypothesize that language models (LMs) can leverage code-writing to improve Chain of Thought reasoning not only for logic and arithmetic tasks, but also for semantic ones (and in particular, those that are a mix of both). For example, consider prompting an LM to write code that counts the number of times it detects sarcasm in an essay: the LM may struggle to write an implementation for \"detect_sarcasm(string)\" that can be executed by the interpreter (handling the edge cases would be insurmountable). However, LMs may still produce a valid solution if they not only write code, but also selectively \"emulate\" the interpreter by generating the expected output of \"detect_sarcasm(string)\". In this work, we propose Chain of Code (CoC), a simple yet surprisingly effective extension that improves LM code-driven reasoning. The key idea is to encourage LMs to format semantic sub-tasks in a program as flexible pseudocode that the interpreter can explicitly catch undefined behaviors and hand off to simulate with an LM (as an \"LMulator\"). Experiments demonstrate that Chain of Code outperforms Chain of Thought and other baselines across a variety of benchmarks; on BIG-Bench Hard, Chain of Code achieves 84%, a gain of 12% over Chain of Thought. In a nutshell, CoC broadens the scope of reasoning questions that LMs can answer by \"thinking in code\".",
        "_bibtex": "@inproceedings{\nli2024chain,\ntitle={Chain of Code: Reasoning with a Language Model-Augmented Code Emulator},\nauthor={Chengshu Li and Jacky Liang and Andy Zeng and Xinyun Chen and Karol Hausman and Dorsa Sadigh and Sergey Levine and Li Fei-Fei and Fei Xia and brian ichter},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=vKtomqlSxm}\n}"
    },
    {
        "title": "Challenges in Training PINNs: A Loss Landscape Perspective",
        "authorids": [
            "~Pratik_Rathore1",
            "~Weimu_Lei1",
            "~Zachary_Frangella1",
            "~Lu_Lu1",
            "~Madeleine_Udell1"
        ],
        "abstract": "This paper explores challenges in training Physics-Informed Neural Networks (PINNs), emphasizing the role of the loss landscape in the training process. We examine difficulties in minimizing the PINN loss function, particularly due to ill-conditioning caused by differential operators in the residual term. We compare gradient-based optimizers Adam, L-BFGS, and their combination Adam+L-BFGS, showing the superiority of Adam+L-BFGS, and introduce a novel second-order optimizer, NysNewton-CG (NNCG), which significantly improves PINN performance. Theoretically, our work elucidates the connection between ill-conditioned differential operators and ill-conditioning in the PINN loss and shows the benefits of combining first- and second-order optimization methods. Our work presents valuable insights and more powerful optimization strategies for training PINNs, which could improve the utility of PINNs for solving difficult partial differential equations.",
        "_bibtex": "@inproceedings{\nrathore2024challenges,\ntitle={Challenges in Training {PINN}s: A Loss Landscape Perspective},\nauthor={Pratik Rathore and Weimu Lei and Zachary Frangella and Lu Lu and Madeleine Udell},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=mJGiFr8jLa}\n}"
    },
    {
        "title": "AI Control: Improving Safety Despite Intentional Subversion",
        "authorids": [
            "~Ryan_Greenblatt1",
            "~Buck_Shlegeris1",
            "~Kshitij_Sachan1",
            "~Fabien_Roger1"
        ],
        "abstract": "As large language models (LLMs) become more powerful and are deployed more autonomously, it will be increasingly important to prevent them from causing harmful outcomes. To do so, safety measures either aim at making LLMs try to avoid harmful outcomes or aim at preventing LLMs from causing harmful outcomes, even if they try to cause them. In this paper, we focus on this second layer of defense. We develop and evaluate pipelines of safety techniques (protocols) that try to ensure safety despite intentional subversion - an approach we call AI control. We investigate a setting in which we want to solve a sequence of programming problems without ever submitting subtly wrong code, using access to a powerful but untrusted model (in our case, GPT-4), access to a less powerful trusted model (in our case, GPT-3.5), and limited access to high-quality trusted labor. We investigate a range of protocols and red-team them by exploring strategies that the untrusted model could use to subvert them. We find that using the trusted model to edit untrusted-model code or using the untrusted model as a monitor substantially improves on simple baselines.",
        "_bibtex": "@inproceedings{\ngreenblatt2024ai,\ntitle={{AI} Control: Improving Safety Despite Intentional Subversion},\nauthor={Ryan Greenblatt and Buck Shlegeris and Kshitij Sachan and Fabien Roger},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=KviM5k8pcP}\n}"
    },
    {
        "title": "APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference",
        "authorids": [
            "~Bowen_Zhao3",
            "~Hannaneh_Hajishirzi1",
            "~Qingqing_Cao1"
        ],
        "abstract": "Fine-tuning and inference with large Language Models (LM) are generally known to be expensive. Parameter-efficient fine-tuning over pretrained LMs reduces training memory by updating a small number of LM parameters but does not improve inference efficiency. Structured pruning improves LM inference efficiency by removing consistent parameter blocks, yet often increases training memory and time. To improve both training and inference efficiency, we introduce APT that adaptively *prunes* and *tunes* parameters for the LMs. At the early stage of fine-tuning, APT dynamically adds *salient* tuning parameters for fast and accurate convergence while discarding unimportant parameters for efficiency. Compared to baselines, our experiments show that APT maintains up to 98% task performance when pruning RoBERTa and T5 models with 40% parameters left while keeping 86.4% LLaMA models' performance with 70% parameters remaining. Furthermore, APT speeds up LMs' fine-tuning by up to 8$\\times$ and reduces large LMs' memory training footprint by up to 70%. Our code and models are publicly available at https://github.com/ROIM1998/APT.",
        "_bibtex": "@inproceedings{\nzhao2024apt,\ntitle={{APT}: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference},\nauthor={Bowen Zhao and Hannaneh Hajishirzi and Qingqing Cao},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=sb81Xl50JG}\n}"
    },
    {
        "title": "Offline Actor-Critic Reinforcement Learning Scales to Large Models",
        "authorids": [
            "~Jost_Tobias_Springenberg1",
            "~Abbas_Abdolmaleki3",
            "~Jingwei_Zhang2",
            "~Oliver_Groth1",
            "~Michael_Bloesch1",
            "~Thomas_Lampe1",
            "~Philemon_Brakel1",
            "~Sarah_Maria_Elisabeth_Bechtle1",
            "~Steven_Kapturowski1",
            "~Roland_Hafner1",
            "~Nicolas_Heess1",
            "~Martin_Riedmiller1"
        ],
        "abstract": "We show that offline actor-critic reinforcement learning can scale to large models - such as transformers - and follows similar scaling laws as supervised learning. We find that offline actor-critic algorithms can outperform strong, supervised, behavioral cloning baselines for multi-task training on a large dataset; containing both sub-optimal and expert behavior on 132 continuous control tasks. We introduce a Perceiver-based actor-critic model and elucidate the key features needed to make offline RL work with self- and cross-attention modules. Overall, we find that: i) simple offline actor critic algorithms are a natural choice for gradually moving away from the currently predominant paradigm of behavioral cloning, and ii) via offline RL it is possible to learn multi-task policies that master many domains simultaneously, including real robotics tasks, from sub-optimal demonstrations or self-generated data.",
        "_bibtex": "@inproceedings{\nspringenberg2024offline,\ntitle={Offline Actor-Critic Reinforcement Learning Scales to Large Models},\nauthor={Jost Tobias Springenberg and Abbas Abdolmaleki and Jingwei Zhang and Oliver Groth and Michael Bloesch and Thomas Lampe and Philemon Brakel and Sarah Maria Elisabeth Bechtle and Steven Kapturowski and Roland Hafner and Nicolas Heess and Martin Riedmiller},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=tl2qmO5kpD}\n}"
    },
    {
        "title": "Fast Timing-Conditioned Latent Audio Diffusion",
        "authorids": [
            "~Zach_Evans1",
            "cj@stability.ai",
            "josiah@stability.ai",
            "~Scott_H._Hawley1",
            "~Jordi_Pons1"
        ],
        "abstract": "Generating long-form 44.1kHz stereo audio from text prompts can be computationally demanding. Further, most previous works do not tackle that music and sound effects naturally vary in their duration. Our research focuses on the efficient generation of long-form, variable-length stereo music and sounds at 44.1kHz using text prompts with a generative model. It is based on latent diffusion, with its latent defined by a fully-convolutional variational autoencoder. The generative model is conditioned on text prompts as well as timing embeddings, allowing for fine control over both the content and length of the generated music and sounds. It is capable of rendering stereo signals of up to 95 sec at 44.1kHz in 8 sec on an A100 GPU. Despite its compute efficiency and fast inference, the proposed model is one of the best in two public text-to-music and -audio benchmarks and, differently from state-of-the-art models, can generate music with structure and stereo sounds.",
        "_bibtex": "@inproceedings{\nevans2024fast,\ntitle={Fast Timing-Conditioned Latent Audio Diffusion},\nauthor={Zach Evans and CJ Carr and Josiah Taylor and Scott H. Hawley and Jordi Pons},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=jOlO8t1xdx}\n}"
    },
    {
        "title": "MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions",
        "authorids": [
            "~Kai_Zhang10",
            "~Yi_Luan1",
            "~Hexiang_Hu1",
            "~Kenton_Lee1",
            "~Siyuan_Qiao1",
            "~Wenhu_Chen3",
            "~Yu_Su2",
            "~Ming-Wei_Chang3"
        ],
        "abstract": "Image retrieval, i.e., finding desired images given a reference image, inherently encompasses rich, multi-faceted search intents that are difficult to capture solely using image-based measures. Recent works leverage text instructions to allow users to more freely express their search intents. However, they primarily focus on image pairs that are visually similar and/or can be characterized by a small set of pre-defined relations. The core thesis of this paper is that text instructions can enable retrieving images with richer relations beyond visual similarity. To show this, we introduce MagicLens, a series of self-supervised image retrieval models that support open-ended instructions. MagicLens is built on a key novel insight: image pairs that naturally occur on the same web pages contain a wide range of implicit relations (e.g., inside view of), and we can bring those implicit relations explicit by synthesizing instructions via foundation models. Trained on 36.7M (query image, instruction, target image) triplets with rich semantic relations mined from the web, MagicLens achieves results comparable with or better than prior best on eight benchmarks of various image retrieval tasks, while maintaining high parameter efficiency with a significantly smaller model size. Additional human analyses on a 1.4M-image unseen corpus further demonstrate the diversity of search intents supported by MagicLens. Code and models are publicly available at the https://open-vision-language.github.io/MagicLens/.",
        "_bibtex": "@inproceedings{\nzhang2024magiclens,\ntitle={MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions},\nauthor={Kai Zhang and Yi Luan and Hexiang Hu and Kenton Lee and Siyuan Qiao and Wenhu Chen and Yu Su and Ming-Wei Chang},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=Zc22RDtsvP}\n}"
    },
    {
        "title": "Making Old Things New: A Unified Algorithm for Differentially Private Clustering",
        "authorids": [
            "~Max_Dupre_la_Tour1",
            "~Monika_Henzinger1",
            "~David_Saulpic1"
        ],
        "abstract": "As a staple of data analysis and unsupervised learning, the problem of private clustering has been widely studied, under various privacy models. Centralized differential privacy is the first of them, and the problem has also been studied for the local and the shuffle variation. In each case, the goal is to design an algorithm that computes privately a clustering, with the smallest possible error. The study of each variation gave rise to new algorithm: the landscape of private clustering algorithm is therefore quite intricate. In this paper, we show that a 20 year-old algorithm can be slightly modified to work for any of those models. This provides a unified picture: while matching almost all previously known results, it allows us to improve some of them, and extend to a new privacy model, the continual observation setting, where the input is changing over time and the algorithm must output a new solution at each time step.",
        "_bibtex": "@inproceedings{\ntour2024making,\ntitle={Making Old Things New: A Unified Algorithm for Differentially Private Clustering},\nauthor={Max Dupre la Tour and Monika Henzinger and David Saulpic},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=3ajK5xplDL}\n}"
    },
    {
        "title": "Interpreting and Improving Large Language Models in Arithmetic Calculation",
        "authorids": [
            "~Wei_Zhang58",
            "~Chaoqun_Wan2",
            "~Yonggang_Zhang1",
            "~Yiu-ming_Cheung1",
            "~Xinmei_Tian1",
            "~Xu_Shen1",
            "~Jieping_Ye4"
        ],
        "abstract": "Large language models (LLMs) have demonstrated remarkable potential across numerous applications and have shown an emergent ability to tackle complex reasoning tasks, such as mathematical computations. However, even for the simplest arithmetic calculations, the intrinsic mechanisms behind LLMs remains mysterious, making it challenging to ensure reliability. In this work, we delve into uncovering a specific mechanism by which LLMs execute calculations. Through comprehensive experiments, we find that LLMs frequently involve a small fraction (<5%) of attention heads, which play a pivotal role in focusing on operands and operators during calculation processes. Subsequently, the information from these operands is processed through multi-layer perceptrons (MLPs), progressively leading to the final solution. These pivotal heads/MLPs, though identified on a specific dataset, exhibit transferability across different datasets and even distinct tasks. This insight prompted us to investigate the potential benefits of selectively fine-tuning these essential heads/MLPs to boost the LLMs' computational performance. We empirically find that such precise tuning can yield notable enhancements on mathematical prowess, without compromising the performance on non-mathematical tasks. Our work serves as a preliminary exploration into the arithmetic calculation abilities inherent in LLMs, laying a solid foundation to reveal more intricate mathematical tasks.",
        "_bibtex": "@inproceedings{\nzhang2024interpreting,\ntitle={Interpreting and Improving Large Language Models in Arithmetic Calculation},\nauthor={Wei Zhang and Chaoqun Wan and Yonggang Zhang and Yiu-ming Cheung and Xinmei Tian and Xu Shen and Jieping Ye},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=CfOtiepP8s}\n}"
    },
    {
        "title": "SparseTSF: Modeling Long-term Time Series Forecasting with *1k* Parameters",
        "authorids": [
            "~Shengsheng_Lin1",
            "~Weiwei_Lin1",
            "~Wentai_Wu1",
            "~Haojun_Chen3",
            "~Junjie_Yang7"
        ],
        "abstract": "This paper introduces SparseTSF, a novel, extremely lightweight model for Long-term Time Series Forecasting (LTSF), designed to address the challenges of modeling complex temporal dependencies over extended horizons with minimal computational resources. At the heart of SparseTSF lies the Cross-Period Sparse Forecasting technique, which simplifies the forecasting task by decoupling the periodicity and trend in time series data. This technique involves downsampling the original sequences to focus on cross-period trend prediction, effectively extracting periodic features while minimizing the model's complexity and parameter count. Based on this technique, the SparseTSF model uses fewer than *1k* parameters to achieve competitive or superior performance compared to state-of-the-art models. Furthermore, SparseTSF showcases remarkable generalization capabilities, making it well-suited for scenarios with limited computational resources, small samples, or low-quality data. The code is publicly available at this repository: https://github.com/lss-1138/SparseTSF.",
        "_bibtex": "@inproceedings{\nlin2024sparsetsf,\ntitle={Sparse{TSF}: Modeling Long-term Time Series Forecasting with *1k* Parameters},\nauthor={Shengsheng Lin and Weiwei Lin and Wentai Wu and Haojun Chen and Junjie Yang},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=54NSHO0lFe}\n}"
    },
    {
        "title": "On the Last-Iterate Convergence of Shuffling Gradient Methods",
        "authorids": [
            "~Zijian_Liu1",
            "~Zhengyuan_Zhou2"
        ],
        "abstract": "Shuffling gradient methods are widely used in modern machine learning tasks and include three popular implementations: Random Reshuffle (RR), Shuffle Once (SO), and Incremental Gradient (IG). Compared to the empirical success, the theoretical guarantee of shuffling gradient methods was not well-understood for a long time. Until recently, the convergence rates had just been established for the average iterate for convex functions and the last iterate for strongly convex problems (using squared distance as the metric). However, when using the function value gap as the convergence criterion, existing theories cannot interpret the good performance of the last iterate in different settings (e.g., constrained optimization). To bridge this gap between practice and theory, we prove the first last-iterate convergence rates for shuffling gradient methods with respect to the objective value even without strong convexity. Our new results either (nearly) match the existing last-iterate lower bounds or are as fast as the previous best upper bounds for the average iterate.",
        "_bibtex": "@inproceedings{\nliu2024on,\ntitle={On the Last-Iterate Convergence of Shuffling Gradient Methods},\nauthor={Zijian Liu and Zhengyuan Zhou},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=Xdy9bjwHDu}\n}"
    },
    {
        "title": "AlphaFold Meets Flow Matching for Generating Protein Ensembles",
        "authorids": [
            "~Bowen_Jing1",
            "~Bonnie_Berger1",
            "~Tommi_S._Jaakkola1"
        ],
        "abstract": "The biological functions of proteins often depend on dynamic structural ensembles. In this work, we develop a flow-based generative modeling approach for learning and sampling the conformational landscapes of proteins. We repurpose highly accurate single-state predictors such as AlphaFold and ESMFold and fine-tune them under a custom flow matching framework to obtain sequence-conditioned generative models of protein structure called AlphaFlow and ESMFlow. When trained and evaluated on the PDB, our method provides a superior combination of precision and diversity compared to AlphaFold with MSA subsampling. When further trained on ensembles from all-atom MD, our method accurately captures conformational flexibility, positional distributions, and higher-order ensemble observables for unseen proteins. Moreover, our method can diversify a static PDB structure with faster wall-clock convergence to certain equilibrium properties than replicate MD trajectories, demonstrating its potential as a proxy for expensive physics-based simulations. Code is available at https://github.com/bjing2016/alphaflow.",
        "_bibtex": "@inproceedings{\njing2024alphafold,\ntitle={AlphaFold Meets Flow Matching for Generating Protein Ensembles},\nauthor={Bowen Jing and Bonnie Berger and Tommi Jaakkola},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=rs8Sh2UASt}\n}"
    },
    {
        "title": "Low-Cost High-Power Membership Inference Attacks",
        "authorids": [
            "~Sajjad_Zarifzadeh1",
            "~Philippe_Liu1",
            "~Reza_Shokri1"
        ],
        "abstract": "Membership inference attacks aim to detect if a particular data point was used in training a model. We design a novel statistical test to perform robust membership inference attacks (RMIA) with low computational overhead. We achieve this by a fine-grained modeling of the null hypothesis in our likelihood ratio tests, and effectively leveraging both reference models and reference population data samples. RMIA has superior test power compared with prior methods, throughout the TPR-FPR curve (even at extremely low FPR, as low as 0). Under computational constraints, where only a limited number of pre-trained reference models (as few as 1) are available, and also when we vary other elements of the attack (e.g., data distribution), our method performs exceptionally well, unlike prior attacks that approach random guessing. RMIA lays the groundwork for practical yet accurate data privacy risk assessment in machine learning.",
        "_bibtex": "@inproceedings{\nzarifzadeh2024lowcost,\ntitle={Low-Cost High-Power Membership Inference Attacks},\nauthor={Sajjad Zarifzadeh and Philippe Liu and Reza Shokri},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=sT7UJh5CTc}\n}"
    },
    {
        "title": "A Touch, Vision, and Language Dataset for Multimodal Alignment",
        "authorids": [
            "~Letian_Fu1",
            "~Gaurav_Datta1",
            "~Huang_Huang1",
            "~William_Chung-Ho_Panitch1",
            "jaimyndrake@berkeley.edu",
            "~Joseph_Ortiz2",
            "~Mustafa_Mukadam1",
            "~Mike_Lambeta1",
            "~Roberto_Calandra1",
            "~Ken_Goldberg1"
        ],
        "abstract": "Touch is an important sensing modality for humans, but it has not yet been incorporated into a multimodal generative language model. This is partially due to the difficulty of obtaining natural language labels for tactile data and the complexity of aligning tactile readings with both visual observations and language descriptions. As a step towards bridging that gap, this work introduces a new dataset of 44K in-the-wild visiontouch pairs, with English language labels annotated by humans (10%) and textual pseudo-labels from GPT-4V (90%). We use this dataset to train a vision-language-aligned tactile encoder for open-vocabulary classification and a touch-visionlanguage (TVL) model for text generation using the trained encoder. Results suggest that by incorporating touch, the TVL model improves (+29% classification accuracy) tactile-vision-language alignment over existing models trained on any pair of those modalities. Although only a small fraction of the dataset is human labeled, the TVL model demonstrates improved visual-tactile understanding over GPT-4V (+12%) and open-source vision-language models (+32%) on a new touch-vision understanding benchmark. Code, checkpoints and data are available on https: //tactile-vlm.github.io.",
        "_bibtex": "@inproceedings{\nfu2024a,\ntitle={A Touch, Vision, and Language Dataset for Multimodal Alignment},\nauthor={Letian Fu and Gaurav Datta and Huang Huang and William Chung-Ho Panitch and Jaimyn Drake and Joseph Ortiz and Mustafa Mukadam and Mike Lambeta and Roberto Calandra and Ken Goldberg},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=tFEOOH9eH0}\n}"
    },
    {
        "title": "Position: Measure Dataset Diversity, Don't Just Claim It",
        "authorids": [
            "~Dora_Zhao1",
            "~Jerone_Andrews1",
            "~Orestis_Papakyriakopoulos1",
            "~Alice_Xiang1"
        ],
        "abstract": "Machine learning (ML) datasets, often perceived as neutral, inherently encapsulate abstract and disputed social constructs. Dataset curators frequently employ value-laden terms such as diversity, bias, and quality to characterize datasets. Despite their prevalence, these terms lack clear definitions and validation. Our research explores the implications of this issue by analyzing \"diversity\" across 135 image and text datasets. Drawing from social sciences, we apply principles from measurement theory to identify considerations and offer recommendations for conceptualizing, operationalizing, and evaluating diversity in datasets. Our findings have broader implications for ML research, advocating for a more nuanced and precise approach to handling value-laden properties in dataset construction.",
        "_bibtex": "@inproceedings{\nzhao2024position,\ntitle={Position: Measure Dataset Diversity, Don't Just Claim It},\nauthor={Dora Zhao and Jerone Andrews and Orestis Papakyriakopoulos and Alice Xiang},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=jsKr6RVDDs}\n}"
    },
    {
        "title": "Contrasting Multiple Representations with the Multi-Marginal Matching Gap",
        "authorids": [
            "~Zoe_Piran1",
            "~Michal_Klein1",
            "~James_Thornton1",
            "~marco_cuturi2"
        ],
        "abstract": "Learning meaningful representations of complex objects that can be seen through multiple ($k\\geq 3$) views or modalities is a core task in machine learning. Existing methods use losses originally intended for paired views, and extend them to $k$ views, either by instantiating $\\tfrac12k(k-1)$ loss-pairs, or by using reduced embeddings, following a *one vs. average-of-rest* strategy. We propose the multi-marginal matching gap (M3G), a loss that borrows tools from multi-marginal optimal transport (MM-OT) theory to simultaneously incorporate all $k$ views. Given a batch of $n$ points, each seen as a $k$-tuple of views subsequently transformed into $k$ embeddings, our loss contrasts the cost of matching these $n$ ground-truth $k$-tuples with the MM-OT polymatching cost, which seeks $n$ optimally arranged $k$-tuples chosen within these $n\\times k$ vectors. While the exponential complexity $O(n^k$) of the MM-OT problem may seem daunting, we show in experiments that a suitable generalization of the Sinkhorn algorithm for that problem can scale to, e.g., $k=3\\sim 6$ views using mini-batches of size $64~\\sim128$. Our experiments demonstrate improved performance over multiview extensions of pairwise losses, for both self-supervised and multimodal tasks.",
        "_bibtex": "@inproceedings{\npiran2024contrasting,\ntitle={Contrasting Multiple Representations with the Multi-Marginal Matching Gap},\nauthor={Zoe Piran and Michal Klein and James Thornton and marco cuturi},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=dV9B9qFeGi}\n}"
    },
    {
        "title": "Candidate Pseudolabel Learning: Enhancing Vision-Language Models by Prompt Tuning with Unlabeled Data",
        "authorids": [
            "~Jiahan_Zhang1",
            "~Qi_Wei4",
            "~Feng_Liu2",
            "~Lei_Feng1"
        ],
        "abstract": "Fine-tuning vision-language models (VLMs) with abundant unlabeled data recently has attracted increasing attention. Existing methods that resort to the pseudolabeling strategy would suffer from heavily incorrect hard pseudolabels when VLMs exhibit low zero-shot performance in downstream tasks. To alleviate this issue, we propose a **C**andidate **P**seudolabel **L**earning method, termed **CPL**, to fine-tune VLMs with suitable candidate pseudolabels of unlabeled data in downstream tasks. The core of our method lies in the generation strategy of candidate pseudolabels, which progressively generates refined candidate pseudolabels by both intra- and inter-instance label selection, based on a confidence score matrix for all unlabeled data. This strategy can result in better performance in true label inclusion and class-balanced instance selection. In this way, we can directly apply existing loss functions to learn with generated candidate psueudolabels. Extensive experiments on nine benchmark datasets with three learning paradigms demonstrate the effectiveness of our method. Our code can be found here.",
        "_bibtex": "@inproceedings{\nzhang2024candidate,\ntitle={Candidate Pseudolabel Learning: Enhancing Vision-Language Models by Prompt Tuning with Unlabeled Data},\nauthor={Jiahan Zhang and Qi Wei and Feng Liu and Lei Feng},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=sBJNokmYuV}\n}"
    },
    {
        "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright BreachesWithout Adjusting Finetuning Pipeline",
        "authorids": [
            "~Haonan_Wang1",
            "~Qianli_Shen1",
            "~Yao_Tong2",
            "~Yang_Zhang22",
            "~Kenji_Kawaguchi1"
        ],
        "abstract": "The commercialization of text-to-image diffusion models (DMs) brings forth potential copyright concerns. Despite numerous attempts to protect DMs from copyright issues, the vulnerabilities of these solutions are underexplored. In this study, we formalized the Copyright Infringement Attack on generative AI models and proposed a backdoor attack method, SilentBadDiffusion, to induce copyright infringement without requiring access to or control over training processes. Our method strategically embeds connections between pieces of copyrighted information and text references in poisoning data while carefully dispersing that information, making the poisoning data inconspicuous when integrated into a clean dataset. Our experiments show the stealth and efficacy of the poisoning data. When given specific text prompts, DMs trained with a poisoning ratio of 0.20% can produce copyrighted images. Additionally, the results reveal that the more sophisticated the DMs are, the easier the success of the attack becomes. These findings underline potential pitfalls in the prevailing copyright protection strategies and underscore the necessity for increased scrutiny to prevent the misuse of DMs.",
        "_bibtex": "@inproceedings{\nwang2024the,\ntitle={The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright BreachesWithout Adjusting Finetuning Pipeline},\nauthor={Haonan Wang and Qianli Shen and Yao Tong and Yang Zhang and Kenji Kawaguchi},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=ZvFLbEPv6x}\n}"
    },
    {
        "title": "Stealing part of a production language model",
        "authorids": [
            "~Nicholas_Carlini1",
            "~Daniel_Paleka1",
            "~Krishnamurthy_Dj_Dvijotham1",
            "~Thomas_Steinke2",
            "~Jonathan_Hayase2",
            "~A._Feder_Cooper1",
            "~Katherine_Lee1",
            "~Matthew_Jagielski1",
            "~Milad_Nasr2",
            "~Arthur_Conmy1",
            "~Eric_Wallace1",
            "~David_Rolnick1",
            "~Florian_Tram\u00e8r1"
        ],
        "abstract": "We introduce the first model-stealing attack that extracts precise, nontrivial information from black-box production language models like OpenAI's ChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access. For under \r\n$20 USD, our attack extracts the entire projection matrix of OpenAI's Ada and Babbage language models. We thereby confirm, for the first time, that these black-box models have a hidden dimension of 1024 and 2048, respectively. We also recover the exact hidden dimension size of the GPT-3.5-turbo model, and estimate it would cost under \\\\$2,000 in queries to recover the entire projection matrix. We conclude with potential defenses and mitigations, and discuss the implications of possible future work that could extend our attack.",
        "_bibtex": "@inproceedings{\ncarlini2024stealing,\ntitle={Stealing part of a production language model},\nauthor={Nicholas Carlini and Daniel Paleka and Krishnamurthy Dj Dvijotham and Thomas Steinke and Jonathan Hayase and A. Feder Cooper and Katherine Lee and Matthew Jagielski and Milad Nasr and Arthur Conmy and Eric Wallace and David Rolnick and Florian Tram{\\`e}r},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=VE3yWXt3KB}\n}"
    },
    {
        "title": "Pausing Policy Learning in Non-stationary Reinforcement Learning",
        "authorids": [
            "~Hyunin_Lee1",
            "~Ming_Jin2",
            "~Javad_Lavaei1",
            "~Somayeh_Sojoudi1"
        ],
        "abstract": "Real-time inference is a challenge of real-world reinforcement learning due to temporal differences in time-varying environments: the system collects data from the past, updates the decision model in the present, and deploys it in the future. We tackle a common belief that continually updating the decision is optimal to minimize the temporal gap. We propose forecasting an online reinforcement learning framework and show that strategically pausing decision updates yields better overall performance by effectively managing aleatoric uncertainty. Theoretically, we compute an optimal ratio between policy update and hold duration, and show that a non-zero policy hold duration provides a sharper upper bound on the dynamic regret. Our experimental evaluations on three different environments also reveal that a non-zero policy hold duration yields higher rewards compared to continuous decision updates.",
        "_bibtex": "@inproceedings{\nlee2024pausing,\ntitle={Pausing Policy Learning in Non-stationary Reinforcement Learning},\nauthor={Hyunin Lee and Ming Jin and Javad Lavaei and Somayeh Sojoudi},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=qY622O6Ehg}\n}"
    },
    {
        "title": "Debating with More Persuasive LLMs Leads to More Truthful Answers",
        "authorids": [
            "~Akbir_Khan1",
            "~John_Hughes4",
            "~Dan_Valentine1",
            "~Laura_Ruis1",
            "~Kshitij_Sachan1",
            "~Ansh_Radhakrishnan1",
            "~Edward_Grefenstette1",
            "~Samuel_R._Bowman1",
            "~Tim_Rockt\u00e4schel1",
            "~Ethan_Perez1"
        ],
        "abstract": "Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is *debate*, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76% and 88% accuracy respectively (naive baselines obtain 48% and 60%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert ability to identify the truth in debates. Our results provide encouraging empirical evidence for the viability of aligning models with debate in the absence of ground truth.",
        "_bibtex": "@inproceedings{\nkhan2024debating,\ntitle={Debating with More Persuasive {LLM}s Leads to More Truthful Answers},\nauthor={Akbir Khan and John Hughes and Dan Valentine and Laura Ruis and Kshitij Sachan and Ansh Radhakrishnan and Edward Grefenstette and Samuel R. Bowman and Tim Rockt{\\\"a}schel and Ethan Perez},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=iLCZtl7FTa}\n}"
    },
    {
        "title": "Probabilistic Generating Circuits - Demystified",
        "authorids": [
            "~Sanyam_Agarwal2",
            "~Markus_Bl\u00e4ser1"
        ],
        "abstract": "Zhang et al. (ICML 2021, PLMR 139, pp. 12447\u201312457) introduced probabilistic generating circuits (PGCs) as a probabilistic model to unify probabilistic circuits (PCs) and determinantal point processes (DPPs). At a first glance, PGCs store a distribution in a very different way, they compute the probability generating polynomial instead of the probability mass function and it seems that this is the main reason why PGCs are more powerful than PCs or DPPs. However, PGCs also allow for negative weights, whereas classical PCs assume that all weights are nonnegative. One main insight of this work is that the negative weights are the cause for the power of PGCs and not the different representation. PGCs are PCs in disguise: we show how to transform any PGC on binary variables into a PC with negative weights with only polynomial blowup. PGCs were defined by Zhang et al. only for binary random variables. As our second main result, we show that there is a good reason for this: we prove that PGCs for categorical variables with larger image size do not support tractable marginalization unless NP=P. On the other hand, we show that we can model categorical variables with larger image size as PC with negative weights computing set-multilinear polynomials. These allow for tractable marginalization. In this sense, PCs with negative weights strictly subsume PGCs.",
        "_bibtex": "@inproceedings{\nagarwal2024probabilistic,\ntitle={Probabilistic Generating Circuits - Demystified},\nauthor={Sanyam Agarwal and Markus Bl{\\\"a}ser},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=EqFxIbGWRU}\n}"
    },
    {
        "title": "Self-Composing Policies for Scalable Continual Reinforcement Learning",
        "authorids": [
            "~Mikel_Malagon1",
            "~Josu_Ceberio1",
            "~Jose_A._Lozano1"
        ],
        "abstract": "This work introduces a growable and modular neural network architecture that naturally avoids catastrophic forgetting and interference in continual reinforcement learning. The structure of each module allows the selective combination of previous policies along with its internal policy accelerating the learning process on the current task. Unlike previous growing neural network approaches, we show that the number of parameters of the proposed approach grows linearly with respect to the number of tasks, and does not sacrifice plasticity to scale. Experiments conducted in benchmark continuous control and visual problems reveal that the proposed approach achieves greater knowledge transfer and performance than alternative methods.",
        "_bibtex": "@inproceedings{\nmalagon2024selfcomposing,\ntitle={Self-Composing Policies for Scalable Continual Reinforcement Learning},\nauthor={Mikel Malagon and Josu Ceberio and Jose A. Lozano},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=f5gtX2VWSB}\n}"
    },
    {
        "title": "Sparse Inducing Points in Deep Gaussian Processes: Enhancing Modeling with Denoising Diffusion Variational Inference",
        "authorids": [
            "~JIAN_XU5",
            "~Delu_Zeng4",
            "~John_Paisley1"
        ],
        "abstract": "Deep Gaussian processes (DGPs) provide a robust paradigm in Bayesian deep learning. In DGPs, a set of sparse integration locations called inducing points are selected to approximate the posterior distribution of the model. This is done to reduce computational complexity and improve model efficiency. However, inferring the posterior distribution of inducing points is not straightforward. Traditional variational inference techniques methods to approximate the posterior often leads to significant bias. To address this issue, we propose an alternative named Denoising Diffusion Variational Inference (DDVI) that utilizes a denoising diffusion stochastic differential equation (SDE) for generating posterior samples of inducing variables. We refer to the score matching method in the denoising diffusion model to approximate challenging score functions using a neural network. Furthermore, by combining classical mathematical theory of SDE with the minimization of KL divergence between the approximate and true processes, we propose a novel explicit variational lower bound for the marginal likelihood function of DGP. Through extensive experiments on various datasets and comparisons with baseline methods, we empirically demonstrate the effectiveness of the DDVI method in posterior inference of inducing points for DGP models.",
        "_bibtex": "@inproceedings{\nxu2024sparse,\ntitle={Sparse Inducing Points in Deep Gaussian Processes: Enhancing Modeling with Denoising Diffusion Variational Inference},\nauthor={JIAN XU and Delu Zeng and John Paisley},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=jTn4AIOgpM}\n}"
    },
    {
        "title": "Optimal Hessian/Jacobian-Free Nonconvex-PL Bilevel Optimization",
        "authorids": [
            "~Feihu_Huang1"
        ],
        "abstract": "Bilevel optimization is widely applied in many machine learning tasks such as hyper-parameter learning, meta learning and reinforcement learning. Although many algorithms recently have been developed to solve the bilevel optimization problems, they generally rely on the (strongly) convex lower-level problems. More recently, some methods have been proposed to solve the nonconvex-PL bilevel optimization problems, where their upper-level problems are possibly nonconvex, and their lower-level problems are also possibly nonconvex while satisfying Polyak-\u0141ojasiewicz (PL) condition. However, these methods still have a high convergence complexity or a high computation complexity such as requiring compute expensive Hessian/Jacobian matrices and its inverses. In the paper, thus, we propose an efficient Hessian/Jacobian-free method (i.e., HJFBiO) with the optimal convergence complexity to solve the nonconvex-PL bilevel problems. Theoretically, under some mild conditions, we prove that our HJFBiO method obtains an optimal convergence rate of $O(\\frac{1}{T})$, where $T$ denotes the number of iterations, and has an optimal gradient complexity of $O(\\epsilon^{-1})$ in finding an $\\epsilon$-stationary solution. We conduct some numerical experiments on the bilevel PL game and hyper-representation learning task to demonstrate efficiency of our proposed method.",
        "_bibtex": "@inproceedings{\nhuang2024optimal,\ntitle={Optimal Hessian/Jacobian-Free Nonconvex-{PL} Bilevel Optimization},\nauthor={Feihu Huang},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=eZiQWM5U0E}\n}"
    },
    {
        "title": "LSEnet: Lorentz Structural Entropy Neural Network for Deep Graph Clustering",
        "authorids": [
            "~Li_Sun4",
            "~Zhenhao_Huang1",
            "~Hao_Peng7",
            "~YuJie_Wang9",
            "~Chunyang_Liu1",
            "~Philip_S._Yu1"
        ],
        "abstract": "Graph clustering is a fundamental problem in machine learning. Deep learning methods achieve the state-of-the-art results in recent years, but they still cannot work without predefined cluster numbers. Such limitation motivates us to pose a more challenging problem of graph clustering with unknown cluster number. We propose to address this problem from a fresh perspective of graph information theory (i.e., structural information). In the literature, structural information has not yet been introduced to deep clustering, and its classic definition falls short of discrete formulation and modeling node features. In this work, we first formulate a differentiable structural information (DSI) in the continuous realm, accompanied by several theoretical results. By minimizing DSI, we construct the optimal partitioning tree where densely connected nodes in the graph tend to have the same assignment, revealing the cluster struc- ture. DSI is also theoretically presented as a new graph clustering objective, not requiring the pre-defined cluster number. Furthermore, we design a neural LSEnet in the Lorentz model of hyperbolic space, where we integrate node features to structural information via manifold-valued graph convolution. Extensive empirical results on real graphs show the superiority of our approach.",
        "_bibtex": "@inproceedings{\nsun2024lsenet,\ntitle={{LSE}net: Lorentz Structural Entropy Neural Network for Deep Graph Clustering},\nauthor={Li Sun and Zhenhao Huang and Hao Peng and YuJie Wang and Chunyang Liu and Philip S. Yu},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=L6SRXG92s6}\n}"
    },
    {
        "title": "VideoPoet: A Large Language Model for Zero-Shot Video Generation",
        "authorids": [
            "~Dan_Kondratyuk1",
            "~Lijun_Yu1",
            "~Xiuye_Gu1",
            "~Jose_Lezama1",
            "~Jonathan_Huang1",
            "~Grant_Schindler3",
            "~Rachel_Hornung1",
            "~Vighnesh_Birodkar1",
            "~Jimmy_Yan1",
            "~Ming-Chang_Chiu1",
            "~Krishna_Somandepalli3",
            "~Hassan_Akbari1",
            "~Yair_Alon1",
            "~Yong_Cheng3",
            "~Joshua_V._Dillon1",
            "~Agrim_Gupta1",
            "~Meera_Hahn1",
            "~Anja_Hauth1",
            "hendon@google.com",
            "~Alonso_Martinez2",
            "~David_Minnen1",
            "~Mikhail_Sirotenko1",
            "~Kihyuk_Sohn1",
            "~Xuan_Yang6",
            "~Hartwig_Adam1",
            "~Ming-Hsuan_Yang1",
            "~Irfan_Essa1",
            "~Huisheng_Wang1",
            "~David_A_Ross1",
            "~Bryan_Seybold1",
            "~Lu_Jiang1"
        ],
        "abstract": "We present VideoPoet, a language model capable of synthesizing high-quality video from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs -- including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks. We present empirical results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting the ability to generate high-fidelity motions. Project page: http://sites.research.google/videopoet/",
        "_bibtex": "@inproceedings{\nkondratyuk2024videopoet,\ntitle={VideoPoet: A Large Language Model for Zero-Shot Video Generation},\nauthor={Dan Kondratyuk and Lijun Yu and Xiuye Gu and Jose Lezama and Jonathan Huang and Grant Schindler and Rachel Hornung and Vighnesh Birodkar and Jimmy Yan and Ming-Chang Chiu and Krishna Somandepalli and Hassan Akbari and Yair Alon and Yong Cheng and Joshua V. Dillon and Agrim Gupta and Meera Hahn and Anja Hauth and David Hendon and Alonso Martinez and David Minnen and Mikhail Sirotenko and Kihyuk Sohn and Xuan Yang and Hartwig Adam and Ming-Hsuan Yang and Irfan Essa and Huisheng Wang and David A Ross and Bryan Seybold and Lu Jiang},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=LRkJwPIDuE}\n}"
    },
    {
        "title": "Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization",
        "authorids": [
            "~Yang_Jin1",
            "~Zhicheng_Sun1",
            "~Kun_Xu4",
            "~Kun_Xu6",
            "~Liwei_Chen3",
            "~Hao_Jiang10",
            "~Quzhe_Huang1",
            "~Chengru_Song1",
            "~Yuliang_Liu3",
            "~Di_ZHANG3",
            "~Yang_Song6",
            "~Kun_Gai1",
            "~Yadong_MU1"
        ],
        "abstract": "In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos. Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics. In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions. These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content. Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13 multimodal benchmarks in image and video understanding and generation. Our code and models are available at https://video-lavit.github.io.",
        "_bibtex": "@inproceedings{\njin2024videolavit,\ntitle={Video-La{VIT}: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization},\nauthor={Yang Jin and Zhicheng Sun and Kun Xu and Kun Xu and Liwei Chen and Hao Jiang and Quzhe Huang and Chengru Song and Yuliang Liu and Di ZHANG and Yang Song and Kun Gai and Yadong MU},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=S9lk6dk4LL}\n}"
    },
    {
        "title": "Position: Considerations for Differentially Private Learning with Large-Scale Public Pretraining",
        "authorids": [
            "~Florian_Tram\u00e8r1",
            "~Gautam_Kamath1",
            "~Nicholas_Carlini1"
        ],
        "abstract": "The performance of differentially private machine learning can be boosted significantly by leveraging the transfer learning capabilities of non-private models pretrained on large *public* datasets. We critically review this approach. We primarily question whether the use of large Web-scraped datasets *should* be viewed as differential-privacy-preserving. We further scrutinize whether existing machine learning benchmarks are appropriate for measuring the ability of pretrained models to generalize to sensitive domains. Finally, we observe that reliance on large pretrained models may lose *other* forms of privacy, requiring data to be outsourced to a more compute-powerful third party.",
        "_bibtex": "@inproceedings{\ntram{\\`e}r2024position,\ntitle={Position: Considerations for Differentially Private Learning with Large-Scale Public Pretraining},\nauthor={Florian Tram{\\`e}r and Gautam Kamath and Nicholas Carlini},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=ncjhi4qAPV}\n}"
    },
    {
        "title": "Stereo Risk: A Continuous Modeling Approach to Stereo Matching",
        "authorids": [
            "~Ce_Liu3",
            "~Suryansh_Kumar1",
            "~Shuhang_Gu3",
            "~Radu_Timofte1",
            "~Yao_Yao1",
            "~Luc_Van_Gool1"
        ],
        "abstract": "We introduce Stereo Risk, a new deep-learning approach to solve the classical stereo-matching problem in computer vision. As it is well-known that stereo matching boils down to a per-pixel disparity estimation problem, the popular state-of-the-art stereo-matching approaches widely rely on regressing the scene disparity values, yet via discretization of scene disparity values. Such discretization often fails to capture the nuanced, continuous nature of scene depth. Stereo Risk departs from the conventional discretization approach by formulating the scene disparity as an optimal solution to a continuous risk minimization problem, hence the name \"stereo risk\". We demonstrate that $L^1$ minimization of the proposed continuous risk function enhances stereo-matching performance for deep networks, particularly for disparities with multi-modal probability distributions. Furthermore, to enable the end-to-end network training of the non-differentiable $L^1$ risk optimization, we exploited the implicit function theorem, ensuring a fully differentiable network. A comprehensive analysis demonstrates our method's theoretical soundness and superior performance over the state-of-the-art methods across various benchmark datasets, including KITTI 2012, KITTI 2015, ETH3D, SceneFlow, and Middlebury 2014.",
        "_bibtex": "@inproceedings{\nliu2024stereo,\ntitle={Stereo Risk: A Continuous Modeling Approach to Stereo Matching},\nauthor={Ce Liu and Suryansh Kumar and Shuhang Gu and Radu Timofte and Yao Yao and Luc Van Gool},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=Mfk6ZbD6eY}\n}"
    },
    {
        "title": "Zeroth-Order Methods for Constrained Nonconvex Nonsmooth Stochastic Optimization",
        "authorids": [
            "~Zhuanghua_Liu2",
            "~Cheng_Chen9",
            "~Luo_Luo1",
            "~Bryan_Kian_Hsiang_Low1"
        ],
        "abstract": "This paper studies the problem of solving nonconvex nonsmooth optimization over a closed convex set. Most previous works tackle such problems by transforming the constrained problem into an unconstrained problem that can be solved by the techniques developed in the unconstrained setting. However, they only provide asymptotic convergence analysis for their methods. In this work, we provide the non-asymptotic analysis for solving constrained nonconvex nonsmooth optimization. We first generalize classical gradient mapping and the Frank\u2013Wolfe gap in the nonsmooth setting. Then we introduce novel notions of approximate stationarity concerning such generalized quantities. We also propose several stochastic zeroth-order algorithms for the problem, along with their non-asymptotic convergence guarantees of obtaining the proposed approximate stationarity. Finally, we conduct numerical experiments that demonstrate the effectiveness of our algorithms.",
        "_bibtex": "@inproceedings{\nliu2024zerothorder,\ntitle={Zeroth-Order Methods for Constrained Nonconvex Nonsmooth Stochastic Optimization},\nauthor={Zhuanghua Liu and Cheng Chen and Luo Luo and Bryan Kian Hsiang Low},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=PxHmxoFOgI}\n}"
    },
    {
        "title": "Information Complexity of Stochastic Convex Optimization: Applications to Generalization, Memorization, and Tracing",
        "authorids": [
            "~Idan_Attias1",
            "~Gintare_Karolina_Dziugaite1",
            "~Mahdi_Haghifam2",
            "~Roi_Livni1",
            "~Daniel_M._Roy1"
        ],
        "abstract": "In this work, we investigate the interplay between memorization and learning in the context of *stochastic convex optimization* (SCO). We define memorization via the information a learning algorithm reveals about its training data points. We then quantify this information using the framework of conditional mutual information (CMI) proposed by Steinke and Zakynthinou (2020). Our main result is a precise characterization of the tradeoff between the accuracy of a learning algorithm and its CMI, answering an open question posed by Livni (2023). We show that, in the $L^2$ Lipschitz--bounded setting and under strong convexity, every learner with an excess error $\\epsilon$ has CMI bounded below by $\\Omega(1/\\epsilon^2)$ and $\\Omega(1/\\epsilon)$, respectively. We further demonstrate the essential role of memorization in learning problems in SCO by designing an adversary capable of accurately identifying a significant fraction of the training samples in specific SCO problems. Finally, we enumerate several implications of our results, such as a limitation of generalization bounds based on CMI and the incompressibility of samples in SCO problems.",
        "_bibtex": "@inproceedings{\nattias2024information,\ntitle={Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization},\nauthor={Idan Attias and Gintare Karolina Dziugaite and Mahdi Haghifam and Roi Livni and Daniel M. Roy},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=CyEJn71Z00}\n}"
    },
    {
        "title": "Image Clustering with External Guidance",
        "authorids": [
            "~Yunfan_Li1",
            "~Peng_Hu2",
            "~Dezhong_Peng1",
            "~Jiancheng_Lv2",
            "~Jianping_Fan4",
            "~Xi_Peng3"
        ],
        "abstract": "The core of clustering lies in incorporating prior knowledge to construct supervision signals. From classic k-means based on data compactness to recent contrastive clustering guided by self-supervision, the evolution of clustering methods intrinsically corresponds to the progression of supervision signals. At present, substantial efforts have been devoted to mining internal supervision signals from data. Nevertheless, the abundant external knowledge such as semantic descriptions, which naturally conduces to clustering, is regrettably overlooked. In this work, we propose leveraging external knowledge as a new supervision signal to guide clustering. To implement and validate our idea, we design an externally guided clustering method (Text-Aided Clustering, TAC), which leverages the textual semantics of WordNet to facilitate image clustering. Specifically, TAC first selects and retrieves WordNet nouns that best distinguish images to enhance the feature discriminability. Then, TAC collaborates text and image modalities by mutually distilling cross-modal neighborhood information. Experiments demonstrate that TAC achieves state-of-the-art performance on five widely used and three more challenging image clustering benchmarks, including the full ImageNet-1K dataset. The code can be accessed at https://github.com/XLearning-SCU/2024-ICML-TAC.",
        "_bibtex": "@inproceedings{\nli2024image,\ntitle={Image Clustering with External Guidance},\nauthor={Yunfan Li and Peng Hu and Dezhong Peng and Jiancheng Lv and Jianping Fan and Xi Peng},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=JSYN891WnB}\n}"
    },
    {
        "title": "A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity",
        "authorids": [
            "~Andrew_Lee2",
            "~Xiaoyan_Bai1",
            "presi@umich.edu",
            "~Martin_Wattenberg1",
            "~Jonathan_K._Kummerfeld2",
            "~Rada_Mihalcea1"
        ],
        "abstract": "While alignment algorithms are commonly used to tune pre-trained language models towards user preferences, we lack explanations for the underlying mechanisms in which models become ``aligned'', thus making it difficult to explain phenomena like jailbreaks. In this work we study a popular algorithm, direct preference optimization (DPO), and the mechanisms by which it reduces toxicity. Namely, we first study how toxicity is represented and elicited in pre-trained language models (GPT2-medium, Llama2-7b). We then apply DPO with a carefully crafted pairwise dataset to reduce toxicity. We examine how the resulting models avert toxic outputs, and find that capabilities learned from pre-training are not removed, but rather bypassed. We use this insight to demonstrate a simple method to un-align the models, reverting them back to their toxic behavior.",
        "_bibtex": "@inproceedings{\nlee2024a,\ntitle={A Mechanistic Understanding of Alignment Algorithms: A Case Study on {DPO} and Toxicity},\nauthor={Andrew Lee and Xiaoyan Bai and Itamar Pres and Martin Wattenberg and Jonathan K. Kummerfeld and Rada Mihalcea},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=dBqHGZPGZI}\n}"
    },
    {
        "title": "EquiPocket: an E(3)-Equivariant Geometric Graph Neural Network for Ligand Binding Site Prediction",
        "authorids": [
            "~yang_zhang28",
            "~Zhewei_Wei1",
            "~Ye_Yuan15",
            "~Chongxuan_Li1",
            "~Wenbing_Huang1"
        ],
        "abstract": "Predicting the binding sites of target proteins plays a fundamental role in drug discovery. Most existing deep-learning methods consider a protein as a 3D image by spatially clustering its atoms into voxels and then feed the voxelized protein into a 3D CNN for prediction. However, the CNN-based methods encounter several critical issues: 1) defective in representing irregular protein structures; 2) sensitive to rotations; 3) insufficient to characterize the protein surface; 4) unaware of protein size shift. To address the above issues, this work proposes EquiPocket, an E(3)-equivariant Graph Neural Network (GNN) for binding site prediction, which comprises three modules: the first one to extract local geometric information for each surface atom, the second one to model both the chemical and spatial structure of protein and the last one to capture the geometry of the surface via equivariant message passing over the surface atoms. We further propose a dense attention output layer to alleviate the effect incurred by variable protein size. Extensive experiments on several representative benchmarks demonstrate the superiority of our framework to the state-of-the-art methods.",
        "_bibtex": "@inproceedings{\nzhang2024equipocket,\ntitle={EquiPocket: an E(3)-Equivariant Geometric Graph Neural Network for Ligand Binding Site Prediction},\nauthor={yang zhang and Zhewei Wei and Ye Yuan and Chongxuan Li and Wenbing Huang},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=1vGN3CSxVs}\n}"
    },
    {
        "title": "Speech Self-Supervised Learning Using Diffusion Model Synthetic Data",
        "authorids": [
            "~Heting_Gao1",
            "~Kaizhi_Qian1",
            "~Junrui_Ni1",
            "~Chuang_Gan1",
            "~Mark_A._Hasegawa-Johnson1",
            "~Shiyu_Chang2",
            "~Yang_Zhang3"
        ],
        "abstract": "While self-supervised learning (SSL) in speech has greatly reduced the reliance of speech processing systems on annotated corpora, the success of SSL still hinges on the availability of a large-scale unannotated corpus, which is still often impractical for many low-resource languages or under privacy concerns. Some existing work seeks to alleviate the problem by data augmentation, but most works are confined to introducing perturbations to real speech and do not introduce new variations in speech prosody, speakers, and speech content, which are important for SSL. Motivated by the recent finding that diffusion models have superior capabilities for modeling data distributions, we propose DiffS4L, a pretraining scheme that augments the limited unannotated data with synthetic data with different levels of variations, generated by a diffusion model trained on the limited unannotated data. Finally, an SSL model is pre-trained on the real and the synthetic speech. Our experiments show that DiffS4L can significantly improve the performance of SSL models, such as reducing the WER of the HuBERT pretrained model by 6.26 percentage points in the English ASR task. Notably, we find that the synthetic speech with all levels of variations, i.e. new prosody, new speakers, and even new content (despite the new content being mostly babble), accounts for significant performance improvement. The code is available at github.com/Hertin/DiffS4L.",
        "_bibtex": "@inproceedings{\ngao2024speech,\ntitle={Speech Self-Supervised Learning Using Diffusion Model Synthetic Data},\nauthor={Heting Gao and Kaizhi Qian and Junrui Ni and Chuang Gan and Mark A. Hasegawa-Johnson and Shiyu Chang and Yang Zhang},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=ecnpYYHjt9}\n}"
    },
    {
        "title": "Position: The Platonic Representation Hypothesis",
        "authorids": [
            "~Minyoung_Huh1",
            "~Brian_Cheung1",
            "~Tongzhou_Wang1",
            "~Phillip_Isola1"
        ],
        "abstract": "We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis.",
        "_bibtex": "@inproceedings{\nhuh2024position,\ntitle={Position Paper: The Platonic Representation Hypothesis},\nauthor={Minyoung Huh and Brian Cheung and Tongzhou Wang and Phillip Isola},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=BH8TYy0r6u}\n}"
    },
    {
        "title": "Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks",
        "authorids": [
            "~Liam_Collins1",
            "~Hamed_Hassani2",
            "~Mahdi_Soltanolkotabi1",
            "~Aryan_Mokhtari3",
            "~Sanjay_Shakkottai1"
        ],
        "abstract": "An increasingly popular machine learning paradigm is to pretrain a neural network (NN) on many tasks offline, then adapt it to downstream tasks, often by re-training only the last linear layer of the network. This approach yields strong downstream performance in a variety of contexts, demonstrating that multitask pretraining leads to effective feature learning. Although several recent theoretical studies have shown that shallow NNs learn meaningful features when either (i) they are trained on a *single* task or (ii) they are *linear*, very little is known about the closer-to-practice case of *nonlinear* NNs trained on *multiple* tasks. In this work, we present the first results proving that feature learning occurs during training with a nonlinear model on multiple tasks. Our key insight is that multi-task pretraining induces a pseudo-contrastive loss that favors representations that align points that typically have the same label across tasks. Using this observation, we show that when the tasks are binary classification tasks with labels depending on the projection of the data onto an $r$-dimensional subspace within the $d\\gg r$-dimensional input space, a simple gradient-based multitask learning algorithm on a two-layer ReLU NN recovers this projection, allowing for generalization to downstream tasks with sample and neuron complexity independent of $d$. In contrast, we show that with high probability over the draw of a single task, training on this single task cannot guarantee to learn all $r$ ground-truth features.",
        "_bibtex": "@inproceedings{\ncollins2024provable,\ntitle={Provable Multi-Task Representation Learning by Two-Layer Re{LU} Neural Networks},\nauthor={Liam Collins and Hamed Hassani and Mahdi Soltanolkotabi and Aryan Mokhtari and Sanjay Shakkottai},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=M8UbECx485}\n}"
    },
    {
        "title": "Rejuvenating image-GPT as Strong Visual Representation Learners",
        "authorids": [
            "~Sucheng_Ren1",
            "~Zeyu_Wang2",
            "~Hongru_Zhu1",
            "~Junfei_Xiao1",
            "~Alan_Yuille1",
            "~Cihang_Xie3"
        ],
        "abstract": "This paper enhances image-GPT (iGPT), one of the pioneering works that introduce autoregressive pretraining to predict the next pixels for visual representation learning. Two simple yet essential changes are made. First, we shift the prediction target from raw pixels to semantic tokens, enabling a higher-level understanding of visual content. Second, we supplement the autoregressive modeling by instructing the model to predict not only the next tokens but also the visible tokens. This pipeline is particularly effective when semantic tokens are encoded by discriminatively trained models, such as CLIP. We introduce this novel approach as D-iGPT. Extensive experiments showcase that D-iGPT excels as a strong learner of visual representations: A notable achievement is its compelling performance on the ImageNet-1K dataset --- by training on publicly available datasets, D-iGPT unprecedentedly achieves **90.0%** top-1 accuracy with a vanilla ViT-H. Additionally, D-iGPT shows strong generalization on the downstream task. Code is available at https://github.com/OliverRensu/D-iGPT.",
        "_bibtex": "@inproceedings{\nren2024rejuvenating,\ntitle={Rejuvenating image-{GPT} as Strong Visual Representation Learners},\nauthor={Sucheng Ren and Zeyu Wang and Hongru Zhu and Junfei Xiao and Alan Yuille and Cihang Xie},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=mzGtunvpJH}\n}"
    },
    {
        "title": "High-Probability Convergence for Composite and Distributed Stochastic Minimization and Variational Inequalities with Heavy-Tailed Noise",
        "authorids": [
            "~Eduard_Gorbunov1",
            "~Abdurakhmon_Sadiev1",
            "~Marina_Danilova1",
            "~Samuel_Horv\u00e1th1",
            "~Gauthier_Gidel1",
            "~Pavel_Dvurechensky1",
            "~Alexander_Gasnikov1",
            "~Peter_Richt\u00e1rik1"
        ],
        "abstract": "High-probability analysis of stochastic first-order optimization methods under mild assumptions on the noise has been gaining a lot of attention in recent years. Typically, gradient clipping is one of the key algorithmic ingredients to derive good high-probability guarantees when the noise is heavy-tailed. However, if implemented naively, clipping can spoil the convergence of the popular methods for composite and distributed optimization (Prox-SGD/Parallel SGD) even in the absence of any noise. Due to this reason, many works on high-probability analysis consider only unconstrained non-distributed problems, and the existing results for composite/distributed problems do not include some important special cases (like strongly convex problems) and are not optimal. To address this issue, we propose new stochastic methods for composite and distributed optimization based on the clipping of stochastic gradient differences and prove tight high-probability convergence results (including nearly optimal ones) for the new methods. In addition, we also develop new methods for composite and distributed variational inequalities and analyze the high-probability convergence of these methods.",
        "_bibtex": "@inproceedings{\ngorbunov2024highprobability,\ntitle={High-Probability Convergence for Composite and Distributed Stochastic Minimization and Variational Inequalities with Heavy-Tailed Noise},\nauthor={Eduard Gorbunov and Abdurakhmon Sadiev and Marina Danilova and Samuel Horv{\\'a}th and Gauthier Gidel and Pavel Dvurechensky and Alexander Gasnikov and Peter Richt{\\'a}rik},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=DBI6AuCD4a}\n}"
    },
    {
        "title": "InfoNet: Neural Estimation of Mutual Information without Test-Time Optimization",
        "authorids": [
            "~Zhengyang_Hu1",
            "~Song_Kang1",
            "~Qunsong_Zeng1",
            "~Kaibin_Huang1",
            "~Yanchao_Yang1"
        ],
        "abstract": "Estimating mutual correlations between random variables or data streams is essential for intelligent behavior and decision-making. As a fundamental quantity for measuring statistical relationships, mutual information has been extensively studied and utilized for its generality and equitability. However, existing methods often lack the efficiency needed for real-time applications, such as test-time optimization of a neural network, or the differentiability required for end-to-end learning, like histograms. We introduce a neural network called InfoNet, which directly outputs mutual information estimations of data streams by leveraging the attention mechanism and the computational efficiency of deep learning infrastructures. By maximizing a dual formulation of mutual information through large-scale simulated training, our approach circumvents time-consuming test-time optimization and offers generalization ability. We evaluate the effectiveness and generalization of our proposed mutual information estimation scheme on various families of distributions and applications. Our results demonstrate that InfoNet and its training process provide a graceful efficiency-accuracy trade-off and order-preserving properties. We will make the code and models available as a comprehensive toolbox to facilitate studies in different fields requiring real-time mutual information estimation.",
        "_bibtex": "@inproceedings{\nhu2024neural,\ntitle={Neural Estimation of Mutual Information without Test-Time Optimization},\nauthor={Zhengyang Hu and Song Kang and Qunsong Zeng and Kaibin Huang and Yanchao Yang},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=40hCy8n5XH}\n}"
    },
    {
        "title": "NExT-GPT: Any-to-Any Multimodal LLM",
        "authorids": [
            "~Shengqiong_Wu2",
            "~Hao_Fei1",
            "~Leigang_Qu1",
            "~Wei_Ji1",
            "~Tat-Seng_Chua2"
        ],
        "abstract": "While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, image, video, and audio. By leveraging the existing well-trained high-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training but also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building a unified AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community.",
        "_bibtex": "@inproceedings{\nwu2024nextgpt,\ntitle={{NE}xT-{GPT}: Any-to-Any Multimodal {LLM}},\nauthor={Shengqiong Wu and Hao Fei and Leigang Qu and Wei Ji and Tat-Seng Chua},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=NZQkumsNlf}\n}"
    },
    {
        "title": "Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs",
        "authorids": [
            "~Yeonhong_Park1",
            "~Jake_Hyun1",
            "~SangLyul_Cho1",
            "~Bonggeun_Sim1",
            "~Jae_W._Lee1"
        ],
        "abstract": "Recently, considerable efforts have been directed towards compressing Large Language Models (LLMs), which showcase groundbreaking capabilities across diverse applications but entail significant deployment costs due to their large sizes. Meanwhile, much less attention has been given to mitigating the costs associated with deploying multiple LLMs of varying sizes despite its practical significance. Thus, this paper introduces any-precision LLM, extending the concept of any-precision DNN to LLMs. Addressing challenges in any-precision LLM, we propose a lightweight method for any-precision quantization of LLMs, leveraging a post-training quantization framework, and develop a specialized software engine for its efficient serving. As a result, our solution significantly reduces the high costs of deploying multiple, different-sized LLMs by overlaying LLMs quantized to varying bit-widths, such as 3, 4, ..., $n$ bits, into a memory footprint comparable to a single $n$-bit LLM. All the supported LLMs with varying bit-widths demonstrate state-of-the-art model quality and inference throughput, proving itself to be a compelling option for deployment of multiple, different-sized LLMs.",
        "_bibtex": "@inproceedings{\npark2024anyprecision,\ntitle={Any-Precision {LLM}: Low-Cost Deployment of Multiple, Different-Sized {LLM}s},\nauthor={Yeonhong Park and Jake Hyun and SangLyul Cho and Bonggeun Sim and Jae W. Lee},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=u09gadH3BU}\n}"
    },
    {
        "title": "Private Truly-Everlasting Robust-Prediction",
        "authorids": [
            "~Uri_Stemmer1"
        ],
        "abstract": "Private everlasting prediction (PEP), recently introduced by Naor et al. [2023], is a model for differentially private learning in which the learner never publicly releases a hypothesis. Instead, it provides black-box access to a \"prediction oracle\" that can predict the labels of an *endless stream* of unlabeled examples drawn from the underlying distribution. Importantly, PEP provides privacy both for the initial training set and for the endless stream of classification queries. We present two conceptual modifications to the definition of PEP, as well as new constructions exhibiting significant improvements over prior work. Specifically, we incorporate robustness against poisoning attacks into the definition of PEP; we present a relaxed privacy definition, suitable for PEP, that allows us to disconnect the privacy parameter $\\delta$ from the number of total time steps $T$; and we present new constructions for axis-aligned rectangles and decision-stumps exhibiting improved sample complexity and runtime.",
        "_bibtex": "@inproceedings{\nstemmer2024private,\ntitle={Private Truly-Everlasting Robust-Prediction},\nauthor={Uri Stemmer},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=BdQTCAuT6L}\n}"
    },
    {
        "title": "ViP: A Differentially Private Foundation Model for Computer Vision",
        "authorids": [
            "~Yaodong_Yu4",
            "~Maziar_Sanjabi1",
            "~Yi_Ma4",
            "~Kamalika_Chaudhuri1",
            "~Chuan_Guo1"
        ],
        "abstract": "Artificial intelligence (AI) has seen a tremendous surge in capabilities thanks to the use of foundation models trained on internet-scale data. On the flip side, the uncurated nature of internet-scale data also poses significant privacy and legal risks, as they often contain personal information or copyrighted material that should not be trained on without permission. In this work, we propose as a mitigation measure a recipe to train foundation vision models via self-supervised learning with differential privacy (DP) guarantee. We identify masked autoencoders as a suitable learning algorithm that aligns well with DP-SGD, and train *ViP*---a **Vi**sion transformer with differential **P**rivacy---under a strict privacy budget of $\\epsilon=8$ on the LAION400M dataset. We evaluate the quality of representation learned by ViP using standard downstream vision tasks; in particular, ViP achieves a (non-private) linear probing accuracy of 55.7% on ImageNet, comparable to that of end-to-end trained AlexNet (trained and evaluated on ImageNet). Our result suggests that scaling to internet-scale data can be practical for private learning. Code and DP pre-trained models are available at https://github.com/facebookresearch/ViP-MAE.",
        "_bibtex": "@inproceedings{\nyu2024vip,\ntitle={ViP: A Differentially Private Foundation Model for Computer Vision},\nauthor={Yaodong Yu and Maziar Sanjabi and Yi Ma and Kamalika Chaudhuri and Chuan Guo},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=6aKwVmHQI1}\n}"
    },
    {
        "title": "Genie: Generative Interactive Environments",
        "authorids": [
            "~Jake_Bruce1",
            "~Michael_D_Dennis1",
            "~Ashley_Edwards1",
            "~Jack_Parker-Holder1",
            "~Yuge_Shi1",
            "~Edward_Hughes1",
            "~Matthew_Lai1",
            "~Aditi_Mavalankar1",
            "~Richie_Steigerwald1",
            "capps@google.com",
            "~Yusuf_Aytar1",
            "~Sarah_Maria_Elisabeth_Bechtle1",
            "~Feryal_Behbahani1",
            "~Stephanie_C.Y._Chan1",
            "~Nicolas_Heess1",
            "lucygps@google.com",
            "~Simon_Osindero1",
            "~Sherjil_Ozair1",
            "~Scott_Reed1",
            "~Jingwei_Zhang2",
            "~Konrad_Zolna1",
            "~Jeff_Clune3",
            "~Nando_de_Freitas1",
            "~Satinder_Singh2",
            "~Tim_Rockt\u00e4schel1"
        ],
        "abstract": "We introduce Genie, the first *generative interactive environment* trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a *foundation world model*. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis *despite training without any ground-truth action labels* or other domain specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future.",
        "_bibtex": "@inproceedings{\nbruce2024genie,\ntitle={Genie: Generative Interactive Environments},\nauthor={Jake Bruce and Michael D Dennis and Ashley Edwards and Jack Parker-Holder and Yuge Shi and Edward Hughes and Matthew Lai and Aditi Mavalankar and Richie Steigerwald and Chris Apps and Yusuf Aytar and Sarah Maria Elisabeth Bechtle and Feryal Behbahani and Stephanie C.Y. Chan and Nicolas Heess and Lucy Gonzalez and Simon Osindero and Sherjil Ozair and Scott Reed and Jingwei Zhang and Konrad Zolna and Jeff Clune and Nando de Freitas and Satinder Singh and Tim Rockt{\\\"a}schel},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=bJbSbJskOS}\n}"
    },
    {
        "title": "SAMformer: Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention",
        "authorids": [
            "~Romain_Ilbert1",
            "~Ambroise_Odonnat1",
            "~Vasilii_Feofanov1",
            "~Aladin_Virmaux1",
            "~Giuseppe_Paolo1",
            "~Themis_Palpanas1",
            "~Ievgen_Redko2"
        ],
        "abstract": "Transformer-based architectures achieved breakthrough performance in natural language processing and computer vision, yet they remain inferior to simpler linear baselines in multivariate long-term forecasting. To better understand this phenomenon, we start by studying a toy linear forecasting problem for which we show that transformers are incapable of converging to their true solution despite their high expressive power. We further identify the attention of transformers as being responsible for this low generalization capacity. Building upon this insight, we propose a shallow lightweight transformer model that successfully escapes bad local minima when optimized with sharpness-aware optimization. We empirically demonstrate that this result extends to all commonly used real-world multivariate time series datasets. In particular, SAMformer surpasses current state-of-the-art methods and is on par with the biggest foundation model MOIRAI while having significantly fewer parameters. The code is available at https://github.com/romilbert/samformer.",
        "_bibtex": "@inproceedings{\nilbert2024samformer,\ntitle={{SAM}former: Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention},\nauthor={Romain Ilbert and Ambroise Odonnat and Vasilii Feofanov and Aladin Virmaux and Giuseppe Paolo and Themis Palpanas and Ievgen Redko},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=8kLzL5QBh2}\n}"
    },
    {
        "title": "Data-free Neural Representation Compression with Riemannian Neural Dynamics",
        "authorids": [
            "~Zhengqi_Pei1",
            "~Anran_Zhang2",
            "~Shuhui_Wang1",
            "~Xiangyang_Ji1",
            "~Qingming_Huang1"
        ],
        "abstract": "Neural models are equivalent to dynamic systems from a physics-inspired view, implying that computation on neural networks can be interpreted as the dynamical interactions between neurons. However, existing work models neuronal interaction as a weight-based linear transformation, and the nonlinearity comes from the nonlinear activation functions, which leads to limited nonlinearity and data-fitting ability of the whole neural model. Inspired by Riemannian geometry, we interpret neural structures by projecting neurons onto the Riemannian neuronal state space and model neuronal interaction with Riemannian metric (${\\it RieM}$), which provides a more efficient neural representation with higher parameter efficiency. With ${\\it RieM}$, we further design a novel data-free neural compression mechanism that does not require additional fine-tuning with real data. Using backbones like ResNet and Vision Transformer, we conduct extensive experiments on datasets such as MNIST, CIFAR-100, ImageNet-1k, and COCO object detection. Empirical results show that, under equal compression rates and computational complexity, models compressed with ${\\it RieM}$ achieve superior inference accuracy compared to existing data-free compression methods.",
        "_bibtex": "@inproceedings{\npei2024datafree,\ntitle={Data-free Neural Representation Compression with Riemannian Neural Dynamics},\nauthor={Zhengqi Pei and Anran Zhang and Shuhui Wang and Xiangyang Ji and Qingming Huang},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=LTifAl5bKb}\n}"
    },
    {
        "title": "Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition",
        "authorids": [
            "~Hao_Fei1",
            "~Shengqiong_Wu2",
            "~Wei_Ji1",
            "~Hanwang_Zhang3",
            "~Meishan_Zhang1",
            "~Mong-Li_Lee1",
            "~Wynne_Hsu1"
        ],
        "abstract": "Existing research of video understanding still struggles to achieve in-depth comprehension and reasoning in complex videos, primarily due to the under-exploration of two key bottlenecks: fine-grained spatial-temporal perceptive understanding and cognitive-level video scene comprehension. This paper bridges the gap by presenting a novel solution. We first introduce a novel video Multimodal Large Language Model (MLLM), MotionEpic, which achieves fine-grained pixel-level spatial-temporal video grounding by integrating video spatial-temporal scene graph (STSG) representation. Building upon MotionEpic, we then develop a Video-of-Thought (VoT) reasoning framework. VoT inherits the Chain-of-Thought (CoT) core, breaking down a complex task into simpler and manageable sub-problems, and addressing them step-by-step from a low-level pixel perception to high-level cognitive interpretation. Extensive experiments across various complex video QA benchmarks demonstrate that our overall framework strikingly boosts existing state-of-the-art. To our knowledge, this is the first attempt at successfully implementing the CoT technique for achieving human-level video reasoning, where we show great potential in extending it to a wider range of video understanding scenarios. Systems and codes will be open later.",
        "_bibtex": "@inproceedings{\nfei2024videoofthought,\ntitle={Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition},\nauthor={Hao Fei and Shengqiong Wu and Wei Ji and Hanwang Zhang and Meishan Zhang and Mong-Li Lee and Wynne Hsu},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=fO31YAyNbI}\n}"
    },
    {
        "title": "Test-Time Model Adaptation with Only Forward Passes",
        "authorids": [
            "~Shuaicheng_Niu1",
            "~Chunyan_Miao1",
            "~Guohao_Chen1",
            "~Pengcheng_Wu1",
            "~Peilin_Zhao2"
        ],
        "abstract": "Test-time adaptation has proven effective in adapting a given trained model to unseen test samples with potential distribution shifts. However, in real-world scenarios, models are usually deployed on resource-limited devices, e.g., FPGAs, and are often quantized and hard-coded with non-modifiable parameters for acceleration. In light of this, existing methods are often infeasible since they heavily depend on computation-intensive backpropagation for model updating that may be not supported. To address this, we propose a test-time Forward-Optimization Adaptation (FOA) method. In FOA, we seek to solely learn a newly added prompt (as model's input) via a derivative-free covariance matrix adaptation evolution strategy. To make this strategy work stably under our online unsupervised setting, we devise a novel fitness function by measuring test-training statistic discrepancy and model prediction entropy. Moreover, we design an activation shifting scheme that directly tunes the model activations for shifted test samples, making them align with the source training domain, thereby further enhancing adaptation performance. Without using any backpropagation and altering model weights, FOA runs on quantized 8-bit ViT outperforms gradient-based TENT on full-precision 32-bit ViT, while achieving an up to *24*-fold memory reduction on ImageNet-C. The source code is available at: https://github.com/mr-eggplant/FOA.",
        "_bibtex": "@inproceedings{\nniu2024testtime,\ntitle={Test-Time Model Adaptation with Only Forward Passes},\nauthor={Shuaicheng Niu and Chunyan Miao and Guohao Chen and Pengcheng Wu and Peilin Zhao},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=qz1Vx1v9iK}\n}"
    },
    {
        "title": "MorphGrower: A Synchronized Layer-by-layer Growing Approach for Plausible Neuronal Morphology Generation",
        "authorids": [
            "~Nianzu_Yang1",
            "~Kaipeng_Zeng1",
            "~Haotian_Lu1",
            "~Yexin_Wu2",
            "~Zexin_Yuan1",
            "~Danni_Chen1",
            "~Shengdian_Jiang1",
            "~Jiaxiang_Wu1",
            "~Yimin_Wang3",
            "~Junchi_Yan2"
        ],
        "abstract": "Neuronal morphology is essential for studying brain functioning and understanding neurodegenerative disorders. As acquiring real-world morphology data is expensive, computational approaches for morphology generation have been studied. Traditional methods heavily rely on expert-set rules and parameter tuning, making it difficult to generalize across different types of morphologies. Recently, MorphVAE was introduced as the sole learning-based method, but its generated morphologies lack plausibility, i.e., they do not appear realistic enough and most of the generated samples are topologically invalid. To fill this gap, this paper proposes **MorphGrower**, which mimicks the neuron natural growth mechanism for generation. Specifically, MorphGrower generates morphologies layer by layer, with each subsequent layer conditioned on the previously generated structure. During each layer generation, MorphGrower utilizes a pair of sibling branches as the basic generation block and generates branch pairs synchronously. This approach ensures topological validity and allows for fine-grained generation, thereby enhancing the realism of the final generated morphologies. Results on four real-world datasets demonstrate that MorphGrower outperforms MorphVAE by a notable margin. Importantly, the electrophysiological response simulation demonstrates the plausibility of our generated samples from a neuroscience perspective. Our code is available at [https://github.com/Thinklab-SJTU/MorphGrower](https://github.com/Thinklab-SJTU/MorphGrower).",
        "_bibtex": "@inproceedings{\nyang2024morphgrower,\ntitle={MorphGrower: A Synchronized Layer-by-layer Growing Approach for Plausible Neuronal Morphology Generation},\nauthor={Nianzu Yang and Kaipeng Zeng and Haotian Lu and Yexin Wu and Zexin Yuan and Danni Chen and Shengdian Jiang and Jiaxiang Wu and Yimin Wang and Junchi Yan},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=ZTN866OsGx}\n}"
    },
    {
        "title": "Flextron: Many-in-One Flexible Large Language Model",
        "authorids": [
            "~Ruisi_Cai1",
            "~Saurav_Muralidharan1",
            "~Greg_Heinrich1",
            "~Hongxu_Yin2",
            "~Zhangyang_Wang1",
            "~Jan_Kautz1",
            "~Pavlo_Molchanov1"
        ],
        "abstract": "Training modern LLMs is extremely resource intensive, and customizing them for various deployment scenarios characterized by limited compute and memory resources through repeated training is impractical. In this paper, we introduce Flextron, a network architecture and post-training model optimization framework supporting flexible model deployment. The Flextron architecture utilizes a nested elastic structure to rapidly adapt to specific user-defined latency and accuracy targets during inference with no additional fine-tuning required. It is also input-adaptive, and can automatically route tokens through its sub-networks for improved performance and efficiency. We present a sample-efficient training method and associated routing algorithms for systematically transforming an existing trained LLM into a Flextron model. We evaluate Flextron on the GPT-3 and LLama-2 family of LLMs, and demonstrate superior performance over multiple end-to-end trained variants and other state-of-the-art elastic networks, all with a single pretraining run that consumes a mere 7.63% tokens compared to original pretraining.",
        "_bibtex": "@inproceedings{\ncai2024flextron,\ntitle={Flextron: Many-in-One Flexible Large Language Model},\nauthor={Ruisi Cai and Saurav Muralidharan and Greg Heinrich and Hongxu Yin and Zhangyang Wang and Jan Kautz and Pavlo Molchanov},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=9vKRhnflAs}\n}"
    },
    {
        "title": "Mean-field Chaos Diffusion Models",
        "authorids": [
            "~Sungwoo_Park3",
            "~Dongjun_Kim1",
            "~Ahmed_Alaa1"
        ],
        "abstract": "In this paper, we introduce a new class of score-based generative models (SGMs) designed to handle high-cardinality data distributions by leveraging concepts from mean-field theory. We present mean-field chaos diffusion models (MF-CDMs), which address the curse of dimensionality inherent in high-cardinality data by utilizing the propagation of chaos property of interacting particles. By treating high-cardinality data as a large stochastic system of interacting particles, we develop a novel score-matching method for infinite-dimensional chaotic particle systems and propose an approximation scheme that employs a subdivision strategy for efficient training. Our theoretical and empirical results demonstrate the scalability and effectiveness of MF-CDMs for managing large high-cardinality data structures, such as 3D point clouds.",
        "_bibtex": "@inproceedings{\npark2024meanfield,\ntitle={Mean-field Chaos Diffusion Models},\nauthor={Sungwoo Park and Dongjun Kim and Ahmed Alaa},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=lgcFX4VFrM}\n}"
    },
    {
        "title": "Principled Preferential Bayesian Optimization",
        "authorids": [
            "~Wenjie_Xu3",
            "~Wenbin_Wang6",
            "~Yuning_Jiang5",
            "~Bratislav_Svetozarevic1",
            "~Colin_Jones1"
        ],
        "abstract": "We study the problem of preferential Bayesian optimization (BO), where we aim to optimize a black-box function with only preference feedback over a pair of candidate solutions. Inspired by the likelihood ratio idea, we construct a confidence set of the black-box function using only the preference feedback. An optimistic algorithm with an efficient computational method is then developed to solve the problem, which enjoys an information-theoretic bound on the total cumulative regret, a first-of-its-kind for preferential BO. This bound further allows us to design a scheme to report an estimated best solution, with a guaranteed convergence rate. Experimental results on sampled instances from Gaussian processes, standard test functions, and a thermal comfort optimization problem all show that our method stably achieves better or competitive performance as compared to the existing state-of-the-art heuristics, which, however, do not have theoretical guarantees on regret bounds or convergence.",
        "_bibtex": "@inproceedings{\nxu2024principled,\ntitle={Principled Preferential Bayesian Optimization},\nauthor={Wenjie Xu and Wenbin Wang and Yuning Jiang and Bratislav Svetozarevic and Colin Jones},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=YqMOM5W9GF}\n}"
    },
    {
        "title": "Preference Optimization for Molecule Synthesis with Conditional Residual Energy-based Models",
        "authorids": [
            "~Songtao_Liu2",
            "~Hanjun_Dai1",
            "~Yue_Zhao13",
            "~Peng_Liu3"
        ],
        "abstract": "Molecule synthesis through machine learning is one of the fundamental problems in drug discovery. Current data-driven strategies employ one-step retrosynthesis models and search algorithms to predict synthetic routes in a top-bottom manner. Despite their effective performance, these strategies face limitations in the molecule synthetic route generation due to a greedy selection of the next molecule set without any lookahead. Furthermore, existing strategies cannot control the generation of synthetic routes based on possible criteria such as material costs, yields, and step count. In this work, we propose a general and principled framework via conditional residual energy-based models (EBMs), that focus on the quality of the entire synthetic route based on the specific criteria. By incorporating an additional energy-based function into our probabilistic model, our proposed algorithm can enhance the quality of the most probable synthetic routes (with higher probabilities) generated by various strategies in a plug-and-play fashion. Extensive experiments demonstrate that our framework can consistently boost performance across various strategies and outperforms previous state-of-the-art top-1 accuracy by a margin of 2.5%. Code is available at https://github.com/SongtaoLiu0823/CREBM.",
        "_bibtex": "@inproceedings{\nliu2024preference,\ntitle={Preference Optimization for Molecule Synthesis with Conditional Residual Energy-based Models},\nauthor={Songtao Liu and Hanjun Dai and Yue Zhao and Peng Liu},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=oLfq1KKneW}\n}"
    },
    {
        "title": "Accurate LoRA-Finetuning Quantization of LLMs via Information Retention",
        "authorids": [
            "~Haotong_Qin1",
            "~Xudong_Ma3",
            "~Xingyu_Zheng1",
            "lixiaoyang.x@bytedance.com",
            "~Yang_Zhang21",
            "liushouda@bytedance.com",
            "~Jie_Luo5",
            "~Xianglong_Liu3",
            "~Michele_Magno1"
        ],
        "abstract": "The LoRA-finetuning quantization of LLMs has been extensively studied to obtain accurate yet compact LLMs for deployment on resource-constrained hardware. However, existing methods cause the quantized LLM to severely degrade and even fail to benefit from the finetuning of LoRA. This paper proposes a novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate through information retention. The proposed IR-QLoRA mainly relies on two technologies derived from the perspective of unified information: (1) statistics-based Information Calibration Quantization allows the quantized parameters of LLM to retain original information accurately; (2) finetuning-based Information Elastic Connection makes LoRA utilizes elastic representation transformation with diverse information. Comprehensive experiments show that IR-QLoRA can significantly improve accuracy across LLaMA and LLaMA2 families under 2-4 bit-widths, e.g., 4-bit LLaMA-7B achieves 1.4% improvement on MMLU compared with the state-of-the-art methods. The significant performance gain requires only a tiny 0.31% additional time consumption, revealing the satisfactory efficiency of our IR-QLoRA. We highlight that IR-QLoRA enjoys excellent versatility, compatible with various frameworks (e.g., NormalFloat and Integer quantization) and brings general accuracy gains. The code is available at https://github.com/htqin/ir-qlora .",
        "_bibtex": "@inproceedings{\nqin2024accurate,\ntitle={Accurate Lo{RA}-Finetuning Quantization of {LLM}s via Information Retention},\nauthor={Haotong Qin and Xudong Ma and Xingyu Zheng and Xiaoyang Li and Yang Zhang and Shouda Liu and Jie Luo and Xianglong Liu and Michele Magno},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=jQ92egz5Ym}\n}"
    },
    {
        "title": "DiJiang: Efficient Large Language Models through Compact Kernelization",
        "authorids": [
            "~Hanting_Chen1",
            "~Liuzhicheng2",
            "~Xutao_Wang1",
            "~Yuchuan_Tian1",
            "~Yunhe_Wang1"
        ],
        "abstract": "In an effort to reduce the computational load of Transformers, research on linear attention has gained significant momentum. However, the improvement strategies for attention mechanisms typically necessitate extensive retraining, which is impractical for large language models with a vast array of parameters. In this paper, we present DiJiang, a novel Frequency Domain Kernelization approach that enables the transformation of a pre-trained vanilla Transformer into a linear complexity model with little training costs. By employing a weighted Quasi-Monte Carlo method for sampling, the proposed approach theoretically offers superior approximation efficiency. To further reduce the training computational complexity, our kernelization is based on Discrete Cosine Transform (DCT) operations. Extensive experiments demonstrate that the proposed method achieves comparable performance to the original Transformer, but with significantly reduced training costs and much faster inference speeds. Our DiJiang-7B achieves comparable performance with LLaMA2-7B on various benchmark while requires only about 1/50 training cost. Code is available at https://github.com/YuchuanTian/DiJiang.",
        "_bibtex": "@inproceedings{\nchen2024dijiang,\ntitle={DiJiang: Efficient Large Language Models through Compact Kernelization},\nauthor={Hanting Chen and Liuzhicheng and Xutao Wang and Yuchuan Tian and Yunhe Wang},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=0uUHfhXdnH}\n}"
    },
    {
        "title": "GPTSwarm: Language Agents as Optimizable Graphs",
        "authorids": [
            "~Mingchen_Zhuge2",
            "~Wenyi_Wang1",
            "~Louis_Kirsch1",
            "~Francesco_Faccio1",
            "~Dmitrii_Khizbullin2",
            "~J\u00fcrgen_Schmidhuber1"
        ],
        "abstract": "Various human-designed prompt engineering techniques have been proposed to improve problem solvers based on Large Language Models (LLMs), yielding many disparate code bases. We unify these approaches by describing LLM-based agents as computational graphs. The nodes implement functions to process multimodal data or query LLMs, and the edges describe the information flow between operations. Graphs can be recursively combined into larger composite graphs representing hierarchies of inter-agent collaboration (where edges connect operations of different agents). Our novel automatic graph optimizers (1) refine node-level LLM prompts (node optimization) and (2) improve agent orchestration by changing graph connectivity (edge optimization). Experiments demonstrate that our framework can be used to efficiently develop, integrate, and automatically improve various LLM agents. Our code is public.",
        "_bibtex": "@inproceedings{\nzhuge2024gptswarm,\ntitle={{GPTS}warm: Language Agents as Optimizable Graphs},\nauthor={Mingchen Zhuge and Wenyi Wang and Louis Kirsch and Francesco Faccio and Dmitrii Khizbullin and J{\\\"u}rgen Schmidhuber},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=uTC9AFXIhg}\n}"
    },
    {
        "title": "DoRA: Weight-Decomposed Low-Rank Adaptation",
        "authorids": [
            "~Shih-yang_Liu1",
            "~Chien-Yi_Wang1",
            "~Hongxu_Yin2",
            "~Pavlo_Molchanov1",
            "~Yu-Chiang_Frank_Wang2",
            "~Kwang-Ting_Cheng1",
            "~Min-Hung_Chen2"
        ],
        "abstract": "Among the widely used parameter-efficient fine-tuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruction tuning, and image/video-text understanding. The code is available at https://github.com/NVlabs/DoRA.",
        "_bibtex": "@inproceedings{\nliu2024dora,\ntitle={Do{RA}: Weight-Decomposed Low-Rank Adaptation},\nauthor={Shih-yang Liu and Chien-Yi Wang and Hongxu Yin and Pavlo Molchanov and Yu-Chiang Frank Wang and Kwang-Ting Cheng and Min-Hung Chen},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=3d5CIRG1n2}\n}"
    },
    {
        "title": "Position: Embracing Negative Results in Machine Learning",
        "authorids": [
            "~Florian_Karl1",
            "malte.kemeter@iis.fraunhofer.de",
            "~Gabriel_Dax1",
            "paulina.sierak@iis.fraunhofer.de"
        ],
        "abstract": "Publications proposing novel machine learning methods are often primarily rated by exhibited predictive performance on selected problems. In this position paper we argue that predictive performance alone is not a good indicator for the worth of a publication. Using it as such even fosters problems like inefficiencies of the machine learning research community as a whole and setting wrong incentives for researchers. We therefore put out a call for the publication of ``negative'' results, which can help alleviate some of these problems and improve the scientific output of the machine learning research community. To substantiate our position, we present the advantages of publishing negative results and provide concrete measures for the community to move towards a paradigm where their publication is normalized.",
        "_bibtex": "@inproceedings{\nkarl2024position,\ntitle={Position: Embracing Negative Results in Machine Learning},\nauthor={Florian Karl and Malte Kemeter and Gabriel Dax and Paulina Sierak},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=3RXAiU7sss}\n}"
    },
    {
        "title": "Does Label Smoothing Help Deep Partial Label Learning?",
        "authorids": [
            "~Xiuwen_Gong2",
            "nitin.bisht@student.uts.edu.au",
            "~Guandong_Xu2"
        ],
        "abstract": "Although deep partial label learning (deep PLL) classifiers have shown their competitive performance, they are heavily influenced by the noisy false-positive labels leading to poorer performance as the training progresses. Meanwhile, existing deep PLL research lacks theoretical guarantee on the analysis of correlation between label noise (or ambiguity degree) and classification performance. This paper addresses the above limitations with label smoothing (LS) from both theoretical and empirical aspects. In theory, we prove lower and upper bounds of the expected risk to show that label smoothing can help deep PLL. We further derive the optimal smoothing rate to investigate the conditions, i.e., when label smoothing benefits deep PLL. In practice, we design a benchmark solution and a novel optimization algorithm called Label Smoothing-based Partial Label Learning (LS-PLL). Extensive experimental results on benchmark PLL datasets and various deep architectures validate that label smoothing does help deep PLL in improving classification performance and learning distinguishable representations, and the best results can be achieved when the empirical smoothing rate approximately approaches the optimal smoothing rate in theoretical findings. Code is publicly available at https://github.com/kalpiree/LS-PLL.",
        "_bibtex": "@inproceedings{\ngong2024does,\ntitle={Does Label Smoothing Help Deep Partial Label Learning?},\nauthor={Xiuwen Gong and Nitin Bisht and Guandong Xu},\nbooktitle={Forty-first International Conference on Machine Learning},\nyear={2024},\nurl={https://openreview.net/forum?id=drjjxmi2Ha}\n}"
    }
]