[
    {
        "title": "Efficiently Deciding Algebraic Equivalence of Bow-Free Acyclic Path Diagrams",
        "authorids": [
            "~Thijs_van_Ommen1"
        ],
        "keywords": [
            "mixed graphs",
            "latent confounders",
            "algebraic models"
        ],
        "abstract": "For causal discovery in the presence of latent confounders, constraints beyond conditional independences exist that can enable causal discovery algorithms to distinguish more pairs of graphs. Such constraints are not well-understood yet. In the setting of linear structural equation models without bows, we study algebraic constraints and argue that these provide the most fine-grained resolution achievable. We propose efficient algorithms that decide whether two graphs impose the same algebraic constraints, or whether the constraints imposed by one graph are a subset of those imposed by another graph.",
        "_bibtex": "@inproceedings{\nommen2024efficiently,\ntitle={Efficiently Deciding Algebraic Equivalence of Bow-Free Acyclic Path Diagrams},\nauthor={Thijs van Ommen},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=0s7uKfEfua}\n}"
    },
    {
        "title": "Neural Architecture Search Finds Robust Models by Knowledge Distillation",
        "authorids": [
            "~Utkarsh_Nath1",
            "~Yancheng_Wang2",
            "~Yingzhen_Yang1"
        ],
        "keywords": [
            "Adversarial Attacks",
            "Neural Architecture Search",
            "Cross-Layer Knowledge Distillation"
        ],
        "abstract": "Despite their superior performance, Deep Neural Networks (DNNs) are often vulnerable to adversarial attacks. Neural Architecture Search (NAS), a method for automatically designing the architectures of DNNs, has shown remarkable performance across various machine learning applications. However, the adversarial robustness of architectures learned by NAS against adversarial threats remains under-explored. By integrating a robust teacher, we examine whether NAS can yield a robust neural architecture by inheriting robustness from the teacher. In this paper, we propose Robust Neural Architecture Search by Cross-Layer Knowledge Distillation (RNAS-CL), a novel NAS algorithm that enhances the robustness of architectures learned by NAS through employing cross-layer knowledge distillation from a robust teacher. Distinct from previous knowledge distillation approaches that only align student-teacher outputs at the final layer, RNAS-CL dynamically searches for the optimal teacher layer to guide each student layer. Our experimental findings validate the effectiveness of RNAS-CL, demonstrating that it can generate both compact and adversarially robust neural architectures. Our results pave the way for developing new strategies for compact and robust neural architecture design applicable across various fields. The code of RNAS-CL is available at \\url{https://github.com/Statistical-Deep-Learning/RNAS-CL}.",
        "_bibtex": "@inproceedings{\nnath2024neural,\ntitle={Neural Architecture Search Finds Robust Models by Knowledge Distillation},\nauthor={Utkarsh Nath and Yancheng Wang and Yingzhen Yang},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=S0nrdTCNEn}\n}"
    },
    {
        "title": "Linear Opinion Pooling for Uncertainty Quantification on Graphs",
        "authorids": [
            "~Clemens_Damke1",
            "~Eyke_H\u00fcllermeier1"
        ],
        "keywords": [
            "Machine learning",
            "uncertainty quantification",
            "graph-structured data",
            "node classification."
        ],
        "abstract": "We address the problem of uncertainty quantification for graph-structured data, or, more specifically, the problem to quantify the predictive uncertainty in (semi-supervised) node classification. Key questions in this regard concern the distinction  between two different types of uncertainty, aleatoric and epistemic, and how to support uncertainty quantification by leveraging the structural information provided by the graph topology. Challenging assumptions and postulates of state-of-the-art methods, we propose a novel approach that represents (epistemic) uncertainty in terms of mixtures of Dirichlet distributions and refers to the established principle of linear opinion pooling for propagating information between neighbored nodes in the graph. The effectiveness of this approach is demonstrated in a series of experiments on a variety of graph-structured datasets.",
        "_bibtex": "@inproceedings{\ndamke2024linear,\ntitle={Linear Opinion Pooling for Uncertainty Quantification on Graphs},\nauthor={Clemens Damke and Eyke H{\\\"u}llermeier},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=qLGkfpXTSn}\n}"
    },
    {
        "title": "Offline Reward Perturbation Boosts Distributional Shift in Online RL",
        "authorids": [
            "~Zishun_Yu1",
            "~Siteng_Kang1",
            "~Xinhua_Zhang3"
        ],
        "keywords": [
            "data poisoning attack",
            "machine learning safety",
            "offline to online reinforcement learning"
        ],
        "abstract": "Offline-to-online reinforcement learning has recently been shown effective in reducing the online sample complexity by first training from offline collected data. However, this additional data source may also invite new poisoning attacks that target offline training. In this work, we reveal such vulnerabilities in $\\textit{critic-regularized}$ offline RL by proposing a novel data poisoning attack method, which is stealthy in the sense that the performance during the offline training remains intact, but the online fine-tuning stage will suffer a significant performance drop. Our method leverages the techniques from bi-level optimization to promote the over-estimation/distribution shift under offline-to-online reinforcement learning. Experiments on four environments confirm the satisfaction of the new stealthiness requirement,\nand can be effective in attacking with only a small budget and without having white-box access to the victim model.",
        "_bibtex": "@inproceedings{\nyu2024offline,\ntitle={Offline Reward Perturbation Boosts Distributional Shift in Online {RL}},\nauthor={Zishun Yu and Siteng Kang and Xinhua Zhang},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=wbwTF909Ve}\n}"
    },
    {
        "title": "Evaluating Bayesian deep learning for radio galaxy classification",
        "authorids": [
            "~Devina_Mohan1",
            "~Anna_M_M_Scaife1"
        ],
        "keywords": [
            "bayesian neural networks",
            "variational inference",
            "radio astronomy",
            "distribution shift",
            "uncertainty calibration"
        ],
        "abstract": "The radio astronomy community is rapidly adopting deep learning techniques to deal with the huge data volumes expected from the next generation of radio observatories. Bayesian neural networks (BNNs) provide a principled way to model uncertainty in the predictions made by such deep learning models and will play an important role in extracting well-calibrated uncertainty estimates on their outputs. In this work, we evaluate the performance of different BNNs against the following criteria: predictive performance, uncertainty calibration and distribution-shift detection for the radio galaxy classification problem.",
        "_bibtex": "@inproceedings{\nmohan2024evaluating,\ntitle={Evaluating Bayesian deep learning for radio galaxy classification},\nauthor={Devina Mohan and Anna M M Scaife},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=JX5Rp1Nuzv}\n}"
    },
    {
        "title": "Sound Heuristic Search Value Iteration for Undiscounted POMDPs with Reachability Objectives",
        "authorids": [
            "~Qi_Heng_Ho1",
            "~Martin_S._Feather1",
            "~Federico_Rossi1",
            "~Zachary_N_Sunberg1",
            "~Morteza_Lahijanian1"
        ],
        "keywords": [
            "Partially Observable Markov Decision Processes",
            "Planning under Uncertainty",
            "Probabilistic Model Checking",
            "Heuristic Search",
            "Point Based Methods"
        ],
        "abstract": "Partially Observable Markov Decision Processes (POMDPs) are powerful models for sequential decision making under transition and observation uncertainties. This paper studies the challenging yet important problem in POMDPs known as the (indefinite-horizon) Maximal Reachability Probability Problem (MRPP), where the goal is to maximize the probability of reaching some target states.  This is also a core problem in model checking with logical specifications and is naturally undiscounted (discount factor is one). Inspired by the success of point-based methods developed for discounted problems, we study their extensions to MRPP. Specifically, we focus on trial-based heuristic search value iteration techniques and present a novel algorithm that leverages the strengths of these techniques for efficient exploration of the belief space (informed search via value bounds) while addressing their drawbacks in handling loops for indefinite-horizon problems. The algorithm produces policies with two-sided bounds on optimal reachability probabilities. We prove convergence to an optimal policy from below under certain conditions. Experimental evaluations on a suite of benchmarks show that our algorithm outperforms existing methods in almost all cases in both probability guarantees and computation time.",
        "_bibtex": "@inproceedings{\nho2024sound,\ntitle={Sound Heuristic Search Value Iteration for Undiscounted {POMDP}s with Reachability Objectives},\nauthor={Qi Heng Ho and Martin S. Feather and Federico Rossi and Zachary N Sunberg and Morteza Lahijanian},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=3zSiuXYtqf}\n}"
    },
    {
        "title": "On the Inductive Biases of Demographic Parity-based Fair Learning Algorithms",
        "authorids": [
            "~Haoyu_LEI1",
            "~Amin_Gohari1",
            "~Farzan_Farnia1"
        ],
        "keywords": [
            "Fairness in Machine Learning",
            "Demographic Parity",
            "Inductive Bias"
        ],
        "abstract": "Fair supervised learning algorithms assigning labels with little dependence on a sensitive attribute have attracted great attention in the machine learning community. While the demographic parity (DP)  notion has been frequently used to measure a model's fairness in training fair classifiers, several studies in the literature suggest potential impacts of enforcing DP in fair learning algorithms. In this work, we analytically study the effect of standard DP-based regularization methods on the conditional distribution of the predicted label given the sensitive attribute. Our analysis shows that an imbalanced training dataset with a non-uniform distribution of the sensitive attribute could lead to a classification rule biased toward the sensitive attribute outcome holding the majority of training data. To control such inductive biases in DP-based fair learning, we propose a sensitive attribute-based distributionally robust optimization (SA-DRO) method improving robustness against the marginal distribution of the sensitive attribute. Finally, we present several numerical results on the application of DP-based learning methods to standard centralized and distributed learning problems. The empirical findings support our theoretical results on the inductive biases in DP-based fair learning algorithms and the debiasing effects of the proposed SA-DRO method. The project code is available at [github.com/lh218/Fairness-IB.git](https://github.com/lh218/Fairness-IB.git).",
        "_bibtex": "@inproceedings{\nlei2024on,\ntitle={On the Inductive Biases of Demographic Parity-based Fair Learning Algorithms},\nauthor={Haoyu LEI and Amin Gohari and Farzan Farnia},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=r1t2m5IDVV}\n}"
    },
    {
        "title": "Causal Discovery with Deductive Reasoning: One Less Problem",
        "authorids": [
            "~Jonghwan_Kim1",
            "~Inwoo_Hwang1",
            "~Sanghack_Lee1"
        ],
        "keywords": [
            "causal discovery",
            "deductive reasoning"
        ],
        "abstract": "Constraint-based causal discovery algorithms aim to extract causal relationships between variables of interest by using conditional independence tests (CITs). However, CITs with large conditioning sets often lead to unreliable results due to their low statistical power, propagating errors throughout the course of causal discovery. As the reliability of CITs is crucial for their practical applicability, recent approaches rely on either tricky heuristics or complicated routines with high computational costs to tackle inconsistent test results. Against this background, we propose a principled, simple, yet effective method, coined \\textsc{deduce-dep}, which corrects unreliable conditional independence statements by replacing them with deductively reasoned results from lower-order CITs. An appealing property of \\textsc{deduce-dep} is that it can be seamlessly plugged into existing constraint-based methods and serves as a modular subroutine. In particular, we showcase the integration of \\textsc{deduce-dep} into representative algorithms such as HITON-PC and PC, illustrating its practicality. Empirical evaluation demonstrates that our method properly corrects unreliable CITs, leading to improved performance in causal structure learning.",
        "_bibtex": "@inproceedings{\nkim2024causal,\ntitle={Causal Discovery with Deductive Reasoning: One Less Problem},\nauthor={Jonghwan Kim and Inwoo Hwang and Sanghack Lee},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=HmhAFOD1Bz}\n}"
    },
    {
        "title": "Vertical Validation: Evaluating Implicit Generative Models for Graphs on Thin Support Regions",
        "authorids": [
            "~Mai_Elkady1",
            "~Thu_Bui1",
            "~Bruno_Ribeiro1",
            "~David_I._Inouye1"
        ],
        "keywords": [
            "Graph Generative Models Evaluation",
            "Evaluation",
            "Implicit Generative Models",
            "Metrics",
            "Distribution Shist",
            "Generative Models"
        ],
        "abstract": "There has been a growing excitement that implicit graph generative models could be used to design or discover new molecules for medicine or material design. Because these molecules have not been discovered, they naturally lie in unexplored or scarcely supported regions of the distribution of known molecules. However, prior evaluation methods for implicit graph generative models have focused on validating statistics computed from the thick support (e.g., mean and variance of a graph property). Therefore, there is a mismatch between the goal of generating novel graphs and the evaluation methods. To address this evaluation gap, we design a novel evaluation method called Vertical Validation (VV) that systematically creates thin support regions during the train-test splitting procedure and then reweights generated samples so that they can be compared to the held-out test data. This procedure can be seen as a generalization of the standard train-test procedure except that the splits are dependent on sample features. We demonstrate that our method can be used to perform model selection if performance on thin support regions is the desired goal. As a side benefit, we also show that our approach can better detect overfitting as exemplified by memorization.",
        "_bibtex": "@inproceedings{\nelkady2024vertical,\ntitle={Vertical Validation: Evaluating Implicit Generative Models for Graphs on Thin Support Regions},\nauthor={Mai Elkady and Thu Bui and Bruno Ribeiro and David I. Inouye},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=u5zVjFaxSs}\n}"
    },
    {
        "title": "Value-Based Abstraction Functions for Abstraction Sampling",
        "authorids": [
            "~Bobak_Pezeshki1",
            "~Kalev_Kask1",
            "~Alexander_Ihler1",
            "~Rina_Dechter1"
        ],
        "keywords": [
            "Abstraction",
            "Sampling",
            "Z",
            "Partition Function",
            "Normalizing Constant",
            "Graphical Models",
            "Bayesian Networks",
            "Probability"
        ],
        "abstract": "Monte Carlo methods are powerful tools for solving problems involving complex probability distributions. Despite their versatility, these methods often suffer from inefficiencies, especially when dealing with rare events. As such, importance sampling emerged as a prominent technique for alleviating these challenges. Recently, a new scheme called Abstraction Sampling was developed that incorporated stratification to importance sampling over graphical models. However, existing work only explored a limited set of abstraction functions that guide  stratification. This study introduces three new classes of abstraction functions combined with seven distinct partitioning schemes, resulting in twenty-one new abstraction functions, each motivated by theory and intuition from both search and sampling domains. An extensive empirical analysis on over 400 problems compares these new schemes highlighting several well-performing candidates.",
        "_bibtex": "@inproceedings{\npezeshki2024valuebased,\ntitle={Value-Based Abstraction Functions for Abstraction Sampling},\nauthor={Bobak Pezeshki and Kalev Kask and Alexander Ihler and Rina Dechter},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=S000gKUEmP}\n}"
    },
    {
        "title": "Stein Random Feature Regression",
        "authorids": [
            "~Houston_Warren1",
            "~Rafael_Oliveira1",
            "~Fabio_Ramos1"
        ],
        "keywords": [
            "gaussian processes",
            "kernel learning",
            "random fourier features",
            "stein variational gradient descent"
        ],
        "abstract": "In large-scale regression problems, random Fourier features (RFFs) have significantly enhanced the computational scalability and flexibility of Gaussian processes (GPs) by defining kernels through their spectral density, from which a finite set of Monte Carlo samples can be used to form an approximate low-rank GP. However, the efficacy of RFFs in kernel approximation and Bayesian kernel learning depends on the ability to tractably sample the kernel spectral measure and the quality of the generated samples. We introduce Stein random features (SRF), leveraging Stein variational gradient descent, which can be used to both generate high-quality RFF samples of known spectral densities as well as flexibly and efficiently approximate traditionally non-analytical spectral measure posteriors. SRFs require only the evaluation of log-probability gradients to perform both kernel approximation and Bayesian kernel learning that results in superior performance over traditional approaches. We empirically validate the effectiveness of SRFs by comparing them to baselines on kernel approximation and well-known GP regression problems.",
        "_bibtex": "@inproceedings{\nwarren2024stein,\ntitle={Stein Random Feature Regression},\nauthor={Houston Warren and Rafael Oliveira and Fabio Ramos},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=IsRWDeQqxf}\n}"
    },
    {
        "title": "Beyond Dirichlet-based Models: When Bayesian Neural Networks Meet Evidential Deep Learning",
        "authorids": [
            "~Hanjing_Wang2",
            "~Qiang_Ji1"
        ],
        "keywords": [
            "Uncertainty Quantification",
            "Bayesian Deep Learning",
            "Evidential Deep Learning"
        ],
        "abstract": "Bayesian neural networks (BNNs) excel in uncertainty quantification (UQ) by estimating the posterior distribution of model parameters, yet face challenges due to the high computational demands of Bayesian inference. Evidential deep learning methods address this by treating target distribution parameters as random variables with a learnable conjugate distribution, enabling efficient UQ. However, there's debate over whether these methods can accurately estimate epistemic uncertainty due to their single-network, sampling-free nature. In this paper, we combine the strengths of both approaches by distilling BNN knowledge into a Dirichlet-based model, endowing it with a Bayesian perspective and theoretical guarantees. Additionally, we introduce two enhancements to further improve the integration of Bayesian UQ with Dirichlet-based models. To relax the heavy computational load with BNNs, we introduce a self-regularized training strategy using Laplacian approximation (LA) for self-distillation. To alleviate the conjugate prior assumption, we employ an expressive normalizing flow for refining the model in a post-processing manner, where a few training iterations can enhance model performance. The experimental results have demonstrated the effectiveness of our proposed methods in both UQ accuracy and robustness.",
        "_bibtex": "@inproceedings{\nwang2024beyond,\ntitle={Beyond Dirichlet-based Models: When Bayesian Neural Networks Meet Evidential Deep Learning},\nauthor={Hanjing Wang and Qiang Ji},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=2Eh7gLpIyG}\n}"
    },
    {
        "title": "Generalization and Learnability in Multiple Instance Regression",
        "authorids": [
            "~Kushal_Chauhan1",
            "~Rishi_Saket1",
            "~Lorne_Applebaum1",
            "~Ashwinkumar_Badanidiyuru1",
            "~Chandan_Giri1",
            "~Aravindan_Raghuveer1"
        ],
        "keywords": [
            "multiple instance regression",
            "generalization",
            "inapproximability"
        ],
        "abstract": "Multiple instance regression (MIR) was introduced by Ray and Page (2001) as an analogue of multiple instance learning (MIL) in which we are given bags of feature-vectors (instances) and for each bag there is a bag-label which matches the label of one (unknown) primary instance from that bag. The goal is to compute a hypothesis regressor consistent with the underlying instance-labels. A natural approach  is to find the best primary instance assignment and regressor optimizing the mse loss on the bags though no formal generalization guarantees were known. Our work is the first to prove generalization error bounds for MIR when the bags are drawn i.i.d. at random. Essentially, with high probability any MIR regressor with low error on sampled bags also has low error on the underlying instance-label distribution.\n\nWe next study the complexity of linear regression on MIR bags, shown to be NP-hard in general by Ray and Page (2001), who however left open the possibility of arbitrarily good approximations. Significantly strengthening previous work, we prove a strong inapproximability bound: even if there exists zero bag-loss MIR linear regressor on a collection of $2$-sized bags with labels in $[-1,1]$, it is NP-hard to find an MIR linear regressor with bag-loss  $< C$ for some absolute constant $C > 0$.\n\nOur work also proposes a model training method for MIR based on a novel weighted assignment loss, geared towards handling overlapping bags which have not received much attention previously.  We conduct empirical evaluations on synthetic and real-world datasets showing that our method outperforms the baseline MIR methods.",
        "_bibtex": "@inproceedings{\nchauhan2024generalization,\ntitle={Generalization and Learnability in Multiple Instance Regression},\nauthor={Kushal Chauhan and Rishi Saket and Lorne Applebaum and Ashwinkumar Badanidiyuru and Chandan Giri and Aravindan Raghuveer},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=Pn6gcfciMy}\n}"
    },
    {
        "title": "Bounding causal effects with leaky instruments",
        "authorids": [
            "~David_Watson2",
            "~Gecia_Bravo-Hermsdorff1",
            "~Lee_M._Gunderson1",
            "~Jordan_Penn1",
            "~Afsaneh_Mastouri1",
            "~Ricardo_Silva1"
        ],
        "keywords": [
            "Causality; instrumental variables; partial identification; optimization; uncertainty"
        ],
        "abstract": "Instrumental variables (IVs) are a popular and powerful tool for estimating causal effects in the presence of unobserved confounding. However, classical approaches rely on strong assumptions such as the $\\textit{exclusion criterion}$, which states that instrumental effects must be entirely mediated by treatments. This assumption often fails in practice. When IV methods are improperly applied to data that do not meet the exclusion criterion, estimated causal effects may be badly biased. In this work, we propose a novel solution that provides $\\textit{partial}$ identification in linear systems given a set of $\\textit{leaky instruments}$, which are allowed to violate the exclusion criterion to some limited degree. We derive a convex optimization objective that provides provably sharp bounds on the average treatment effect under some common forms of information leakage, and implement inference procedures to quantify the uncertainty of resulting estimates. We demonstrate our method in a set of experiments with simulated data, where it performs favorably against the state of the art. An accompanying $\\texttt{R}$ package, $\\texttt{leakyIV}$, is available from $\\texttt{CRAN}$.",
        "_bibtex": "@inproceedings{\nwatson2024bounding,\ntitle={Bounding causal effects with leaky instruments},\nauthor={David Watson and Gecia Bravo-Hermsdorff and Lee M. Gunderson and Jordan Penn and Afsaneh Mastouri and Ricardo Silva},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=OaJLMx2nwS}\n}"
    },
    {
        "title": "Learning to Rank for Active Learning via Multi-Task Bilevel Optimization",
        "authorids": [
            "~Zixin_Ding1",
            "~Si_Chen5",
            "~Ruoxi_Jia1",
            "~Yuxin_Chen1"
        ],
        "keywords": [
            "Active Learning",
            "Multi-Task Learning",
            "Bilevel Optimization",
            "Utility Model"
        ],
        "abstract": "Active learning is a promising paradigm for reducing labeling costs by strategically requesting labels to improve model performance. However, existing active learning methods often rely on expensive acquisition functions, extensive model retraining, and multiple rounds of interaction with annotators. To address these limitations, we propose a novel approach for active learning, which aims to select batches of unlabeled instances through a learned surrogate model for data acquisition. A key challenge in this approach is to develop an acquisition function that generalizes well, as the history of data, which forms part of the utility function's input, grows over time. Our novel algorithmic contribution is a multi-task bilevel optimization framework that predicts the relative utility---measured by the validation accuracy---of different training sets, and ensures the learned acquisition function generalizes effectively. For cases where validation accuracy is expensive to evaluate, we introduce efficient interpolation-based surrogate models to estimate the utility function, reducing the evaluation cost. We demonstrate the performance of our approach through extensive experiments on standard active classification benchmarks.",
        "_bibtex": "@inproceedings{\nding2024learning,\ntitle={Learning to Rank for Active Learning via Multi-Task Bilevel Optimization},\nauthor={Zixin Ding and Si Chen and Ruoxi Jia and Yuxin Chen},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=o5Iw3kN9Eg}\n}"
    },
    {
        "title": "End-to-end Conditional Robust Optimization",
        "authorids": [
            "~Abhilash_Reddy_Chenreddy1",
            "~Erick_Delage2"
        ],
        "keywords": [
            "Machine Learning",
            "Deep Learning",
            "Conformal prediction",
            "Robust Optimization",
            "Conditional Robust Optimization",
            "Stochastic Optimization",
            "Task based learning",
            "end to end learning"
        ],
        "abstract": "The field of Contextual Optimization (CO) integrates machine learning and optimization to solve decision making problems under uncertainty. Recently, a risk sensitive variant of CO, known as Conditional Robust Optimization (CRO), combines uncertainty quantification with robust optimization in order to promote safety and reliability in high stake applications. Exploiting modern differentiable optimization methods, we propose a novel end-to-end approach to train a CRO model in a way that accounts for both the empirical risk of the prescribed decisions and the quality of conditional coverage of the contextual uncertainty set that supports them. While guarantees of success for the latter objective are impossible to obtain from the point of view of conformal prediction theory, high quality conditional coverage is achieved empirically by ingeniously employing a logistic regression differentiable layer within the calculation of coverage quality in our training loss.We show that the proposed training algorithms produce decisions that outperform the traditional estimate then optimize approaches.",
        "_bibtex": "@inproceedings{\nchenreddy2024endtoend,\ntitle={End-to-end Conditional Robust Optimization},\nauthor={Abhilash Reddy Chenreddy and Erick Delage},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=Oe9ngGi8Gh}\n}"
    },
    {
        "title": "Revisiting Kernel Attention with Correlated Gaussian Process Representation",
        "authorids": [
            "~Long_Minh_Bui1",
            "~Tho_Tran_Huu1",
            "~Duy_Dinh1",
            "~Tan_Minh_Nguyen1",
            "~Trong_Nghia_Hoang1"
        ],
        "keywords": [
            "Deep learning",
            "uncertainty calibration",
            "transformers"
        ],
        "abstract": "Transformers have increasingly become the de facto method to model sequential data with state-of-the-art performance. Due to its widespread use, being able to estimate and calibrate its modeling uncertainty is important to understand and design robust transformer models. To achieve this, previous works have used Gaussian processes (GPs) to perform uncertainty calibration for the attention units of transformers and attained notable successes. However, such approaches have to confine the transformers to the space of symmetric attention to ensure the necessary symmetric requirement of their GP's kernel specification, which reduces the representation capacity of the model. To mitigate this restriction, we propose the Correlated Gaussian Process Transformer (CGPT), a new class of transformers whose self-attention units are modeled as cross-covariance between two correlated GPs (CGPs). This allows asymmetries in attention and can enhance the representation capacity of GP-based transformers. We also derive a sparse approximation for CGP to make it scale better. Our empirical studies show that both CGP-based and sparse CGP-based transformers achieve better performance than state-of-the-art GP-based transformers on a variety of benchmark tasks.",
        "_bibtex": "@inproceedings{\nbui2024revisiting,\ntitle={Revisiting Kernel Attention with Correlated Gaussian Process Representation},\nauthor={Long Minh Bui and Tho Tran Huu and Duy Dinh and Tan Minh Nguyen and Trong Nghia Hoang},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=xlIK0vu3MW}\n}"
    },
    {
        "title": "Label-wise Aleatoric and Epistemic Uncertainty Quantification",
        "authorids": [
            "~Yusuf_Sale1",
            "~Paul_Hofman1",
            "~Timo_L\u00f6hr1",
            "~Lisa_Wimmer1",
            "~Thomas_Nagler1",
            "~Eyke_H\u00fcllermeier1"
        ],
        "keywords": [
            "uncertainty quantification",
            "label-wise uncertainty",
            "second-order distribution",
            "variance"
        ],
        "abstract": "We present a novel approach to uncertainty quantification in classification tasks based on label-wise decomposition of uncertainty measures. This label-wise perspective allows uncertainty to be quantified at the individual class level, thereby improving cost-sensitive decision-making and helping understand the sources of uncertainty. Furthermore, it allows to define total, aleatoric, and epistemic uncertainty on the basis of non-categorical measures such as variance, going beyond common entropy-based measures. In particular, variance-based measures address some of the limitations associated with established methods that have recently been discussed in the literature. We show that our proposed measures adhere to a number of desirable properties. Through empirical evaluation on a variety of benchmark data sets -- including applications in the medical domain where accurate uncertainty quantification is crucial -- we establish the effectiveness of label-wise uncertainty quantification.",
        "_bibtex": "@inproceedings{\nsale2024labelwise,\ntitle={Label-wise Aleatoric and Epistemic Uncertainty Quantification},\nauthor={Yusuf Sale and Paul Hofman and Timo L{\\\"o}hr and Lisa Wimmer and Thomas Nagler and Eyke H{\\\"u}llermeier},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=1Wg0J2WtEU}\n}"
    },
    {
        "title": "Quantization of Large Language Models with an Overdetermined Basis",
        "authorids": [
            "~Daniil_Merkulov1",
            "~Daria_Cherniuk1",
            "~Alexander_Rudikov1",
            "~Ivan_Oseledets1",
            "~Ekaterina_Muravleva1",
            "~Aleksandr_Mikhalev1",
            "~Boris_Kashin1"
        ],
        "keywords": [
            "Quantization",
            "Large models",
            "Kashin representation"
        ],
        "abstract": "In this paper, we introduce an algorithm for data quantization based on the principles of Kashin representation. This approach hinges on decomposing any given vector, matrix, or tensor into two factors. The first factor maintains a small infinity norm, while the second exhibits a similarly constrained norm when multiplied by an orthogonal matrix. Surprisingly, the entries of factors after decomposition are well-concentrated around several peaks, which allows us to efficiently replace them with corresponding centroids for quantization purposes. We study the theoretical properties of the proposed approach and rigorously evaluate our compression algorithm in the context of next-word prediction tasks, employing models like OPT of varying sizes. Our findings demonstrate that Kashin Quantization achieves competitive quality in model performance while ensuring superior data compression, marking a significant advancement in the field of data quantization.",
        "_bibtex": "@inproceedings{\nmerkulov2024quantization,\ntitle={Quantization of Large Language Models with an Overdetermined Basis},\nauthor={Daniil Merkulov and Daria Cherniuk and Alexander Rudikov and Ivan Oseledets and Ekaterina Muravleva and Aleksandr Mikhalev and Boris Kashin},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=40rikZ1pCm}\n}"
    },
    {
        "title": "Non-stationary Domain Generalization: Theory and Algorithm",
        "authorids": [
            "~Thai-Hoang_Pham1",
            "~Xueru_Zhang2",
            "~Ping_Zhang5"
        ],
        "keywords": [
            "non-stationary",
            "domain generalization",
            "invariant",
            "adaptive",
            "theory"
        ],
        "abstract": "Although recent advances in machine learning have shown its success to learn from independent and identically distributed (IID) data, it is vulnerable to out-of-distribution (OOD) data in an open world. Domain generalization (DG) deals with such an issue and it aims to learn a model from multiple source domains that can be generalized to unseen target domains. Existing studies on DG have largely focused on stationary settings with homogeneous source domains. However, in many applications,  domains may evolve along a specific direction (e.g., time, space). Without accounting for such non-stationary patterns, models trained with existing methods may fail to generalize on OOD data. In this paper, we study domain generalization in non-stationary environment. We first examine the impact of environmental non-stationarity on model performance and establish the theoretical upper bounds for the model error at target domains. Then, we propose a novel algorithm based on adaptive invariant representation learning, which leverages the non-stationary pattern to train a model that attains good performance on target domains. Experiments on both synthetic and real data validate the proposed algorithm.",
        "_bibtex": "@inproceedings{\npham2024nonstationary,\ntitle={Non-stationary Domain Generalization: Theory and Algorithm},\nauthor={Thai-Hoang Pham and Xueru Zhang and Ping Zhang},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=AMxdbjUvWg}\n}"
    },
    {
        "title": "Enhancing Patient Recruitment Response in Clinical Trials: an Adaptive Learning Framework",
        "authorids": [
            "~Xinying_Fang1",
            "~Shouhao_Zhou1"
        ],
        "keywords": [
            "Adaptive learning; Patient recruitment; Ensemble learning; Machine Learning; Clinical trial"
        ],
        "abstract": "Patient recruitment remains a key challenge in contemporary clinical trials, often leading to trial failures due to insufficient recruitment rates.  To address this issue, we introduce a novel adaptive learning framework that integrates machine learning methods to facilitate evidence-informed recruitment. Through dynamic testing, predictive learning, and adaptive pruning of recruitment plans, the proposed framework ensures superiority over the conventional random assignment approach. We discuss the practical considerations for implementing this framework and conduct a simulation study to assess the overall response rates and chances of improvement. The findings suggest that the proposed approach can substantially enhance patient recruitment efficiency. By systematically optimizing recruitment plan allocation, this adaptive learning framework shows promise in addressing recruitment challenges across broad clinical research settings, potentially transforming how patient recruitment is managed in clinical trials.",
        "_bibtex": "@inproceedings{\nfang2024enhancing,\ntitle={Enhancing Patient Recruitment Response in Clinical Trials: an Adaptive Learning Framework},\nauthor={Xinying Fang and Shouhao Zhou},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=qjY8a8jRgy}\n}"
    },
    {
        "title": "Products, Abstractions and Inclusions of Causal Spaces",
        "authorids": [
            "~Simon_Buchholz1",
            "~Junhyung_Park1",
            "~Bernhard_Sch\u00f6lkopf1"
        ],
        "keywords": [
            "causality",
            "causal abstraction",
            "causal spaces",
            "product spaces"
        ],
        "abstract": "Causal spaces have recently been introduced as a measure-theoretic framework to encode the notion of causality. While it has some advantages over established frameworks, such as structural causal models, the theory is so far only developed for single causal spaces. In many mathematical theories, not least the theory of probability spaces of which causal spaces are a direct extension, combinations of objects and maps between objects form a central part. In this paper, taking inspiration from such objects in probability theory, we propose the definitions of products of causal spaces, as well as (stochastic) transformations between causal spaces. In the context of causality, these quantities can be given direct semantic interpretations as causally independent components, abstractions and extensions.",
        "_bibtex": "@inproceedings{\nbuchholz2024products,\ntitle={Products, Abstractions and Inclusions of Causal Spaces},\nauthor={Simon Buchholz and Junhyung Park and Bernhard Sch{\\\"o}lkopf},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=QURqCXNyjM}\n}"
    },
    {
        "title": "Identifying Causal Changes Between Linear Structural Equation Models",
        "authorids": [
            "~Vineet_Malik1",
            "~Kevin_Bello1",
            "~Asish_Ghoshal2",
            "~Jean_Honorio1"
        ],
        "keywords": [
            "structural equation models",
            "distribution shifts",
            "root cause analysis",
            "intervention target estimation",
            "difference of precision matrices"
        ],
        "abstract": "Learning the structures of structural equation models (SEMs) as directed acyclic graphs (DAGs) from data is crucial for representing causal relationships in various scientific domains. Instead of estimating individual DAG structures, it is often preferable to directly estimate changes in causal relations between conditions, such as changes in genetic expression between healthy and diseased subjects.\nThis work studies the problem of directly estimating the difference between two linear SEMs, i.e.  *without estimating the individual DAG structures*, given two sets of samples drawn from the individual SEMs. We consider general classes of linear SEMs where the noise distributions are allowed to be Gaussian or non-Gaussian and have different noise variances across the variables in the individual SEMs. We rigorously characterize novel conditions related to the topological layering of the structural difference that lead to the *identifiability* of the difference DAG (DDAG). Moreover, we propose an *efficient* algorithm to identify the DDAG via sequential re-estimation of the difference of precision matrices. A surprising implication of our results is that causal changes can be identifiable even between *non-identifiable* models such as Gaussian SEMs with unequal noise variances. Synthetic experiments are presented to validate our theoretical results and to show the scalability of our method.",
        "_bibtex": "@inproceedings{\nmalik2024identifying,\ntitle={Identifying Causal Changes Between Linear Structural Equation Models},\nauthor={Vineet Malik and Kevin Bello and Asish Ghoshal and Jean Honorio},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=y31rgYaLaS}\n}"
    },
    {
        "title": "General Markov Model for Solving Patrolling Games",
        "authorids": [
            "~Andrzej_Nag\u00f3rko1",
            "~Micha\u0142_Tomasz_Godziszewski1",
            "~Marcin_Waniek1",
            "~Barbara_Rosiak1",
            "~Ma\u0142gorzata_R\u00f3g1",
            "~Tomasz_Pawe\u0142_Michalak1"
        ],
        "keywords": [
            "patrolling games",
            "markov chain",
            "stochastic patrolling",
            "game theory"
        ],
        "abstract": "Safeguarding critical infrastructure has recently emerged as a global challenge. To address complex security concerns raised by broadening array of threats, effective mobile security forces are essential. A key aspect involves designing optimal patrolling strategies for mobile units. Two bodies of research dealt with this: stochastic patrolling and partially observable stochastic games. Alas, the first approach makes too-far-reaching simplifying assumption and the second one is more expressive but computationally challenging.\nThe model proposed in this paper is inspired by partially observable stochastic games so that it is general enough to enable comprehensive modeling of attacker-defender interactions but a the same time remains computationally friendly.\nWith our proposed robust SHIELD algorithm, we are able to find a defense strategy where the probability of apprehending the attacker can be nearly doubled compared to the state of the art.",
        "_bibtex": "@inproceedings{\nnag{\\'o}rko2024general,\ntitle={General Markov Model for Solving Patrolling Games},\nauthor={Andrzej Nag{\\'o}rko and Micha{\\l} Tomasz Godziszewski and Marcin Waniek and Barbara Rosiak and Ma{\\l}gorzata R{\\'o}g and Tomasz Pawe{\\l} Michalak},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=GIqMAWbu0P}\n}"
    },
    {
        "title": "Probabilities of Causation for Continuous and Vector Variables",
        "authorids": [
            "~Yuta_Kawakami1",
            "~manabu_kuroki1",
            "~Jin_Tian1"
        ],
        "keywords": [
            "Probabilities of causation",
            "Continuous variables",
            "Vector variables"
        ],
        "abstract": "*Probabilities of causation* (PoC) are valuable concepts for explainable artificial intelligence and practical decision-making. PoC are originally defined for scalar binary variables. In this paper, we extend the concept of PoC to continuous treatment and outcome variables, and further generalize PoC  to capture causal effects between multiple treatments and multiple outcomes. In addition, we consider PoC for a sub-population and PoC with multi-hypothetical terms to capture more sophisticated counterfactual information useful for decision-making. We provide a nonparametric identification theorem for each type of PoC we introduce. Finally, we illustrate the application of our results on a real-world dataset about education.",
        "_bibtex": "@inproceedings{\nkawakami2024probabilities,\ntitle={Probabilities of Causation for Continuous and Vector Variables},\nauthor={Yuta Kawakami and manabu kuroki and Jin Tian},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=em3fzm95So}\n}"
    },
    {
        "title": "FedAST: Federated Asynchronous Simultaneous Training",
        "authorids": [
            "~Baris_Askin1",
            "~Pranay_Sharma2",
            "~Carlee_Joe-Wong1",
            "~Gauri_Joshi1"
        ],
        "keywords": [
            "Asynchronous Federated Learning",
            "Simultaneous Training",
            "Federated Learning with Multiple Models",
            "Non-convex Optimization"
        ],
        "abstract": "Federated Learning (FL) enables edge devices or clients to collaboratively train machine learning (ML) models without sharing their private data. Much of the existing work in FL focuses on efficiently learning a model for a single task. In this paper, we study simultaneous training of multiple FL models using a common set of clients. The few existing simultaneous training methods employ synchronous aggregation of client updates, which can cause significant delays because large models and/or slow clients can bottleneck the aggregation. On the other hand, a naive asynchronous aggregation is adversely affected by stale client updates. We propose FedAST, a buffered asynchronous federated simultaneous training algorithm that overcomes bottlenecks from slow models and adaptively allocates client resources across heterogeneous tasks. We provide theoretical convergence guarantees for FedAST for smooth non-convex objective functions. Extensive experiments over multiple real-world datasets demonstrate that our proposed method outperforms existing simultaneous FL approaches, achieving up to 46.0% reduction in time to train multiple tasks to completion.",
        "_bibtex": "@inproceedings{\naskin2024fedast,\ntitle={Fed{AST}: Federated Asynchronous Simultaneous Training},\nauthor={Baris Askin and Pranay Sharma and Carlee Joe-Wong and Gauri Joshi},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=yfQEtBP988}\n}"
    },
    {
        "title": "A Homogenization Approach for Gradient-Dominated Stochastic Optimization",
        "authorids": [
            "~Jiyuan_Tan1",
            "~Chenyu_Xue1",
            "~Chuwen_Zhang1",
            "~Qi_Deng1",
            "~Dongdong_Ge2",
            "~Yinyu_Ye1"
        ],
        "keywords": [
            "second-order algorithm",
            "gradient dominance",
            "reinforcement learing"
        ],
        "abstract": "Gradient dominance property is a condition weaker than strong convexity, yet sufficiently ensures global convergence even in non-convex optimization. This property finds wide applications in machine learning, reinforcement learning (RL), and operations management. In this paper, we propose the stochastic homogeneous second-order descent method (SHSODM) for stochastic functions enjoying gradient dominance property based on a recently proposed homogenization approach. Theoretically, we provide its sample complexity analysis, and further present an enhanced result by incorporating variance reduction techniques. Our findings show that SHSODM matches the best-known sample complexity achieved by other second-order methods for gradient-dominated stochastic optimization but without cubic regularization. Empirically, since the homogenization approach only relies on solving extremal eigenvector problem at each iteration instead of Newton-type system, our methods gain the advantage of cheaper computational cost and robustness in ill-conditioned problems. Numerical experiments on several RL tasks demonstrate the better performance of SHSODM compared to other off-the-shelf methods.",
        "_bibtex": "@inproceedings{\ntan2024a,\ntitle={A Homogenization Approach for Gradient-Dominated Stochastic Optimization},\nauthor={Jiyuan Tan and Chenyu Xue and Chuwen Zhang and Qi Deng and Dongdong Ge and Yinyu Ye},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=cgsUdqLnyt}\n}"
    },
    {
        "title": "Optimizing Language Models for Human Preferences is a Causal Inference Problem",
        "authorids": [
            "~Victoria_Lin2",
            "~Eli_Ben-Michael1",
            "~Louis-Philippe_Morency1"
        ],
        "keywords": [
            "causal inference",
            "optimization",
            "large language models",
            "doubly robust"
        ],
        "abstract": "As large language models (LLMs) see greater use in academic and commercial settings, there is increasing interest in methods that allow language models to generate texts aligned with human preferences. In this paper, we present an initial exploration of language model optimization for human preferences from *direct outcome datasets*, where each sample consists of a text and an associated numerical outcome measuring the reader's response. We first propose that language model optimization should be viewed as a *causal problem* to ensure that the model correctly learns the relationship between the text and the outcome. We formalize this causal language optimization problem, and we develop a method\u2014*causal preference optimization* (CPO)\u2014that solves an unbiased surrogate objective for the problem. We further extend CPO with *doubly robust* CPO (DR-CPO), which reduces the variance of the surrogate objective while retaining provably strong guarantees on bias. Finally, we empirically demonstrate the effectiveness of (DR-)CPO in optimizing state-of-the-art LLMs for human preferences on direct outcome data, and we validate the robustness of DR-CPO under difficult confounding conditions.",
        "_bibtex": "@inproceedings{\nlin2024optimizing,\ntitle={Optimizing Language Models for Human Preferences is a Causal Inference Problem},\nauthor={Victoria Lin and Eli Ben-Michael and Louis-Philippe Morency},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=VmI8qE0UK6}\n}"
    },
    {
        "title": "Adaptive Softmax Trees for Many-Class Classification",
        "authorids": [
            "~Rasul_Kairgeldin1",
            "~Magzhan_Gabidolla1",
            "~Miguel_\u00c1._Carreira-Perpi\u00f1\u00e1n2"
        ],
        "keywords": [
            "decision trees",
            "softmax",
            "many-class classification"
        ],
        "abstract": "NLP tasks such as language models or document classification involve classification problems with thousands of classes. In these situations, it is difficult to get high predictive accuracy and the resulting model can be huge in number of parameters and inference time. A recent, successful approach is the softmax tree (ST): a decision tree having sparse hyperplane splits at the decision nodes (which make hard, not soft, decisions) and small softmax classifiers at the leaves. Inference here is very fast because only a small subset of class probabilities need to be computed, yet the model is quite accurate. However, a significant drawback is that it assumes a complete tree, whose size grows exponentially with depth. We propose a new algorithm to train a ST of arbitrary structure. The tree structure itself is learned optimally by interleaving steps that grow the structure with steps that optimize the parameters of the current structure. This makes it possible to learn STs that can grow much deeper but in an irregular way, adapting to the data distribution. The resulting STs improve considerably the predictive accuracy while reducing the model size and inference time even further, as demonstrated in datasets with thousands of classes. In addition, they are interpretable to some extent.",
        "_bibtex": "@inproceedings{\nkairgeldin2024adaptive,\ntitle={Adaptive Softmax Trees for Many-Class Classification},\nauthor={Rasul Kairgeldin and Magzhan Gabidolla and Miguel {\\'A}. Carreira-Perpi{\\~n}{\\'a}n},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=WK7rXij5VC}\n}"
    },
    {
        "title": "ILP-FORMER: Solving Integer Linear Programming with Sequence to Multi-Label Learning",
        "authorids": [
            "~Shufeng_Kong2",
            "~Caihua_Liu1",
            "~Carla_P_Gomes1"
        ],
        "keywords": [
            "Deep Learning",
            "Integer Linear Programming",
            "Learning to Optimize"
        ],
        "abstract": "Integer Linear Programming (ILP) is an essential class of combinatorial optimization problems (COPs). Its inherent NP-hardness has fostered considerable efforts towards the development of heuristic strategies. An emerging approach involves leveraging data-driven methods to automatically learn these heuristics. For example, using deep (reinforcement) learning to recurrently reoptimize an initial solution with Large Neighborhood Search (LNS) has demonstrated exceptional performance across numerous applications. A pivotal challenge within LNS lies in identifying an optimal subset of variables for reoptimization at each stage. Existing methods typically learn a policy to select a subset, either by maintaining a fixed cardinality or by decomposing the subset into independent binary decisions for each variable. However, such strategies overlook the modeling of LNS\u2019s sequential processes and fail to explore the correlations inherent in variable selection. To overcome these shortcomings, we introduce ILP-FORMER, an innovative model that reimagines policy learning as a sequence-to-multi-label classification (MLC) problem. Our approach uniquely integrates a  causal transformer encoder to capture the sequential nature of LNS. Additionally, we employ an MLC decoder with contrastive learning to exploit the correlations in variable selection. Our extensive experiments confirm that ILP-FORMER delivers state-of-the-art anytime performance on several ILP benchmarks. Furthermore, ILP-FORMER exhibits impressive generalization capabilities when dealing with larger problem instances.",
        "_bibtex": "@inproceedings{\nkong2024ilpformer,\ntitle={{ILP}-{FORMER}: Solving Integer Linear Programming with Sequence to Multi-Label Learning},\nauthor={Shufeng Kong and Caihua Liu and Carla P Gomes},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=3H90d2FnY0}\n}"
    },
    {
        "title": "Efficient Interactive Maximization of BP and Weakly Submodular Objectives",
        "authorids": [
            "~Adhyyan_Narang1",
            "~Omid_Sadeghi1",
            "~Lillian_J._Ratliff1",
            "~Maryam_Fazel1",
            "~Jeff_Bilmes1"
        ],
        "keywords": [
            "submodular",
            "supermodular",
            "interactive",
            "bandit",
            "UCB",
            "online learning",
            "submodularity ratio",
            "submodularity curvature"
        ],
        "abstract": "In the context of online interactive machine learning with combinatorial objectives, we extend purely submodular prior work to more general non-submodular objectives.  This includes: (1) those that are additively decomposable into a sum of two terms (a monotone submodular and monotone supermodular term, known as a BP decomposition); and (2) those that are only weakly submodular.  In both cases, this allows representing not only competitive (submodular) but also complementary (supermodular) relationships between objects, enhancing this setting to a broader range of applications (e.g., movie recommendations, medical treatments, etc.) where this is beneficial.  In the two-term case, moreover, we study not only the more typical monolithic feedback approach but also a novel framework where feedback is available separately for each term.  With real-world practicality and scalability in mind, we integrate \\Nystrom{} sketching techniques to significantly improve the computational complexity, including for the purely submodular case. In the Gaussian process contextual bandits setting, we show sub-linear theoretical regret bounds in all cases. We also empirically show good applicability to recommendation systems and data subset selection.",
        "_bibtex": "@inproceedings{\nnarang2024efficient,\ntitle={Efficient Interactive Maximization of {BP} and Weakly Submodular Objectives},\nauthor={Adhyyan Narang and Omid Sadeghi and Lillian J. Ratliff and Maryam Fazel and Jeff Bilmes},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=qYXL8ClMnP}\n}"
    },
    {
        "title": "On Overcoming Miscalibrated Conversational Priors in LLM-based ChatBots",
        "authorids": [
            "~Christine_Herlihy1",
            "~Jennifer_Neville1",
            "~Tobias_Schnabel1",
            "~Adith_Swaminathan1"
        ],
        "keywords": [
            "preference elicitation",
            "cost-aware",
            "under-specification",
            "large language model",
            "LLM copilot"
        ],
        "abstract": "We explore the use of Large Language Model (LLM-based) chatbots to power recommender systems. We observe that the chatbots respond poorly when they encounter under-specified requests  (e.g., they make incorrect assumptions, hedge with a long response, or refuse to answer). We conjecture that such miscalibrated response tendencies (i.e., conversational priors) can be attributed to LLM fine-tuning by annotators --- single-turn annotations may not capture multi-turn conversation utility, and the annotators' preferences may not even be representative of users interacting with a recommender system. \n\nWe first analyze public LLM chat logs to conclude that query under-specification is common. Next, we study synthetic recommendation problems with known but latent item utilities, and frame them as Partially Observed Decision Processes (PODP). We find that pre-trained LLMs can be sub-optimal for PODPs and derive better policies that clarify under-specified queries when appropriate. Then, we re-calibrate LLMs by prompting them with learned control messages to approximate the improved policy. Finally, we show empirically that our lightweight learning approach effectively uses logged conversation data to re-calibrate the response strategies of LLM-based chatbots for recommendation tasks.",
        "_bibtex": "@inproceedings{\nherlihy2024on,\ntitle={On Overcoming Miscalibrated Conversational Priors in {LLM}-based ChatBots},\nauthor={Christine Herlihy and Jennifer Neville and Tobias Schnabel and Adith Swaminathan},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=iXQIgIsmIr}\n}"
    },
    {
        "title": "Power Mean Estimation in Stochastic Monte-Carlo Tree Search",
        "authorids": [
            "~Tuan_Quang_Dam1",
            "~Odalric-Ambrym_Maillard3",
            "~Emilie_Kaufmann1"
        ],
        "keywords": [
            "Monte-Carlo Tree Search",
            "Monte-Carlo Planning"
        ],
        "abstract": "Monte-Carlo Tree Search (MCTS) is a widely-used online planning strategy that effectively combines Monte-Carlo sampling with forward tree search to make optimal decisions in various real-world scenarios. Its success hinges on the application of the Upper Confidence bound for Trees (UCT) algorithm, an extension of the Upper Confidence bound (UCB) method used in multi-arm bandits. However, theoretical investigations of UCT have been found incomplete due to an error in the estimated \"logarithmic\" bonus term used for action selection in the tree. This issue was addressed by introducing a \"polynomial\" exploration bonus, which effectively balances the exploration-exploitation trade-off. However, most theoretical studies have focused on deterministic Markov Decision Processes (MDPs), neglecting stochastic settings. In this paper, we introduce Stochastic-Power-UCT, an MCTS algorithm explicitly designed for stochastic MDPs using power mean as the value estimator. We conduct a comprehensive study of the polynomial convergence assurance for selecting the optimal action and estimating the value at the root node within our methodology. Our findings demonstrate that Stochastic-Power-UCT shares the same convergence rate as UCT, with UCT being a special case of Stochastic-Power-UCT. Furthermore, we validate our theoretical results through empirical assessments across diverse stochastic MDP environments, providing empirical evidence to support our method's theoretical claims",
        "_bibtex": "@inproceedings{\ndam2024power,\ntitle={Power Mean Estimation in Stochastic Monte-Carlo Tree Search},\nauthor={Tuan Quang Dam and Odalric-Ambrym Maillard and Emilie Kaufmann},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=NE2S89Eb9e}\n}"
    },
    {
        "title": "Optimization Framework for Semi-supervised Attributed Graph Coarsening",
        "authorids": [
            "~Manoj_Kumar4",
            "~Subhanu_Halder1",
            "~Archit_Kane1",
            "~Ruchir_Gupta1",
            "~Sandeep_Kumar8"
        ],
        "keywords": [
            "Graph coarsening",
            "node classification",
            "graph neural network"
        ],
        "abstract": "In data-intensive applications, graphs serve as foundational structures across various domains. However, the increasing size of datasets poses significant challenges to performing downstream tasks. To address this problem, techniques such as graph coarsening, condensation, and summarization have been developed to create a coarsened graph while preserving important properties of the original graph by considering both the graph matrix and the feature or attribute matrix of the original graph as inputs. However, existing graph coarsening techniques often neglect the label information during the coarsening process, which can result in a lower-quality coarsened graph and limit its suitability for downstream tasks.  To overcome this limitation, we introduce the Label-Aware Graph Coarsening (LAGC) algorithm, a semi-supervised approach that incorporates the graph matrix, feature matrix, and some of the node label information to learn a coarsened graph. Our proposed formulation is a non-convex optimization problem that is efficiently solved using  block successive upper bound minimization(BSUM) technique, and it is provably convergent. Our extensive results demonstrate that the LAGC algorithm outperforms the existing state-of-the-art method by a significant margin.",
        "_bibtex": "@inproceedings{\nkumar2024optimization,\ntitle={Optimization Framework for Semi-supervised Attributed Graph Coarsening},\nauthor={Manoj Kumar and Subhanu Halder and Archit Kane and Ruchir Gupta and Sandeep Kumar},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=ehcPPdsFx4}\n}"
    },
    {
        "title": "On Convergence of Federated Averaging Langevin Dynamics",
        "authorids": [
            "~Wei_Deng1",
            "~Qian_Zhang10",
            "~Yian_Ma1",
            "~Zhao_Song3",
            "~Guang_Lin1"
        ],
        "keywords": [
            "Federated Learning",
            "Langevin Dynamics",
            "Bayesian Inference",
            "Convex Analysis"
        ],
        "abstract": "We propose a federated averaging Langevin algorithm (FA-LD) for uncertainty quantification and mean predictions  with distributed clients. In particular, we generalize beyond normal posterior distributions and consider a general class of models. We develop theoretical guarantees for FA-LD for strongly log-concave distributions with non-i.i.d data and study how the injected noise and the stochastic-gradient noise, the heterogeneity of data, and the varying learning rates affect the convergence. Such an analysis sheds light on the optimal choice of local updates to minimize the communication cost. Important to our approach is that the communication efficiency does not deteriorate with the injected noise in the Langevin algorithms. In addition, we examine in our FA-LD algorithm both independent and correlated noise used over different clients. We observe that there is a trade-off between the pairs among communication, accuracy, and data privacy. As local devices may become inactive in federated networks, we also show convergence results based on different averaging schemes where only partial device updates are available. In such a case, we discover an additional bias that does not decay to zero.",
        "_bibtex": "@inproceedings{\ndeng2024on,\ntitle={On Convergence of Federated Averaging Langevin Dynamics},\nauthor={Wei Deng and Qian Zhang and Yian Ma and Zhao Song and Guang Lin},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=EmQGdBsOPx}\n}"
    },
    {
        "title": "Masking the Unknown: Leveraging Masked Samples for Enhanced Data Augmentation",
        "authorids": [
            "~Xun_Yao1",
            "~Zijian_Huang4",
            "~Xinrong_Hu2",
            "~JACK_Yang1",
            "~Yi_Guo3"
        ],
        "keywords": [
            "Data augmentation",
            "Masked Language Model",
            "Text classification",
            "Data variance"
        ],
        "abstract": "Data Augmentation (DA) has become a widely adopted strategy for addressing data scarcity in numerous NLP tasks, especially in scenarios with limited resources or imbalanced classes. However, many existing augmentation techniques rely on randomness or additional resources, presenting challenges in both performance and practical implementation. Furthermore, there is a lack of exploration into what constitutes effective augmentation. \n\nIn this paper, we systematically evaluate existing DA methods across a comprehensive range of text-classification benchmarks. The empirical analysis highlights that the most significant change resulting from augmentation is observed in the data variance. This observation inspires the proposed approach, termed Mask-for-Data Augmentation (M4DA), which strategically masks tokens from original samples for augmentation. Specifically, M4DA consists of a Variance-Oriented Masker Module (VMM), which ensures an increase in data variances, and a Complexity-Enhanced Selection Module (CSM), designed to select the augmented sample with the highest semantic complexity. \n\nThe effectiveness of the proposed method is empirically validated across various text-classification benchmarks, including scenarios with limited or full resources and imbalanced classes. Experimental results demonstrate considerable improvements over state-of-the-arts.",
        "_bibtex": "@inproceedings{\nyao2024masking,\ntitle={Masking the Unknown: Leveraging Masked Samples for Enhanced Data Augmentation},\nauthor={Xun Yao and Zijian Huang and Xinrong Hu and JACK Yang and Yi Guo},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=Rfm4Kzw7DP}\n}"
    },
    {
        "title": "Active Learning Framework for Incomplete Networks",
        "authorids": [
            "~Tung_Khong1",
            "~Cong_Tran1",
            "~Cuong_Pham1"
        ],
        "keywords": [
            "Active Learning",
            "Incomplete Graphs",
            "Graph Neural Networks",
            "Node Classification",
            "Link Prediction",
            "Semi-Supervised Learning"
        ],
        "abstract": "Significant progression has been made in active learning algorithms for graph networks in various tasks. However real-world applications frequently involve incomplete graphs with missing links, which pose the challenge that existing approaches might not adequately address. This paper presents an active learning approach tailored specifically for handling incomplete graphs, termed ALIN. Our algorithm employs graph neural networks (GNN) to generate node embeddings and calculates losses for both node classification and link prediction tasks. The losses are combined with appropriate weights and iteratively updating the GNN, ALIN efficiently queries nodes in batches, thereby achieving a balance between training feedbacks and resource utilization. Our empirical experiments have shown ALIN can surpass state-of-the-art baselines on Cora, Citeseer, Pubmed, and Coauthor-CS datasets.",
        "_bibtex": "@inproceedings{\nkhong2024active,\ntitle={Active Learning Framework for Incomplete Networks},\nauthor={Tung Khong and Cong Tran and Cuong Pham},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=n6wh6WiWvV}\n}"
    },
    {
        "title": "Decision-Focused Evaluation of Worst-Case Distribution Shift",
        "authorids": [
            "~Kevin_Ren1",
            "~Yewon_Byun1",
            "~Bryan_Wilder2"
        ],
        "keywords": [
            "model evaluation",
            "reliability",
            "distribution shift",
            "resource allocation"
        ],
        "abstract": "Recent studies have shown that performance on downstream optimization tasks often diverges from standard accuracy-based losses, highlighting that the loss function of a predictive model should align with the decision task of the downstream optimizer. Despite this observation, no work\u2014 to our knowledge\u2014has yet examined the impact of this divergence for distribution shift. In this paper, we demonstrate that worst-case distribution shifts identified by traditional average accuracy-based metrics fundamentally differ from those for the downstream decision task at hand. We introduce a novel framework that employs a hierarchical model structure to identify worst-case distribution shifts in predictive resource allocation settings. This task is more difficult than in standard distribution shift settings because of combinatorial interactions, where decisions depend on the joint presence of individuals in the allocation task. We show that the problem can be reformulated as a submodular optimization problem, enabling efficient approximations, to capture shifts both within and across instances of the optimization problem.",
        "_bibtex": "@inproceedings{\nren2024decisionfocused,\ntitle={Decision-Focused Evaluation of Worst-Case Distribution Shift},\nauthor={Kevin Ren and Yewon Byun and Bryan Wilder},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=I2wonyR7Tm}\n}"
    },
    {
        "title": "Privacy-Aware Randomized Quantization via Linear Programming",
        "authorids": [
            "~Zhongteng_Cai1",
            "~Xueru_Zhang2",
            "~Mohammad_Mahdi_Khalili3"
        ],
        "keywords": [
            "Differential privacy",
            "Quantization",
            "Optimization"
        ],
        "abstract": "Differential privacy mechanisms such as the Gaussian or Laplace mechanism have been widely used in data analytics for preserving individual privacy. However, they are mostly designed for continuous outputs and are unsuitable for scenarios where discrete values are necessary. Although various quantization mechanisms were proposed recently to generate discrete outputs under differential privacy, the outcomes are either biased or have an inferior accuracy-privacy trade-off. In this paper, we propose a family of quantization mechanisms that is unbiased and differentially private. It has a high degree of freedom and we show that some existing mechanisms can be considered as special cases of ours. To find the optimal mechanism, we formulate a linear optimization that can be solved efficiently using linear programming tools. Experiments show that our proposed mechanism can attain a better privacy-accuracy trade-off compared to baselines.",
        "_bibtex": "@inproceedings{\ncai2024privacyaware,\ntitle={Privacy-Aware Randomized Quantization via Linear Programming},\nauthor={Zhongteng Cai and Xueru Zhang and Mohammad Mahdi Khalili},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=vWsf4L7rHq}\n}"
    },
    {
        "title": "Response Time Improves Gaussian Process Models for Perception and Preferences",
        "authorids": [
            "~Michael_Shvartsman1",
            "~Benjamin_Letham1",
            "~Eytan_Bakshy1",
            "~Stephen_L_Keeley1"
        ],
        "keywords": [
            "Gaussian process",
            "preference learning",
            "diffusion decision model"
        ],
        "abstract": "Models for human choice prediction in preference learning and perception science often use binary response data, requiring many samples to accurately learn latent utilities or perceptual intensities. The response time (RT) to make each choice captures additional information about the decision process, but existing models incorporating RTs for choice prediction do so in a fully parametric way or over discrete inputs. At the same time, state-of-the-art Gaussian process (GP) models of perception and preferences operate on choices only, ignoring RTs. We propose two approaches for incorporating RTs into GP preference and perception models. The first is based on stacking GP models, and the second uses a novel differentiable approximation to the likelihood of the diffusion decision model (DDM), the de-facto standard model for choice RTs. Our RT-choice GPs enable better latent value estimation and held-out choice prediction relative to baselines, which we demonstrate on three real-world multivariate datasets covering both human psychophysics and preference learning.",
        "_bibtex": "@inproceedings{\nshvartsman2024response,\ntitle={Response Time Improves Gaussian Process Models for Perception and Preferences},\nauthor={Michael Shvartsman and Benjamin Letham and Eytan Bakshy and Stephen L Keeley},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=oUZ5JweNRc}\n}"
    },
    {
        "title": "\u03b1-Former: Local-Feature-Aware (L-FA) Transformer",
        "authorids": [
            "~Zhi_Xu2",
            "~Bin_Sun1",
            "~Yue_Bai1",
            "~Yun_Fu1"
        ],
        "keywords": [
            "Camouflaged instance segmentation"
        ],
        "abstract": "Despite the success of current segmentation models powered by the transformer, the camouflaged instance segmentation (CIS) task remains a challenge due to the similarity between the target and the background. To address this issue, we propose a novel approach called the local-feature-aware transformer ($\\alpha$-Former), inspired by how humans find the camouflaged instance in a given photograph. We use traditional computer vision descriptors to simulate how humans find the unnatural boundary in a given photograph. Then, the information extracted by traditional descriptors can be employed as prior knowledge to enhance the neural network's performance. Moreover, due to the non-learnable characteristics of traditional descriptors, we designed a learnable binary filter to simulate the traditional descriptors. In order to aggregate the information from the backbone and binary filter, we introduce an adapter to merge local features into the transformer framework. Additionally, we introduce an edge-aware feature fusion module to improve boundary results in the segmentation model. Using the proposed transformer-based encoder-decoder architecture, our $\\alpha$-Former surpasses state-of-the-art performance on the COD10K and NC4K datasets.",
        "_bibtex": "@inproceedings{\nxu2024former,\ntitle={\\ensuremath{\\alpha}-Former: Local-Feature-Aware (L-{FA}) Transformer},\nauthor={Zhi Xu and Bin Sun and Yue Bai and Yun Fu},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=FPUL7VYbca}\n}"
    },
    {
        "title": "End-to-End Learning for Fair Multiobjective Optimization Under Uncertainty",
        "authorids": [
            "~My_H_Dinh1",
            "~James_Kotary1",
            "~Ferdinando_Fioretto1"
        ],
        "keywords": [
            "Predict-then-Optimize",
            "Multi-objective optimization",
            "fairness"
        ],
        "abstract": "Many decision processes in artificial intelligence and operations research are modeled by parametric optimization problems whose defining parameters are unknown and must be inferred from observable data. The Predict-Then-Optimize (PtO) paradigm in machine learning aims to maximize downstream decision quality by training the parametric inference model end-to-end with the subsequent constrained optimization. This requires backpropagation through the optimization problem using approximation techniques specific to the problem's form, especially for nondifferentiable linear and mixed-integer programs. This paper extends the PtO methodology to optimization problems with nondifferentiable Ordered Weighted Averaging (OWA) objectives, known for their ability to ensure properties of fairness and robustness in decision models. Through a collection of training techniques and proposed application settings, it shows how optimization of OWA functions can be effectively integrated with parametric prediction for fair and robust optimization under uncertainty.",
        "_bibtex": "@inproceedings{\ndinh2024endtoend,\ntitle={End-to-End Learning for Fair Multiobjective Optimization Under Uncertainty},\nauthor={My H Dinh and James Kotary and Ferdinando Fioretto},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=ptbCobFKtX}\n}"
    },
    {
        "title": "Local Discovery by Partitioning: Polynomial-Time Causal Discovery Around Exposure-Outcome Pairs",
        "authorids": [
            "~Jacqueline_R._M._A._Maasch1",
            "~Weishen_Pan1",
            "~Shantanu_Gupta2",
            "~Volodymyr_Kuleshov1",
            "~Kyra_Gan1",
            "~Fei_Wang3"
        ],
        "keywords": [
            "causal discovery",
            "graphical models",
            "covariate adjustment"
        ],
        "abstract": "Causal discovery is crucial for causal inference in observational studies, as it can enable the identification of *valid adjustment sets* (VAS) for unbiased effect estimation. However,  global causal discovery is notoriously hard in the nonparametric setting, with exponential time and sample complexity in the worst case. To address this, we propose *local discovery by partitioning* (LDP): a local causal discovery method that is tailored for downstream inference tasks without requiring parametric and pretreatment assumptions. LDP is a constraint-based procedure that returns a VAS for an exposure-outcome pair under latent confounding, given sufficient conditions. The total number of independence tests performed is worst-case quadratic with respect to the cardinality of the variable set. Asymptotic theoretical guarantees are numerically validated on synthetic graphs. Adjustment sets from LDP yield less biased and more precise average treatment effect estimates than baseline discovery algorithms, with LDP outperforming on confounder recall, runtime, and test count for VAS discovery. Notably, LDP ran at least $1300\\times$ faster than baselines on a benchmark.",
        "_bibtex": "@inproceedings{\nmaasch2024local,\ntitle={Local Discovery by Partitioning: Polynomial-Time Causal Discovery Around Exposure-Outcome Pairs},\nauthor={Jacqueline R. M. A. Maasch and Weishen Pan and Shantanu Gupta and Volodymyr Kuleshov and Kyra Gan and Fei Wang},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=g8TAHxJ3bh}\n}"
    },
    {
        "title": "One Shot Inverse Reinforcement Learning for Stochastic Linear Bandits",
        "authorids": [
            "~Etash_Kumar_Guha1",
            "~Jim_Thannikary_James1",
            "~Krishna_Acharya1",
            "~Vidya_Muthukumar3",
            "~Ashwin_Pananjady1"
        ],
        "keywords": [
            "Inverse Reinforcement Learning",
            "Linear Bandits"
        ],
        "abstract": "The paradigm of inverse reinforcement learning (IRL) is used to specify the reward function of an agent purely from its actions and is critical for value alignment and AI safety. While IRL is successful in practice, theoretical guarantees remain nascent. Motivated by the need for IRL in large action spaces with limited data, we consider as a first step the problem of learning from a single sequence of actions (i.e., a demonstration) of a stochastic linear bandit algorithm. When the demonstrator employs the Phased Elimination algorithm, we develop a simple inverse learning procedure that estimates the linear reward function consistently in the time horizon with just a \\textit{single} demonstration. In particular, we show that our inverse learner approximates the true reward parameter within a error of $\\mathcal{O}(T^{-\\frac{\\omega - 1}{2\\omega }})$ (where $T$ is the length of the demonstrator's trajectory and $\\omega$ is a constant that depends on the geometry of the action set). We complement this result with an information-theoretic lower bound for any inverse learning procedure. We corroborate our theoretical results with simulations on synthetic data and a demonstration constructed from the MovieLens dataset.",
        "_bibtex": "@inproceedings{\nguha2024one,\ntitle={One Shot Inverse Reinforcement Learning for Stochastic Linear Bandits},\nauthor={Etash Kumar Guha and Jim Thannikary James and Krishna Acharya and Vidya Muthukumar and Ashwin Pananjady},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=kQQ20pTbxI}\n}"
    },
    {
        "title": "CSS: Contrastive Semantic Similarities for Uncertainty Quantification of LLMs",
        "authorids": [
            "~Shuang_Ao4",
            "~Stefan_Rueger1",
            "~Advaith_Siddharthan1"
        ],
        "keywords": [
            "uncertainty quantification",
            "LLMs",
            "CLIP",
            "semantic similarity",
            "spectral cluster"
        ],
        "abstract": "Despite the impressive capability of large language models (LLMs), knowing when to trust their generations remains an open challenge. The recent literature on uncertainty quantification of natural language generation (NLG) utilizes a conventional natural language inference (NLI) classifier to measure the semantic dispersion of LLMs responses. These studies employ logits of NLI classifier for semantic clustering to estimate uncertainty. However, logits represent the probability of the predicted class and barely contain feature information for potential clustering. Alternatively, CLIP (Contrastive Language\u2013Image Pre-training) performs impressively in extracting image-text pair features and measuring their similarity. To extend its usability, we propose Contrastive Semantic Similarity, the CLIP-based feature extraction module to obtain similarity features for measuring uncertainty for text pairs. We apply this method to selective NLG, which detects and rejects unreliable generations for better trustworthiness of LLMs. We conduct extensive experiments with three LLMs on several benchmark question-answering datasets with comprehensive evaluation metrics. Results show that our proposed method performs better in estimating reliable responses of LLMs than comparable baselines.",
        "_bibtex": "@inproceedings{\nao2024css,\ntitle={{CSS}: Contrastive Semantic Similarities for Uncertainty Quantification of {LLM}s},\nauthor={Shuang Ao and Stefan Rueger and Advaith Siddharthan},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=k0Fy2SPwtc}\n}"
    },
    {
        "title": "Learning Causal Abstractions of Linear Structural Causal Models",
        "authorids": [
            "~Riccardo_Massidda1",
            "~Sara_Magliacane1",
            "~Davide_Bacciu1"
        ],
        "keywords": [
            "Causal Abstraction",
            "Structural Causal Models",
            "Causal Discovery"
        ],
        "abstract": "The need for modelling causal knowledge at different levels of granularity arises in several settings. Causal Abstraction provides a framework for formalizing this problem by relating two Structural Causal Models at different levels of detail. Despite increasing interest in applying causal abstraction, e.g. in the interpretability of large machine learning models, the graphical and parametrical conditions under which a causal model can abstract another are not known. Furthermore, learning causal abstractions from data is still an open problem. In this work, we tackle both issues for linear causal models with linear abstraction functions. First, we characterize how the low-level coefficients and the abstraction function determine the high-level coefficients and how the high-level model constrains the causal ordering of low-level variables. Then, we apply our theoretical results to learn high-level and low-level causal models and their abstraction function from observational data. In particular, we introduce Abs-LiNGAM, a method that leverages the constraints induced by the learned high-level model and the abstraction function to speedup the recovery of the larger low-level model, under the assumption of non-Gaussian noise terms. In simulated settings, we show the effectiveness of learning causal abstractions from data and the potential of our method in improving scalability of causal discovery.",
        "_bibtex": "@inproceedings{\nmassidda2024learning,\ntitle={Learning Causal Abstractions of Linear Structural Causal Models},\nauthor={Riccardo Massidda and Sara Magliacane and Davide Bacciu},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=XlFqI9TMhf}\n}"
    },
    {
        "title": "Iterated INLA for State and Parameter Estimation in Nonlinear Dynamical Systems",
        "authorids": [
            "~Rafael_Anderka1",
            "~Marc_Peter_Deisenroth1",
            "~So_Takao1"
        ],
        "keywords": [
            "Data assimilation",
            "Integrated Nested Laplace Approximation",
            "Gaussian process",
            "Stochastic PDEs"
        ],
        "abstract": "Data assimilation (DA) methods use priors arising from differential equations to robustly interpolate and extrapolate data. Popular techniques such as ensemble methods that handle high-dimensional, nonlinear PDE priors focus mostly on state estimation, however can have difficulty learning the parameters accurately. On the other hand, machine learning based approaches can naturally learn the state and parameters, but their applicability can be limited, or produce uncertainties that are hard to interpret. Inspired by the Integrated Nested Laplace Approximation (INLA) method in spatial statistics, we propose an alternative approach to DA based on iteratively linearising the dynamical model. This produces a Gaussian Markov random field at each iteration, enabling one to use INLA to infer the state and parameters. Our approach can be used for arbitrary nonlinear systems, while retaining interpretability, and is furthermore demonstrated to outperform existing methods on the DA task. By providing a more nuanced approach to handling nonlinear PDE priors, our methodology offers improved accuracy and robustness in predictions, especially where data sparsity is prevalent.",
        "_bibtex": "@inproceedings{\nanderka2024iterated,\ntitle={Iterated {INLA} for State and Parameter Estimation in Nonlinear Dynamical Systems},\nauthor={Rafael Anderka and Marc Peter Deisenroth and So Takao},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=dQUdw0lEfA}\n}"
    },
    {
        "title": "Towards Representation Learning for Weighting Problems in Design-Based Causal Inference",
        "authorids": [
            "~Oscar_Clivio1",
            "~Avi_Feller1",
            "~Christopher_C._Holmes1"
        ],
        "keywords": [
            "representation",
            "weighting",
            "treatment effect",
            "causal inference",
            "transportability"
        ],
        "abstract": "Reweighting a distribution to minimize a distance to a target distribution is a powerful and flexible strategy for estimating a wide range of causal effects, but can be challenging in practice because optimal weights typically depend on knowledge of the underlying data generating process. In this paper, we focus on design-based weights, which do not incorporate outcome information; prominent examples include prospective cohort studies, survey weighting, and the weighting portion of augmented weighting estimators. In such applications, we explore the central role of representation learning in finding desirable weights in practice. Unlike the common approach of assuming a well-specified representation, we highlight the error due to the choice of a representation and outline a general framework for finding suitable representations that minimize this error. Building on recent work that combines balancing weights and neural networks, we propose an end-to-end estimation procedure that learns a flexible representation, while retaining promising theoretical properties. We show that this approach is competitive in a range of common causal inference tasks.",
        "_bibtex": "@inproceedings{\nclivio2024towards,\ntitle={Towards Representation Learning for Weighting Problems in Design-Based Causal Inference},\nauthor={Oscar Clivio and Avi Feller and Christopher C. Holmes},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=KNgBCZXJkY}\n}"
    },
    {
        "title": "Knowledge Intensive Learning of Credal Networks",
        "authorids": [
            "~Saurabh_Mathur1",
            "~Alessandro_Antonucci1",
            "~Sriraam_Natarajan1"
        ],
        "keywords": [
            "Probabilistic Graphical Models",
            "Bayesian Networks",
            "Credal Networks",
            "Knowledge-guided learning"
        ],
        "abstract": "Bayesian networks are a popular class of directed probabilistic graphical models that allow for closed-form learning of the local parameters if complete data are available. However, learning the parameters is challenging when the data are sparse, incomplete, and uncertain. In this work, we present an approach to this problem based on credal networks, a generalization of Bayesian networks based on set-valued local parameters. We derive an algorithm to learn such set-valued parameters from data using qualitative knowledge in the form of monotonic influence statements. Our empirical evaluation shows that using qualitative knowledge reduces uncertainty about the parameters without significant loss in accuracy.",
        "_bibtex": "@inproceedings{\nmathur2024knowledge,\ntitle={Knowledge Intensive Learning of Credal Networks},\nauthor={Saurabh Mathur and Alessandro Antonucci and Sriraam Natarajan},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=wpJD6hQMuU}\n}"
    },
    {
        "title": "Gradient descent in matrix factorization: Understanding large initialization",
        "authorids": [
            "~Hengchao_Chen1",
            "~Xin_Chen29",
            "~Mohamad_Elmasri1",
            "~Qiang_Sun2"
        ],
        "keywords": [
            "Gradient descent",
            "matrix factorization",
            "large initialization",
            "trajectory analysis",
            "incremental learning"
        ],
        "abstract": "Gradient Descent (GD) has been proven effective in solving various matrix factorization problems. However, its optimization behavior with large initial values remains less understood. To address this gap, this paper presents a novel theoretical framework for examining the convergence trajectory of GD with a large initialization. The framework is grounded in signal-to-noise ratio concepts and inductive arguments. The results uncover an implicit incremental learning phenomenon in GD and offer a deeper understanding of its performance in large initialization scenarios.",
        "_bibtex": "@inproceedings{\nchen2024gradient,\ntitle={Gradient descent in matrix factorization: Understanding large initialization},\nauthor={Hengchao Chen and Xin Chen and Mohamad Elmasri and Qiang Sun},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=zHplexWZSr}\n}"
    },
    {
        "title": "Multi-Relational Structural Entropy",
        "authorids": [
            "~Yuwei_Cao1",
            "~Hao_Peng7",
            "~Angsheng_Li1",
            "~Chenyu_You1",
            "~Zhifeng_Hao4",
            "~Philip_S._Yu1"
        ],
        "keywords": [
            "Multi-relational Structural Entropy",
            "graph entropy",
            "multi-relational graphs",
            "node clustering"
        ],
        "abstract": "Structural Entropy (SE) measures the structural information contained in a graph. Minimizing or maximizing SE helps to reveal or obscure the intrinsic structural patterns underlying graphs in an interpretable manner, finding applications in various tasks driven by networked data. However, SE ignores the heterogeneity inherent in the graph relations, which is ubiquitous in modern networks. In this work, we extend SE to consider heterogeneous relations and propose the first metric for multi-relational graph structural information, namely, Multi-relational Structural Entropy (MrSE). To this end, we first cast SE through the novel lens of the stationary distribution from random surfing, which readily extends to multi-relational networks by considering the choices of both nodes and relation types simultaneously at each step. The resulting MrSE is then optimized by a new greedy algorithm to reveal the essential structures within a multi-relational network. Experimental results highlight that the proposed MrSE offers a more insightful interpretation of the structure of multi-relational graphs compared to SE. Additionally, it enhances the performance of two tasks that involve real-world multi-relational graphs, including node clustering and social event detection.",
        "_bibtex": "@inproceedings{\ncao2024multirelational,\ntitle={Multi-Relational Structural Entropy},\nauthor={Yuwei Cao and Hao Peng and Angsheng Li and Chenyu You and Zhifeng Hao and Philip S. Yu},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=9sAha7AoNA}\n}"
    },
    {
        "title": "Unsupervised Feature Selection towards Pattern Discrimination Power",
        "authorids": [
            "~Wangduk_Seo1",
            "~Jaesung_Lee2"
        ],
        "keywords": [
            "Unsupervised Feature Selection",
            "Information Entropy",
            "Entropy Approximation",
            "Pattern Discrimination Power"
        ],
        "abstract": "The goal of unsupervised feature selection is to identify a feature subset based on the intrinsic characteristics of a given dataset without user-guided information such as class variables. To achieve this, score functions based on information measures can be used to identify essential features. The major research direction of conventional information-theoretic unsupervised feature selection is to minimize the entropy of the final feature subset. Although the opposite way, i.e., maximization of the joint entropy, can also lead to novel insights, studies in this direction are rare. For example, in the field of information retrieval, selected features that maximize the joint entropy of a feature subset can be effective discriminators for reaching the target tuple in the database. Thus, in this work, we first demonstrate how two feature subsets, each obtained by minimizing/maximizing the joint entropy, respectively, are different based on a toy dataset. By comparing these two feature subsets, we show that the maximization of the joint entropy enhances the pattern discrimination power of the feature subset. Then, we derive a score function by remedying joint entropy calculation; high-dimensional joint entropy calculation is circumvented by using the low-order approximation. The experimental results on 30 public datasets indicate that the proposed method yields superior performance in terms of pattern discrimination power-related measures.",
        "_bibtex": "@inproceedings{\nseo2024unsupervised,\ntitle={Unsupervised Feature Selection towards Pattern Discrimination Power},\nauthor={Wangduk Seo and Jaesung Lee},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=SFA15IB8Ic}\n}"
    },
    {
        "title": "Differentially Private No-regret Exploration in Adversarial Markov Decision Processes",
        "authorids": [
            "~Shaojie_Bai2",
            "~Lanting_Zeng1",
            "~Chengcheng_Zhao1",
            "~Xiaoming_Duan1",
            "~Mohammad_Sadegh_Talebi3",
            "~Peng_Cheng9",
            "~Jiming_Chen1"
        ],
        "keywords": [
            "Differential Privacy",
            "Adversarial Markov Decision Proess",
            "Regret Minimization"
        ],
        "abstract": "We study learning adversarial Markov decision process (MDP) in the episodic setting under the constraint of differential privacy (DP).\nThis is motivated by the widespread applications of reinforcement learning (RL) in non-stationary and even adversarial scenarios, where protecting users' sensitive information is vital.\nWe first propose two efficient frameworks for adversarial MDPs, spanning full-information and bandit settings.\nWithin each framework, we consider both Joint DP (JDP), where a central agent is trusted to protect the sensitive data, and Local DP (LDP), where the information is protected directly on the user side.\nThen, we design novel privacy mechanisms to privatize the stochastic transition and adversarial losses.\nBy instantiating such privacy mechanisms to satisfy JDP and LDP requirements, we obtain near-optimal regret guarantees for both frameworks.\nTo our knowledge, these are the first algorithms to tackle the challenge of private learning in adversarial MDPs.",
        "_bibtex": "@inproceedings{\nbai2024differentially,\ntitle={Differentially Private No-regret Exploration in Adversarial Markov Decision Processes},\nauthor={Shaojie Bai and Lanting Zeng and Chengcheng Zhao and Xiaoming Duan and Mohammad Sadegh Talebi and Peng Cheng and Jiming Chen},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=foNKGt20YE}\n}"
    },
    {
        "title": "Offline Bayesian Aleatoric and Epistemic Uncertainty Quantification and Posterior Value Optimisation in Finite-State MDPs",
        "authorids": [
            "~Filippo_Valdettaro1",
            "~Aldo_A._Faisal1"
        ],
        "keywords": [
            "Uncertain MDPs",
            "Uncertainty quantification",
            "Bayesian decision theory"
        ],
        "abstract": "We address the challenge of quantifying Bayesian uncertainty and incorporating it in offline use cases of finite-state Markov Decision Processes (MDPs) with unknown dynamics.\nOur approach provides a principled method to disentangle epistemic and aleatoric uncertainty, and a novel technique to find policies that optimise Bayesian posterior expected value without relying on strong assumptions about the MDP\u2019s posterior distribution.\nFirst, we utilise standard Bayesian reinforcement learning methods to capture the posterior uncertainty in MDP parameters based on available data.\nWe then analytically compute the first two moments of the return distribution across posterior samples and apply the law of total variance to disentangle aleatoric and epistemic uncertainties. \nTo find policies that maximise posterior expected value, we leverage the closed-form expression for value as a function of policy. This allows us to propose a stochastic gradient-based approach for solving the problem.\nWe illustrate the uncertainty quantification and Bayesian posterior value optimisation performance of our agent in simple, interpretable gridworlds and validate it through ground-truth evaluations on synthetic MDPs.\nFinally, we highlight the real-world impact and computational scalability of our method by applying it to the AI Clinician problem, which recommends treatment for patients in intensive care units and has emerged as a key use case of finite-state MDPs with offline data.\nWe discuss the challenges that arise with Bayesian modelling of larger scale MDPs while demonstrating the potential to apply our methods rooted in Bayesian decision theory into the real world.\nWe make our code available at https://github.com/filippovaldettaro/finite-state-mdps.",
        "_bibtex": "@inproceedings{\nvaldettaro2024offline,\ntitle={Offline Bayesian Aleatoric and Epistemic Uncertainty Quantification and Posterior Value Optimisation in Finite-State {MDP}s},\nauthor={Filippo Valdettaro and Aldo A. Faisal},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=Kr0UJQ7Vbx}\n}"
    },
    {
        "title": "Domain Adaptation with Cauchy-Schwarz Divergence",
        "authorids": [
            "~Wenzhe_Yin1",
            "~Shujian_Yu1",
            "~Yicong_Lin2",
            "~Jie_Liu21",
            "~Jan-Jakob_Sonke2",
            "~Stratis_Gavves1"
        ],
        "keywords": [
            "Domain Adaptation",
            "Transfer Learning",
            "Cauchy-Schwarz Divergence"
        ],
        "abstract": "Domain adaptation aims to use training data from one or multiple source domains to learn a hypothesis that can be generalized to a different, but related, target domain. As such, having a reliable measure for evaluating the discrepancy of both marginal and conditional distributions is crucial. We introduce Cauchy-Schwarz (CS) divergence to the problem of unsupervised domain adaptation (UDA). The CS divergence offers a theoretically tighter generalization error bound than the popular Kullback-Leibler divergence. This holds for the general case of supervised learning, including multi-class classification and regression. Furthermore, we illustrate that the CS divergence enables a simple estimator on the discrepancy of both marginal and conditional distributions between source and target domains in the representation space, without requiring any distributional assumptions. We provide multiple examples to illustrate how the CS divergence can be conveniently used in both distance metric- or adversarial training-based UDA frameworks, resulting in compelling performance. The code of our paper is available at \\url{https://github.com/ywzcode/CS-adv}.",
        "_bibtex": "@inproceedings{\nyin2024domain,\ntitle={Domain Adaptation with Cauchy-Schwarz Divergence},\nauthor={Wenzhe Yin and Shujian Yu and Yicong Lin and Jie Liu and Jan-Jakob Sonke and Stratis Gavves},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=62m7yvkGIY}\n}"
    },
    {
        "title": "Equilibrium Computation in Multidimensional Congestion Games: CSP and Learning Dynamics Approaches",
        "authorids": [
            "~Mohammad_T._Irfan1",
            "~Hau_Chan1",
            "~Jared_Soundy1"
        ],
        "keywords": [
            "computational game theory",
            "congestion game",
            "constraint satisfaction problem",
            "CSP",
            "learning dynamics",
            "Nash equilibrium"
        ],
        "abstract": "We present algorithms of two flavors\u2014one rooted in constraint satisfaction problems (CSPs) and the other in learning dynamics\u2014to compute pure-strategy Nash equilibrium (PSNE) in k-dimensional congestion games (k-DCGs) and their variants. The two algorithmic approaches are driven by whether or not a PSNE is guaranteed to exist. We first show that deciding the existence of a PSNE in a k-DCG is NP-complete even when players have binary and unit demand vectors. For general cost functions (potentially non-monotonic), we devise a new CSP-inspired algorithmic framework for PSNE computation, leading to algorithms that run in polynomial time under certain assumptions while offering exponential savings over standard CSP algorithms. We further refine these algorithms for variants of k-DCGs. Our experiments demonstrate the effectiveness of this new CSP framework for hard, non-monotonic k-DCGs. We then provide learning dynamics-based PSNE computation algorithms for linear and exponential cost functions. These algorithms run in polynomial time under certain assumptions. For general cost, we give a learning dynamics algorithm for an (\u03b1, \u03b2)-approximate PSNE (for certain \u03b1 and \u03b2). Lastly, we also devise polynomial-time algorithms for structured demands and cost functions.",
        "_bibtex": "@inproceedings{\nirfan2024equilibrium,\ntitle={Equilibrium Computation in Multidimensional Congestion Games: {CSP} and Learning Dynamics Approaches},\nauthor={Mohammad T. Irfan and Hau Chan and Jared Soundy},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=9Sw5gGPzQ1}\n}"
    },
    {
        "title": "Sample Average Approximation for Black-Box Variational Inference",
        "authorids": [
            "~Javier_Burroni1",
            "~Justin_Domke1",
            "~Daniel_Sheldon1"
        ],
        "keywords": [
            "black-box variational inference",
            "sample average approximation",
            "stochastic optimization"
        ],
        "abstract": "Black-box variational inference (BBVI) is a general-purpose approximate inference approach that converts inference to a stochastic optimization problem.\nHowever, the difficulty of solving the BBVI optimization problem reliably and robustly using stochastic gradient methods has limited its applicability. \nWe present a novel optimization approach for BBVI using the sample average approximation (SAA). \nSAA converts stochastic problems to deterministic ones by optimizing over a fixed random sample, which enables optimization tools such as quasi-Newton methods and line search that bypass the difficulties faced by stochastic gradient methods.\nWe design an approach called \"SAA for VI\" that solves a sequence of SAA problems with increasing sample sizes to reliably and robustly solve BBVI problems without problem-specific tuning. \nWe focus on quasi-Newton methods, which are well suited to problems with up to hundreds of latent variables.\nOur experiments show that SAA for VI simplifies the VI problem and achieves faster performance than existing methods.",
        "_bibtex": "@inproceedings{\nburroni2024sample,\ntitle={Sample Average Approximation for Black-Box Variational Inference},\nauthor={Javier Burroni and Justin Domke and Daniel Sheldon},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=1EV6XonWQx}\n}"
    },
    {
        "title": "$\\chi$SPN: Characteristic Interventional Sum-Product Networks for Causal Inference in Hybrid Domains",
        "authorids": [
            "~Harsh_Poonia1",
            "~Moritz_Willig1",
            "~Zhongjie_Yu2",
            "~Matej_Ze\u010devi\u01071",
            "~Kristian_Kersting1",
            "~Devendra_Singh_Dhami1"
        ],
        "keywords": [
            "causality",
            "hybrid domains",
            "sum-product networks",
            "characteristic functions"
        ],
        "abstract": "Causal inference in hybrid domains, characterized by a mixture of discrete and continuous variables, presents a formidable challenge. We take a step towards this direction and propose \\textbf{Ch}aracteristic \\textbf{I}nterventional Sum-Product Network ($\\chi$SPN) that is capable of estimating interventional distributions in presence of random variables drawn from mixed distributions. $\\chi$SPN uses characteristic functions in the leaves of an interventional SPN (iSPN) thereby providing a unified view for discrete and continuous random variables through the Fourier\u2013Stieltjes transform of the probability measures. A neural network is used to estimate the parameters of the learned iSPN using the intervened data. Our experiments on 3 synthetic heterogeneous datasets suggest that $\\chi$SPN can effectively capture the interventional distributions for both discrete and continuous variables while being expressive and causally adequate. We also show that $\\chi$SPN generalize to multiple interventions while being trained only on a single intervention data.",
        "_bibtex": "@inproceedings{\npoonia2024chispn,\ntitle={\\${\\textbackslash}chi\\${SPN}: Characteristic Interventional Sum-Product Networks for Causal Inference in Hybrid Domains},\nauthor={Harsh Poonia and Moritz Willig and Zhongjie Yu and Matej Ze{\\v{c}}evi{\\'c} and Kristian Kersting and Devendra Singh Dhami},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=s3kqfH5KBI}\n}"
    },
    {
        "title": "Shedding Light on Large Generative Networks: Estimating Epistemic Uncertainty in Diffusion Models",
        "authorids": [
            "~Lucas_Berry1",
            "~Axel_Brando1",
            "~David_Meger2"
        ],
        "keywords": [
            "Uncertainty Quantification",
            "Diffusion Models",
            "Image Generation"
        ],
        "abstract": "Generative diffusion models, notable for their large parameter count (exceeding 100 million) and operation within high-dimensional image spaces, pose significant challenges for traditional uncertainty estimation methods due to computational demands. In this work, we introduce an innovative framework, Diffusion Ensembles for Capturing Uncertainty (DECU), designed for estimating epistemic uncertainty for diffusion models. The DECU framework introduces a novel method that efficiently trains ensembles of conditional diffusion models by incorporating a static set of pre-trained parameters, drastically reducing the computational burden and the number of parameters that require training. Additionally, DECU employs Pairwise-Distance Estimators (PaiDEs) to accurately measure epistemic uncertainty by evaluating the mutual information between model outputs and weights in high-dimensional spaces. The effectiveness of this framework is demonstrated through experiments on the ImageNet dataset, highlighting its capability to capture epistemic uncertainty, specifically in under-sampled image classes.",
        "_bibtex": "@inproceedings{\nberry2024shedding,\ntitle={Shedding Light on Large Generative Networks: Estimating Epistemic Uncertainty in Diffusion Models},\nauthor={Lucas Berry and Axel Brando and David Meger},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=512IkGDqA8}\n}"
    },
    {
        "title": "Linearly Constrained Gaussian Processes are SkewGPs: application to Monotonic Preference Learning and Desirability",
        "authorids": [
            "~Alessio_Benavoli2",
            "~Dario_Azzimonti1"
        ],
        "keywords": [
            "linearly constrained",
            "inequality",
            "Gaussian Process",
            "Skew Gaussian Process",
            "preference",
            "monotonicity",
            "desiderability"
        ],
        "abstract": "We show that existing approaches to Linearly Constrained Gaussian Processes (LCGP) for regression, based on imposing constraints on a finite set of operational points, can be seen as Skew Gaussian Processes (SkewGPs). In particular, focusing on inequality constraints and building upon a recent unification of regression, classification, and preference learning through SkewGPs, we extend LCGP to handle monotonic preference learning and desirability, crucial for  understanding and predicting human decision making.  We demonstrate the efficacy of the proposed model on simulated and real data.",
        "_bibtex": "@inproceedings{\nbenavoli2024linearly,\ntitle={Linearly Constrained Gaussian Processes are Skew{GP}s: application to Monotonic Preference Learning and Desirability},\nauthor={Alessio Benavoli and Dario Azzimonti},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=CJqN5503XZ}\n}"
    },
    {
        "title": "Uncertainty Estimation with Recursive Feature Machines",
        "authorids": [
            "~Daniel_Gedon1",
            "~Amirhesam_Abedsoltan1",
            "~Thomas_B._Sch\u00f6n1",
            "~Mikhail_Belkin1"
        ],
        "keywords": [
            "Uncertainty estimation",
            "kernel methods",
            "gaussian processes",
            "boosting",
            "feature learning"
        ],
        "abstract": "In conventional regression analysis, predictions are typically represented as point estimates derived from covariates. The Gaussian Process (GP) offer a kernel-based framework that predicts and quantifies associated uncertainties. However, kernel-based methods often underperform ensemble-based decision tree approaches in regression tasks involving tabular and categorical data. Recently, Recursive Feature Machines (RFMs) were proposed as a novel feature-learning kernel which strengthens the capabilities of kernel machines. In this study, we harness the power of these RFMs in a probabilistic GP-based approach to enhance uncertainty estimation through feature extraction within kernel methods. We employ this learned kernel for in-depth uncertainty analysis. On tabular datasets, our RFM-based method surpasses other leading uncertainty estimation techniques, including NGBoost and CatBoost-ensemble. Additionally, when assessing out-of-distribution performance, we found that boosting-based methods are surpassed by our RFM-based approach.",
        "_bibtex": "@inproceedings{\ngedon2024uncertainty,\ntitle={Uncertainty Estimation with Recursive Feature Machines},\nauthor={Daniel Gedon and Amirhesam Abedsoltan and Thomas B. Sch{\\\"o}n and Mikhail Belkin},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=TBKLXswKnO}\n}"
    },
    {
        "title": "Bandits with Knapsacks and Predictions",
        "authorids": [
            "~Davide_Drago1",
            "~Andrea_Celli1",
            "~Marek_Elias1"
        ],
        "keywords": [
            "bandits with knapsacks",
            "ML-augmented algorithms",
            "algorithms with predictions"
        ],
        "abstract": "We study the Bandits with Knapsacks problem with the aim of designing a learning-augmented online learning algorithm upholding better regret guarantees than the state-of-the-art primal-dual algorithms with worst-case guarantees, under both stochastic and adversarial inputs. In the adversarial case, we obtain better competitive ratios when the input predictions are accurate, while also maintaining worst-case guarantees for imprecise predictions. We introduce two algorithms tailored for the full and bandit feedback settings, respectively. Both algorithms integrate a static prediction with a worst-case no-$\\alpha$-regret algorithm. This yields an optimized competitive ratio of $(\\pi + (1 -\\pi)/\\alpha)^{-1}$ in scenarios where the prediction is perfect, and a competitive ratio of $\\alpha/(1 - \\pi)$ in the case of highly imprecise predictions, where $\\pi \\in (0,1)$ is chosen by the learner and $\\alpha$ is Slater's parameter. We complement this analysis by studying the stochastic setting under full feedback. We provide an algorithm which guarantees a pseudo-regret of $\\widetilde{O}(\\sqrt{T})$ with poor predictions, and 0 pseudo-regret with perfect predictions. We also characterize the smoothness of the algorithm.",
        "_bibtex": "@inproceedings{\ndrago2024bandits,\ntitle={Bandits with Knapsacks and Predictions},\nauthor={Davide Drago and Andrea Celli and Marek Elias},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=pGnoioQ9z1}\n}"
    },
    {
        "title": "MetaCOG: A Heirarchical Probabilistic Model for Learning Meta-Cognitive Visual Representations",
        "authorids": [
            "~Marlene_Berke1",
            "~Zhangir_Azerbayev1",
            "~Mario_Belledonne1",
            "~Zenna_Tavares1",
            "~Julian_Jara-Ettinger1"
        ],
        "keywords": [
            "Bayesian",
            "Probablistic modeling",
            "Meta-cognition",
            "Neurosymbolic AI"
        ],
        "abstract": "Humans have the capacity to question what we see and to recognize when our vision is unreliable (e.g., when we realize that we are experiencing a visual illusion). Inspired by this capacity, we present MetaCOG: a hierarchical probabilistic model that can be attached to a neural object detector to monitor its outputs and determine their reliability. MetaCOG achieves this by learning a probabilistic model of the object detector\u2019s performance via Bayesian inference\u2014i.e., a meta-cognitive representation of the network\u2019s propensity to hallucinate or miss different object categories. Given a set of video frames processed by an object detector, MetaCOG performs joint inference over the underlying 3D scene and the detector\u2019s performance, grounding inference on a basic assumption of object permanence. Paired with three neural object detectors, we show that MetaCOG accurately recovers each detector\u2019s performance parameters and improves the overall system\u2019s accuracy. We additionally show that MetaCOG is robust to varying levels of error in object detector outputs, showing proof-of-concept for a novel approach to the problem of detecting and correcting errors in vision systems when ground-truth is not available.",
        "_bibtex": "@inproceedings{\nberke2024metacog,\ntitle={Meta{COG}: A Heirarchical Probabilistic Model for Learning Meta-Cognitive Visual Representations},\nauthor={Marlene Berke and Zhangir Azerbayev and Mario Belledonne and Zenna Tavares and Julian Jara-Ettinger},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=hESlov4NHG}\n}"
    },
    {
        "title": "Computing Low-Entropy Couplings for Large-Support Distributions",
        "authorids": [
            "~Samuel_Sokota1",
            "~Dylan_Sam1",
            "~Christian_Schroeder_de_Witt1",
            "~Spencer_Compton1",
            "~Jakob_Nicolaus_Foerster1",
            "~J_Zico_Kolter1"
        ],
        "keywords": [
            "minimum-entropy coupling",
            "steganography"
        ],
        "abstract": "Minimum-entropy coupling (MEC)---the process of finding a joint distribution with minimum entropy for given marginals---has applications in areas such as causality and steganography. However, existing algorithms are either computationally intractable for large-support distributions or limited to specific distribution types and sensitive to hyperparameter choices. This work addresses these limitations by unifying a prior family of iterative MEC (IMEC) approaches into a generalized partition-based formalism. From this framework, we derive a novel IMEC algorithm called ARIMEC, capable of handling arbitrary discrete distributions, and introduce a method to make IMEC robust to suboptimal hyperparameter settings. These innovations facilitate the application of IMEC to high-throughput steganography with language models, among other settings.",
        "_bibtex": "@inproceedings{\nsokota2024computing,\ntitle={Computing Low-Entropy Couplings for Large-Support Distributions},\nauthor={Samuel Sokota and Dylan Sam and Christian Schroeder de Witt and Spencer Compton and Jakob Nicolaus Foerster and J Zico Kolter},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=4IfjcKoU15}\n}"
    },
    {
        "title": "DataSP: A Differential All-to-All Shortest Path Algorithm for Learning Costs and Predicting Paths with Context",
        "authorids": [
            "~Alan_Lahoud1",
            "~Erik_Schaffernicht1",
            "~Johannes_A._Stork1"
        ],
        "keywords": [
            "neural networks",
            "inverse optimization",
            "path planning",
            "structured predictions",
            "shortest path"
        ],
        "abstract": "Learning latent costs of transitions on graphs from trajectories demonstrations under various contextual features is challenging but useful for path planning. Yet, existing methods either oversimplify cost assumptions or scale poorly with the number of observed trajectories. This paper introduces DataSP, a differentiable all-to-all shortest path algorithm to facilitate learning latent costs from trajectories. It allows to learn from a large number of trajectories in each learning step without additional computation. Complex latent cost functions from contextual features can be represented in the algorithm through a neural network approximation. We further propose a method to sample paths from DataSP in order to reconstruct/mimic observed paths' distributions. We prove that the inferred distribution follows the maximum entropy principle. We show that DataSP outperforms state-of-the-art differentiable combinatorial solver and classical machine learning approaches in predicting paths on graphs.",
        "_bibtex": "@inproceedings{\nlahoud2024datasp,\ntitle={Data{SP}: A Differential All-to-All Shortest Path Algorithm for Learning Costs and Predicting Paths with Context},\nauthor={Alan Lahoud and Erik Schaffernicht and Johannes A. Stork},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=nbwSnQBLU3}\n}"
    },
    {
        "title": "Unified PAC-Bayesian Study of Pessimism for Offline Policy Learning with Regularized Importance Sampling",
        "authorids": [
            "~Imad_Aouali2",
            "~Victor-Emmanuel_Brunel1",
            "~David_Rohde1",
            "~Anna_Korba2"
        ],
        "keywords": [
            "Off-Policy Learning",
            "Importance Sampling",
            "PAC-Bayes"
        ],
        "abstract": "Off-policy learning (OPL) often involves minimizing a risk estimator based on importance weighting to correct bias from the logging policy used to collect data. However, this method can produce an estimator with a high variance. A common solution is to regularize the importance weights and learn the policy by minimizing an estimator with penalties derived from generalization bounds specific to the estimator. This approach, known as pessimism, has gained recent attention but lacks a unified framework for analysis. To address this gap, we introduce a comprehensive PAC-Bayesian framework to examine pessimism with regularized importance weighting. We derive a tractable PAC-Bayesian generalization bound that universally applies to common importance weight regularizations, enabling their comparison within a single framework. Our empirical results challenge common understanding, demonstrating the effectiveness of standard IW regularization techniques.",
        "_bibtex": "@inproceedings{\naouali2024unified,\ntitle={Unified {PAC}-Bayesian Study of Pessimism for Offline Policy Learning with Regularized Importance Sampling},\nauthor={Imad Aouali and Victor-Emmanuel Brunel and David Rohde and Anna Korba},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=d7W4H0sTXU}\n}"
    },
    {
        "title": "Partial identification of the maximum mean discrepancy with mismeasured data",
        "authorids": [
            "~Ron_Nafshi1",
            "~Maggie_Makar1"
        ],
        "keywords": [
            "maximum mean discrepancy",
            "measurement error",
            "uncertainty quantification"
        ],
        "abstract": "Nonparametric estimates of the distance between two distributions such as the Maximum Mean Discrepancy (MMD) are often used in machine learning applications. However, the majority of existing literature assumes that error-free samples from the two distributions of interest are available.We relax this assumption and study the estimation of the MMD under $\\epsilon$-contamination, where a possibly non-random $\\epsilon$ proportion of one distribution is erroneously grouped with the other. We show that under $\\epsilon$-contamination, the typical estimate of the MMD is unreliable. Instead, we study partial identification of the MMD, and characterize sharp upper and lower bounds that contain the true, unknown MMD. We propose a method to estimate these bounds, and show that it gives estimates that converge to the sharpest possible bounds on the MMD as sample size increases, with a convergence rate that is faster than alternative approaches. Using three datasets, we empirically validate that our approach is superior to the alternatives: it gives tight bounds with a low false coverage rate.",
        "_bibtex": "@inproceedings{\nnafshi2024partial,\ntitle={Partial identification of the maximum mean discrepancy with mismeasured data},\nauthor={Ron Nafshi and Maggie Makar},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=2vWb5l3xXy}\n}"
    },
    {
        "title": "Learning Distributionally Robust Tractable Probabilistic Models in Continuous Domains",
        "authorids": [
            "~Hailiang_Dong2",
            "~James_Amato1",
            "~Vibhav_Giridhar_Gogate1",
            "~Nicholas_Ruozzi1"
        ],
        "keywords": [
            "Probabilistic Graphical Models",
            "Robust and Reliable ML",
            "Distribution Shift"
        ],
        "abstract": "Tractable probabilistic models (TPMs) have attracted substantial research interest in recent years, particularly because of their ability to answer various reasoning queries in polynomial time. In this study, we focus on the distributionally robust learning of continuous TPMs and address the challenge of distribution shift at test time by tackling the adversarial risk minimization problem within the framework of distributionally robust learning. Specifically, we demonstrate that the adversarial risk minimization problem can be efficiently addressed when the model permits exact log-likelihood evaluation and efficient learning on weighted data. Our experimental results on several real-world datasets show that our approach achieves significantly higher log-likelihoods on adversarial test sets. Remarkably, we note that the model learned via distributionally robust learning can achieve higher average log-likelihood on the initial uncorrupted test set at times.",
        "_bibtex": "@inproceedings{\ndong2024learning,\ntitle={Learning Distributionally Robust Tractable Probabilistic Models in Continuous Domains},\nauthor={Hailiang Dong and James Amato and Vibhav Giridhar Gogate and Nicholas Ruozzi},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=SlxO1NpLiE}\n}"
    },
    {
        "title": "Identifiability of total effects from abstractions of time series causal graphs",
        "authorids": [
            "~Charles_K._Assaad1",
            "~Emilie_Devijver1",
            "~Eric_Gaussier1",
            "~Gregor_Goessler1",
            "~Anouar_Meynaoui1"
        ],
        "keywords": [
            "Causal analysis",
            "identification",
            "time series",
            "summary causal graph"
        ],
        "abstract": "We study the problem of identifiability of the total effect of an intervention from observational time series only given an abstraction of the causal graph of the system. Specifically, we consider two types of abstractions: the extended summary causal graph which conflates all lagged causal relations but distinguishes between lagged and instantaneous relations; and the summary causal graph which does not give any indication about the lag between causal relations. We show that the total effect is always identifiable in extended summary causal graphs and we provide necessary and sufficient graphical conditions for identifiability in summary causal graphs. Furthermore, we provide adjustment sets allowing to estimate the total effect whenever it is identifiable.",
        "_bibtex": "@inproceedings{\nassaad2024identifiability,\ntitle={Identifiability of total effects from abstractions of time series causal graphs},\nauthor={Charles K. Assaad and Emilie Devijver and Eric Gaussier and Gregor Goessler and Anouar Meynaoui},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=lL4EzE8bY8}\n}"
    },
    {
        "title": "No-Regret Learning of Nash Equilibrium for Black-Box Games via Gaussian Processes",
        "authorids": [
            "~Minbiao_Han1",
            "~Fengxue_Zhang1",
            "~Yuxin_Chen1"
        ],
        "keywords": [
            "Nash Equilibrium",
            "Game Theory",
            "Gaussian Process"
        ],
        "abstract": "This paper investigates the challenge of learning in black-box games, where the underlying utility function is unknown to any of the agents. While there is an extensive body of literature on the theoretical analysis of algorithms for computing the Nash equilibrium with *complete information* about the game, studies on Nash equilibrium in *black-box* games are less common. In this paper, we focus on learning the Nash equilibrium when the only available information about an agent's payoff comes in the form of empirical queries. We provide a no-regret learning algorithm that utilizes Gaussian processes to identify equilibria in such games. Our approach not only ensures a theoretical convergence rate but also demonstrates effectiveness across a variety collection of games through experimental validation.",
        "_bibtex": "@inproceedings{\nhan2024noregret,\ntitle={No-Regret Learning of Nash Equilibrium for Black-Box Games via Gaussian Processes},\nauthor={Minbiao Han and Fengxue Zhang and Yuxin Chen},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=LMcHRkpSKZ}\n}"
    },
    {
        "title": "Neural Active Learning Meets the Partial Monitoring Framework",
        "authorids": [
            "~Maxime_Heuillet1",
            "~Ola_Ahmad1",
            "~Audrey_Durand1"
        ],
        "keywords": [
            "online learning",
            "active learning",
            "partial monitoring",
            "online classification"
        ],
        "abstract": "We focus on the online-based active learning (OAL) setting where an agent operates over a stream of observations and trades-off between the costly acquisition of information (labelled observations) and the cost of prediction errors.\nWe propose a novel foundation for OAL tasks based on partial monitoring, a theoretical framework specialized in online learning from partially informative actions. \nWe show that previously studied binary and multi-class OAL tasks are instances of partial monitoring.\nWe expand the real-world potential of OAL by introducing a new class of cost-sensitive OAL tasks.\nWe propose NeuralCBP, the first PM strategy that accounts for predictive uncertainty with deep neural networks.\nOur extensive empirical evaluation on open source datasets shows that NeuralCBP has competitive performance against state-of-the-art baselines on multiple binary, multi-class and cost-sensitive OAL tasks.",
        "_bibtex": "@inproceedings{\nheuillet2024neural,\ntitle={Neural Active Learning Meets the Partial Monitoring Framework},\nauthor={Maxime Heuillet and Ola Ahmad and Audrey Durand},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=K99ZSDvGiD}\n}"
    },
    {
        "title": "Amortized Variational Inference: When and Why?",
        "authorids": [
            "~Charles_Margossian1",
            "~David_Blei2"
        ],
        "keywords": [
            "variational inference; Bayesian inference; latent variable model"
        ],
        "abstract": "In a probabilistic latent variable model, factorized (or mean-field) variational inference (F-VI) fits a separate parametric distribution for each latent variable.\nAmortized variational inference (A-VI) instead learns a common inference function, which maps each observation to its corresponding latent variable's approximate posterior.\n  Typically, A-VI is used as a cog in the training of variational autoencoders, however it stands to reason that A-VI could also be used as a general alternative to F-VI.\n  In this paper we study when and why A-VI can be used for approximate Bayesian inference.\n  We derive conditions on a latent variable model which are necessary, sufficient, and verifiable under which A-VI can attain F-VI's optimal solution, thereby closing the amortization gap.\n  We prove these conditions are uniquely verified by simple hierarchical models, a broad class that encompasses many models in machine learning.\n  We then show, on a broader class of models, how to expand the domain of AVI\u2019s inference function to improve its solution, and we provide examples, e.g. hidden Markov models, where the amortization gap cannot be closed.",
        "_bibtex": "@inproceedings{\nmargossian2024amortized,\ntitle={Amortized Variational Inference: When and Why?},\nauthor={Charles Margossian and David Blei},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=mCVYIsnctr}\n}"
    },
    {
        "title": "Multi-layer random features and the approximation power of neural networks",
        "authorids": [
            "~Rustem_Takhanov1"
        ],
        "keywords": [
            "Gaussian Process",
            "Barron's theorem",
            "approximation power",
            "multi-layer neural networks",
            "neural tangent kernel",
            "RKHS"
        ],
        "abstract": "A neural architecture with randomly initialized weights, in the infinite width limit, is equivalent to a Gaussian Random Field whose covariance function is the so-called Neural Network Gaussian Process kernel (NNGP). We prove that a reproducing kernel Hilbert space (RKHS) defined by the NNGP contains only functions that can be approximated by the architecture. To achieve a certain approximation error the required number of neurons in each layer is defined by the RKHS norm of the target function. Moreover, the approximation can be constructed from a supervised dataset by a random multi-layer representation of an input vector, together with training of the last layer's weights.\n\nFor a 2-layer NN and a domain equal to an $n-1$-dimensional sphere in ${\\mathbb R}^n$, we compare the number of neurons required by Barron's theorem and by the multi-layer features construction. We show that if eigenvalues of the integral operator of the NNGP decay slower than $k^{-n-\\frac{2}{3}}$ where $k$ is an order of an eigenvalue, then our theorem guarantees a more succinct neural network approximation than Barron's theorem. We also make some computational experiments to verify our theoretical findings. Our experiments show that realistic neural networks easily learn target functions even when both theorems do not give any guarantees.",
        "_bibtex": "@inproceedings{\ntakhanov2024multilayer,\ntitle={Multi-layer random features and the approximation power of neural networks},\nauthor={Rustem Takhanov},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=R6T9yA4aNg}\n}"
    },
    {
        "title": "Detecting critical treatment effect bias in small subgroups",
        "authorids": [
            "~Piersilvio_De_Bartolomeis1",
            "~Javier_Abad1",
            "~Konstantin_Donhauser1",
            "~Fanny_Yang1"
        ],
        "keywords": [
            "treatment effect",
            "causal inference",
            "confounding"
        ],
        "abstract": "Randomized trials are considered the gold standard for making informed decisions in medicine. However, they are often not representative of the patient population in clinical practice.  Observational studies, on the other hand, cover a broader patient population but are prone to various biases. Thus, before using observational data for any downstream task, it is crucial to benchmark its treatment effect estimates against a randomized trial. \nWe propose a novel strategy to benchmark observational studies on a subgroup level. First, we design a statistical test for the null hypothesis that the treatment effects -- conditioned on a subset of relevant features --  differ up to some tolerance value. Our test allows us to estimate an asymptotically valid lower bound on the maximum bias strength for any subgroup.  We validate our lower bound in a real-world setting and show that it leads to conclusions that align with established medical knowledge.",
        "_bibtex": "@inproceedings{\nbartolomeis2024detecting,\ntitle={Detecting critical treatment effect bias in small subgroups},\nauthor={Piersilvio De Bartolomeis and Javier Abad and Konstantin Donhauser and Fanny Yang},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=A6K0r8bjNg}\n}"
    },
    {
        "title": "Zero Inflation as a Missing Data Problem: a Proxy-based Approach",
        "authorids": [
            "~Trung_Q._Phung1",
            "~Jaron_J.R._Lee1",
            "~Opeyemi_Oladapo-Shittu1",
            "~Eili_Y._Klein1",
            "~Ayse_Pinar_Gurses1",
            "~Susan_M._Hannum1",
            "~Kimberly_Weems2",
            "~Jill_A._Marsteller1",
            "~Sara_E._Cosgrove1",
            "~Sara_C._Keller1",
            "~Ilya_Shpitser1"
        ],
        "keywords": [
            "zero-inflated data",
            "missing data",
            "proxy",
            "causal inference",
            "partial identification",
            "sensitivity analysis",
            "graphical models"
        ],
        "abstract": "A common type of zero-inflated data has certain true values incorrectly replaced by zeros due to data recording conventions (rare outcomes assumed to be absent) or details of data recording equipment (e.g. artificial zeros in gene expression data).\n\nExisting methods for zero-inflated data either fit the observed data likelihood via parametric mixture models that explicitly represent excess zeros, or aim to replace excess zeros by imputed values.  If the goal of the analysis relies on knowing true data realizations, a particular challenge with zero-inflated data is identifiability, since it is difficult to correctly determine which observed zeros are real and which are inflated.\n\nThis paper views zero-inflated data as a general type of missing data problem, where the observability indicator for a potentially censored variable is itself unobserved whenever a zero is recorded. We show that, without additional assumptions, target parameters involving a zero-inflated variable are not identified. However, if a proxy of the missingness indicator is observed, a modification of the effect restoration approach of Kuroki and Pearl allows identification and estimation, given the proxy-indicator relationship is known.\n\nIf this relationship is unknown, our approach yields a partial identification strategy for sensitivity analysis. Specifically, we show that only certain proxy-indicator relationships are compatible with the observed data distribution. We give an analytic bound for this relationship in cases with a categorical outcome, which is sharp in certain models. For more complex cases, sharp numerical bounds may be computed using methods in Duarte et al. [2023].\n\nWe illustrate our method via simulation studies and a data application on central line-associated bloodstream infections (CLABSIs).",
        "_bibtex": "@inproceedings{\nphung2024zero,\ntitle={Zero Inflation as a Missing Data Problem: a Proxy-based Approach},\nauthor={Trung Q. Phung and Jaron J.R. Lee and Opeyemi Oladapo-Shittu and Eili Y. Klein and Ayse Pinar Gurses and Susan M. Hannum and Kimberly Weems and Jill A. Marsteller and Sara E. Cosgrove and Sara C. Keller and Ilya Shpitser},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=7MYznm5Kp2}\n}"
    },
    {
        "title": "Invariant Causal Prediction with Local Models",
        "authorids": [
            "~Alexander_Mey1",
            "~Rui_M._Castro1"
        ],
        "keywords": [
            "Causal Discovery",
            "Invariant Causal Prediction",
            "Structural Invariance"
        ],
        "abstract": "We consider the task of identifying the causal parents of a target variable among a set of candidates from observational data. Our main assumption is that the candidate variables are observed in different environments which may, under certain assumptions, be regarded as interventions on the observed system. We assume a linear relationship between target and candidates, which can be different in each environment with the only restriction that the causal structure is invariant across environments. Within our proposed setting we provide sufficient conditions for identifiability of the causal parents and introduce a practical method called L-ICP ($\\textbf{L}$ocalized $\\textbf{I}$nvariant $\\textbf{Ca}$usal $\\textbf{P}$rediction), which is based on a hypothesis test for parent identification using a ratio of minimum and maximum statistics. We then show in a simplified setting that the statistical power of L-ICP converges exponentially fast in the sample size, and finally we analyze the behavior of L-ICP experimentally in more general settings.",
        "_bibtex": "@inproceedings{\nmey2024invariant,\ntitle={Invariant Causal Prediction with Local Models},\nauthor={Alexander Mey and Rui M. Castro},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=rbvGwalXYB}\n}"
    },
    {
        "title": "Fast Interactive Search under a Scale-Free Comparison Oracle",
        "authorids": [
            "~Lars_Henning_Klein1",
            "~Daniyar_Chumbalov1",
            "~Lucas_Maystre1",
            "~Matthias_Grossglauser1"
        ],
        "keywords": [
            "probabilistic modeling",
            "interactive search",
            "human computer interaction",
            "recommender systems",
            "oracle model"
        ],
        "abstract": "A comparison-based search algorithm lets a user find a target item $t$ in a database by answering queries of the form, ``Which of items $i$ and $j$ is closer to $t$?''\n    Instead of formulating an explicit query (such as one or several keywords), the user navigates towards the target via a sequence of such (typically noisy) queries.\n    We propose a scale-free probabilistic oracle model called $\\gamma$-CKL for such similarity triplets $(i,j;t)$, which generalizes the CKL triplet model proposed in the literature.\n    The generalization affords independent control over the discriminating power of the oracle and the dimension of the feature space containing the items.\n    We develop a search algorithm with provably exponential rate of convergence under the $\\gamma$-CKL oracle, thanks to a backtracking strategy that deals with the unavoidable errors in updating the belief region around the target.\n    We evaluate the performance of the algorithm both over the posited oracle and over several real-world triplet datasets.\n    We also report on a comprehensive user study, where human subjects navigate a database of face portraits.",
        "_bibtex": "@inproceedings{\nklein2024fast,\ntitle={Fast Interactive Search under a Scale-Free Comparison Oracle},\nauthor={Lars Henning Klein and Daniyar Chumbalov and Lucas Maystre and Matthias Grossglauser},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=dzqbQTm4bi}\n}"
    },
    {
        "title": "Latent Representation Entropy Density for Distribution Shift Detection",
        "authorids": [
            "~Fabio_Arnez1",
            "~Daniel_Alfonso_Montoya_Vasquez1",
            "~Ansgar_Radermacher1",
            "~Fran\u00e7ois_Terrier1"
        ],
        "keywords": [
            "entropy",
            "uncertainty",
            "out-of-distribution detection",
            "distribution shift",
            "density",
            "distance"
        ],
        "abstract": "Distribution shift detection is paramount in safety-critical tasks that rely on Deep Neural Networks (DNNs). The detection task entails deriving a confidence score to assert whether a new input sample aligns with the training data distribution of the DNN model. While DNN predictive uncertainty offers an intuitive confidence measure, exploring uncertainty-based distribution shift detection with simple sample-based techniques has been relatively overlooked in recent years due to computational overhead and lower performance than plain post-hoc methods. This paper proposes using simple sample-based techniques for estimating uncertainty and employing the entropy density from intermediate representations to detect distribution shifts. We demonstrate the effectiveness of our method using standard benchmark datasets for out-of-distribution detection and across different common perception tasks with convolutional neural network architectures. Our scope extends beyond classification, encompassing image-level distribution shift detection for object detection and semantic segmentation tasks. Our results show that our method's performance is comparable to existing \\textit{State-of-the-Art} methods while being computationally faster and lighter than other Bayesian approaches, affirming its practical utility.",
        "_bibtex": "@inproceedings{\narnez2024latent,\ntitle={Latent Representation Entropy Density for Distribution Shift Detection},\nauthor={Fabio Arnez and Daniel Alfonso Montoya Vasquez and Ansgar Radermacher and Fran{\\c{c}}ois Terrier},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=1CKLfh3Ge7}\n}"
    },
    {
        "title": "Dirichlet Continual Learning: Tackling Catastrophic Forgetting in NLP",
        "authorids": [
            "~Min_Zeng2",
            "~Haiqin_Yang2",
            "~Wei_Xue5",
            "~Qifeng_Liu1",
            "~Yike_Guo1"
        ],
        "keywords": [
            "Continual Learning",
            "Generative-based rehearsal",
            "catastrophic forgetting"
        ],
        "abstract": "Catastrophic forgetting poses a significant challenge in continual learning (CL). In the context of Natural Language Processing, generative-based rehearsal CL methods have made progress in avoiding expensive retraining.  However, generating pseudo samples that accurately capture the task-specific distribution remains a daunting task.  In this paper, we propose Dirichlet Continual Learning (DCL), a novel generative-based rehearsal strategy designed specifically for CL.  Different from the conventional use of Gaussian latent variable in Conditional Variational Autoencoder, DCL employs the flexibility of the Dirichlet distribution to model the latent variable.  This allows DCL to effectively capture sentence-level features from previous tasks and guide the generation of pseudo samples. Additionally, we introduce Jensen-Shannon Knowledge Distillation, a robust logit-based knowledge distillation method that enhances knowledge transfer during pseudo-sample generation.  Our extensive experiments show that DCL outperforms state-of-the-art methods in two typical tasks of task-oriented dialogue systems, demonstrating its efficacy.",
        "_bibtex": "@inproceedings{\nzeng2024dirichlet,\ntitle={Dirichlet Continual Learning: Tackling Catastrophic Forgetting in {NLP}},\nauthor={Min Zeng and Haiqin Yang and Wei Xue and Qifeng Liu and Yike Guo},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=jve2maFPzf}\n}"
    },
    {
        "title": "A Generalized Bayesian Approach to Distribution-on-Distribution Regression",
        "authorids": [
            "~Tin_Lok_James_Ng1"
        ],
        "keywords": [
            "Distribution-on-distribution regression",
            "generalized Bayesian",
            "posterior contraction"
        ],
        "abstract": "In recent years, there has been growing interest in distribution-on-distribution regression, a regression problem where both covariates and responses are represented as probability distributions. Despite various methodologies proposed to address this challenge, a notable absence has been a Bayesian approach, which offers benefits by allowing for the integration of prior knowledge and providing a formal means of quantifying uncertainty. However, a major challenge in employing a Bayesian approach lies in the complexity of fully specifying the data generating process. To overcome this obstacle, we adopt a generalized Bayesian approach and investigate the contraction rates of the resulting generalized (Gibbs) posterior distributions. We propose an MCMC algorithm to sample from the generalized posterior distribution and conduct simulation studies to validate the theoretical findings. Finally, we apply the model to a data application involving mortality data.",
        "_bibtex": "@inproceedings{\nng2024a,\ntitle={A Generalized Bayesian Approach to Distribution-on-Distribution Regression},\nauthor={Tin Lok James Ng},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=UiVfVB87Bs}\n}"
    },
    {
        "title": "Using Autodiff to Estimate Posterior Moments, Marginals and Samples",
        "authorids": [
            "~Sam_Bowyer1",
            "~Thomas_Heap1",
            "~Laurence_Aitchison1"
        ],
        "keywords": [
            "Importance weighting",
            "Importance sampling",
            "Bayesian",
            "Inference",
            "Posterior Moment Estimation"
        ],
        "abstract": "Importance sampling is a popular technique in Bayesian inference: by reweighting samples drawn from a proposal distribution we are able to obtain samples and moment estimates from a Bayesian posterior over latent variables. Recent work, however, indicates that importance sampling scales poorly --- in order to accurately approximate the true posterior, the required number of importance samples grows is exponential in the number of latent variables [Chatterjee and Diaconis, 2018]. Massively parallel importance sampling works around this issue by drawing $K$ samples for each of the $n$ latent variables and reasoning about all $K^n$ combinations of latent samples. In principle, we can reason efficiently over $K^n$ combinations of samples by exploiting conditional independencies in the generative model. However, in practice this requires complex algorithms that traverse backwards through the graphical model, and we need separate backward traversals for each computation (posterior expectations, marginals and samples). Our contribution is to exploit the source term trick from physics to entirely avoid the need to hand-write backward traversals. Instead, we demonstrate how to simply and easily compute all the required quantities --- posterior expectations, marginals and samples --- by differentiating through a slightly modified marginal likelihood estimator.",
        "_bibtex": "@inproceedings{\nbowyer2024using,\ntitle={Using Autodiff to Estimate Posterior Moments, Marginals and Samples},\nauthor={Sam Bowyer and Thomas Heap and Laurence Aitchison},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=QUMZJgrjN0}\n}"
    },
    {
        "title": "Last-iterate Convergence Separation between Extra-gradient and Optimism in Constrained Periodic Games",
        "authorids": [
            "~Yi_Feng3",
            "~Ping_Li14",
            "~Ioannis_Panageas1",
            "~Xiao_Wang4"
        ],
        "keywords": [
            "last-iterate convergence",
            "time-varying games",
            "optimistic methods",
            "extra-gradient methods"
        ],
        "abstract": "Last-iterate behaviors of learning algorithms in repeated two-player zero-sum games have been extensively studied due to their wide applications in machine learning and related tasks. Typical algorithms that exhibit the last-iterate convergence property include optimistic and extra-gradient methods. However, most existing results establish these properties under the assumption that the game is time-independent. Recently, (Feng et al., 2023) studied the last-iterate behaviors of optimistic and extra-gradient methods in games with a time-varying payoff matrix, and proved that in an unconstrained periodic game, extra-gradient method  converges to the equilibrium while optimistic method diverges. This finding challenges the conventional wisdom that these two methods are expected to behave similarly as they do in time-independent games. However, compared to unconstrained games, games with constrains are more common both in practical and theoretical studies. In this paper, we investigate the last-iterate behaviors of optimistic and extra-gradient methods in the constrained periodic games, demonstrating that similar separation results for last-iterate convergence also hold in this setting.",
        "_bibtex": "@inproceedings{\nfeng2024lastiterate,\ntitle={Last-iterate Convergence Separation between Extra-gradient and Optimism in Constrained Periodic Games},\nauthor={Yi Feng and Ping Li and Ioannis Panageas and Xiao Wang},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=IaH9nprqiU}\n}"
    },
    {
        "title": "Finite-Time Analysis of Three-Timescale Constrained Actor-Critic and Constrained Natural Actor-Critic Algorithms.",
        "authorids": [
            "~Prashansa_Panda1",
            "~Shalabh_Bhatnagar1"
        ],
        "keywords": [
            "Constrained actor-critic algorithm",
            "Constrained natural actor-critic algorithm",
            "Three-timescale algorithms",
            "Long-run average cost criterion",
            "Finite-Time Analysis"
        ],
        "abstract": "Actor Critic methods have found immense applications on a wide range of Reinforcement Learning tasks especially when the state-action space is large. In this paper, we consider actor critic and natural actor critic algorithms with function approximation for constrained Markov decision processes (C-MDP) involving inequality constraints and carry out a non-asymptotic analysis for both  of these algorithms in a non-i.i.d (Markovian) setting. We consider the long-run average cost criterion where both the objective and the constraint functions are suitable policy-dependent long-run averages of certain prescribed cost  functions. We handle the inequality constraints using the Lagrange multiplier method.  We prove that these algorithms are guaranteed to find a first-order stationary point (i.e., $\\Vert \\nabla L(\\theta,\\gamma)\\Vert_2^2 \\leq \\epsilon$) of the  performance (Lagrange) function $L(\\theta,\\gamma)$, with a sample complexity of $\\mathcal{\\tilde{O}}(\\epsilon^{-2.5})$  in the case of both Constrained Actor Critic (C-AC)  and Constrained Natural Actor Critic (C-NAC) algorithms. We also show the results of experiments on three different Safety-Gym environments.",
        "_bibtex": "@inproceedings{\npanda2024finitetime,\ntitle={Finite-Time Analysis of Three-Timescale Constrained Actor-Critic and Constrained Natural Actor-Critic Algorithms.},\nauthor={Prashansa Panda and Shalabh Bhatnagar},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=k5Yoa8A0DF}\n}"
    },
    {
        "title": "Quantifying Local Model Validity using Active Learning",
        "authorids": [
            "~Sven_L\u00e4mmle1",
            "~Can_Bogoclu1",
            "~Robert_Vosshall1",
            "~Anselm_Haselhoff1",
            "~Dirk_Roos1"
        ],
        "keywords": [
            "Gaussian Process",
            "Active Learning",
            "Reliability",
            "Validation"
        ],
        "abstract": "Real-world applications of machine learning models are often subject to legal or policy-based regulations. Some of these regulations require ensuring the validity of the model, i.e., the approximation error being smaller than a threshold. A global metric is generally too insensitive to determine the validity of a specific prediction, whereas evaluating local validity is costly since it requires gathering additional data. We propose learning the model error to acquire a local validity estimate while reducing the amount of required data through active learning. Using model validation benchmarks, we provide empirical evidence that the proposed method can lead to an error model with sufficient discriminative properties using a relatively small amount of data. Furthermore, an increased sensitivity to local changes of the validity bounds compared to alternative approaches is demonstrated.",
        "_bibtex": "@inproceedings{\nl{\\\"a}mmle2024quantifying,\ntitle={Quantifying Local Model Validity using Active Learning},\nauthor={Sven L{\\\"a}mmle and Can Bogoclu and Robert Vosshall and Anselm Haselhoff and Dirk Roos},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=muDcwOKf50}\n}"
    },
    {
        "title": "Performative Reinforcement Learning in Gradually Shifting Environments",
        "authorids": [
            "~Ben_Rank1",
            "~Stelios_Triantafyllou1",
            "~Debmalya_Mandal2",
            "~Goran_Radanovic1"
        ],
        "keywords": [
            "reinforcement learning",
            "performative prediction",
            "convex optimization"
        ],
        "abstract": "When Reinforcement Learning (RL) agents are deployed in practice, they might impact their environment and change its dynamics. We propose a new framework to model this phenomenon, where the current environment depends on the deployed policy as well as its previous dynamics. This is a generalization of Performative RL (PRL) [Mandal et al., 2023]. Unlike PRL, our framework allows to model scenarios where the environment gradually adjusts to a deployed policy. We adapt two algorithms from the performative prediction literature to our setting and propose a novel algorithm called Mixed Delayed Repeated Retraining (MDRR). We provide conditions under which these algorithms converge and compare them using three metrics: number of retrainings, approximation guarantee, and number of samples per deployment. MDRR is the first algorithm in this setting which combines samples from multiple deployments in its training. This makes MDRR particularly suitable for scenarios where the environment's response strongly depends on its previous dynamics, which are common in practice. We experimentally compare the algorithms using a simulation-based testbed and our results show that MDRR converges significantly faster than previous approaches.",
        "_bibtex": "@inproceedings{\nrank2024performative,\ntitle={Performative Reinforcement Learning in Gradually Shifting Environments},\nauthor={Ben Rank and Stelios Triantafyllou and Debmalya Mandal and Goran Radanovic},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=YF3rivSYVk}\n}"
    },
    {
        "title": "Quantifying Representation Reliability in Self-Supervised Learning Models",
        "authorids": [
            "~Young-Jin_Park1",
            "~Hao_Wang22",
            "~Shervin_Ardeshir2",
            "~Navid_Azizan1"
        ],
        "keywords": [
            "Uncertainty Quantification",
            "Representation Reliability",
            "Self-Supervised Learning"
        ],
        "abstract": "Self-supervised learning models extract general-purpose representations from data. Quantifying the reliability of these representations is crucial, as many downstream models rely on them as input for their own tasks. To this end, we introduce a formal definition of _representation reliability_: the representation for a given test point is considered to be reliable if the downstream models built on top of that representation can consistently generate accurate predictions for that test point. However, accessing downstream data to quantify the representation reliability is often infeasible or restricted due to privacy concerns. We propose an ensemble-based method for estimating the representation reliability without knowing the downstream tasks a priori. Our method is based on the concept of _neighborhood consistency_ across distinct pre-trained representation spaces. The key insight is to find shared neighboring points as anchors to align these representation spaces before comparing them. We demonstrate through comprehensive numerical experiments that our method effectively captures the representation reliability with a high degree of correlation, achieving robust and favorable performance compared with baseline methods.",
        "_bibtex": "@inproceedings{\npark2024quantifying,\ntitle={Quantifying Representation Reliability in Self-Supervised Learning Models},\nauthor={Young-Jin Park and Hao Wang and Shervin Ardeshir and Navid Azizan},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=YlCRKTNLeJ}\n}"
    },
    {
        "title": "Towards Scalable Bayesian Transformers: Investigating stochastic subset selection for NLP",
        "authorids": [
            "~Peter_J.T._Kampen1",
            "~Gustav_R._S._Als1",
            "~Michael_Riis_Andersen2"
        ],
        "keywords": [
            "Bayesian Machine Learning",
            "Natural Language Processing",
            "Deep Learning",
            "Transformers"
        ],
        "abstract": "Bayesian deep learning provides a framework for quantifying uncertainty. However, the scale of modern neural networks applied in Natural Language Processing (NLP) limits the usability of Bayesian methods. Subnetwork inference aims to approximate the posterior by selecting a stochastic parameter subset for inference, thereby allowing scalable posterior approximations. Determining the optimal parameter space for subnetwork inference is far from trivial. In this paper, we study partially stochastic Bayesian neural networks in the context of transformer models for NLP tasks for the Laplace approximation (LA) and Stochastic weight averaging - Gaussian (SWAG). We propose heuristics for selecting which layers to include in the stochastic subset. We show that norm-based selection is promising for small subsets, and random selection is superior for larger subsets. Moreover, we propose Sparse-KFAC (S-KFAC), an extension of KFAC LA, which selects dense stochastic substructures of linear layers based on parameter magnitudes. S-KFAC retains performance while requiring substantially fewer stochastic parameters and, therefore, drastically limits memory footprint.",
        "_bibtex": "@inproceedings{\nkampen2024towards,\ntitle={Towards Scalable Bayesian Transformers: Investigating stochastic subset selection for {NLP}},\nauthor={Peter J.T. Kampen and Gustav R. S. Als and Michael Riis Andersen},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=ba3McobvmG}\n}"
    },
    {
        "title": "A Global Markov Property for Solutions of Stochastic Difference Equations and the corresponding Full Time Graphs",
        "authorids": [
            "~Tom_Hochsprung1",
            "~Jakob_Runge2",
            "~Andreas_Gerhardus1"
        ],
        "keywords": [
            "causal inference",
            "time series analysis",
            "structural causal models",
            "structural equation models",
            "markov property"
        ],
        "abstract": "Structural Causal Models (SCMs) are an important tool in causal inference. They induce a graph and if the graph is acyclic, a unique observational distribution. A standard result states that in this acyclic case, the induced observational distribution satisfies a d-separation global Markov property relative to the induced graph.\n  Time series can also be modelled like SCMs: One just interprets the stochastic difference equations that a time series solves as structural equations. However, technical problems arise when time series \"start\" at minus infinity. In particular, a d-separation global Markov property for time series and the corresponding infinite graphs, the so-called full time graphs, has thus far only been shown for stable vector autoregressive processes with independent finite-second-moment noise. \n  In this paper, we prove a much more general version of this Markov property. We discuss our assumptions and study violations of them. \n  Doing so hints at several pitfalls at the intersection of time series analysis and causal inference.\n  Moreover, we introduce a new projection procedure for these infinite graphs which might be of independent interest.",
        "_bibtex": "@inproceedings{\nhochsprung2024a,\ntitle={A Global Markov Property for Solutions of Stochastic Difference Equations and the corresponding Full Time Graphs},\nauthor={Tom Hochsprung and Jakob Runge and Andreas Gerhardus},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=o5pOj2IDYB}\n}"
    },
    {
        "title": "Cold-start Recommendation by Personalized Embedding Region Elicitation",
        "authorids": [
            "~Hieu_Trung_Nguyen2",
            "~Duy_Nguyen2",
            "~Khoa_D_Doan1",
            "~Viet_Anh_Nguyen2"
        ],
        "keywords": [
            "Cold-Start Recommender System",
            "Preference Elicitation"
        ],
        "abstract": "Rating elicitation is a success element for recommender systems to perform well at cold-starting, in which the systems need to recommend items to a newly arrived user with no prior knowledge about the user's preference. Existing elicitation methods employ a fixed set of items to learn the user's preference and then infer the users' preferences on the remaining items. Using a fixed seed set can limit the performance of the recommendation system since the seed set is unlikely optimal for all new users with potentially diverse preferences. This paper addresses this challenge using a 2-phase, personalized elicitation scheme. First, the elicitation scheme asks users to rate a small set of popular items in a ``burn-in'' phase. Second, it sequentially asks the user to rate adaptive items to refine the preference and the user's representation. Throughout the process, the system represents the user's embedding value not by a point estimate but by a region estimate. The value of information obtained by asking the user's rating on an item is quantified by the distance from the region center embedding space that contains with high confidence the true embedding value of the user. Finally, the recommendations are successively generated by considering the preference region of the user. We show that each subproblem in the elicitation scheme can be efficiently implemented. Further, we empirically demonstrate the effectiveness of the proposed method against existing rating-elicitation methods on several prominent datasets.",
        "_bibtex": "@inproceedings{\nnguyen2024coldstart,\ntitle={Cold-start Recommendation by Personalized Embedding Region Elicitation},\nauthor={Hieu Trung Nguyen and Duy Nguyen and Khoa D Doan and Viet Anh Nguyen},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=ciOkU5YpvU}\n}"
    },
    {
        "title": "Approximate Kernel Density Estimation under Metric-based Local Differential Privacy",
        "authorids": [
            "~Yi_Zhou30",
            "~Yanhao_Wang1",
            "~Long_Teng2",
            "~Qiang_Huang3",
            "~Cen_Chen1"
        ],
        "keywords": [
            "kernel density estimation",
            "local differential privacy",
            "locality-sensitive hashing"
        ],
        "abstract": "Kernel Density Estimation (KDE) is a fundamental problem with broad machine learning applications. In this paper, we investigate the KDE problem under Local Differential Privacy (LDP), a setting in which users privatize data on their own devices before sending them to an untrusted server for analytics. To strike a balance between ensuring local privacy and preserving high-utility KDE results, we adopt a relaxed definition of LDP based on metrics (mLDP), which is suitable when data points are represented in a metric space and can be more distinguishable as their distances increase. To the best of our knowledge, approximate KDE under mLDP has not been explored in the existing literature. We propose the mLDP-KDE framework, which augments a locality-sensitive hashing-based sketch method to provide mLDP and answer any KDE query unbiasedly within an additive error with high probability in sublinear time and space. Extensive experimental results demonstrate that the mLDP-KDE framework outperforms several existing KDE methods under LDP and mLDP by achieving significantly better trade-offs between privacy and utility, with particularly remarkable advantages on large, high-dimensional data.",
        "_bibtex": "@inproceedings{\nzhou2024approximate,\ntitle={Approximate Kernel Density Estimation under Metric-based Local Differential Privacy},\nauthor={Yi Zhou and Yanhao Wang and Long Teng and Qiang Huang and Cen Chen},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=FFsbpo2fwF}\n}"
    },
    {
        "title": "Memorization Capacity for Additive Fine-Tuning with Small ReLUs",
        "authorids": [
            "~Jy-yong_Sohn1",
            "~Dohyun_Kwon1",
            "~Seoyeon_An1",
            "~Kangwook_Lee1"
        ],
        "keywords": [
            "Fine-Tuning",
            "Memorization Capacity"
        ],
        "abstract": "Fine-tuning large pre-trained models is a common practice in machine learning applications, yet its mathematical analysis remains largely unexplored. In this paper, we study fine-tuning through the lens of memorization capacity. Our new measure, the Fine-Tuning Capacity (FTC), is defined as the maximum number of samples a neural network can fine-tune, or equivalently, as the minimum number of neurons ($m$) needed to arbitrarily change $N$ labels among $K$ samples considered in the fine-tuning process. In essence, FTC extends the memorization capacity concept to the fine-tuning scenario. We analyze FTC for the additive fine-tuning scenario where the fine-tuned network is defined as the summation of the frozen pre-trained network $f$ and a neural network $g$ (with $m$ neurons) designed for fine-tuning. When $g$ is a ReLU network with either 2 or 3 layers, we obtain tight upper and lower bounds on FTC; we show that $N$ samples can be fine-tuned with $m=\\Theta(N)$ neurons for 2-layer networks, and with $m=\\Theta(\\sqrt{N})$ neurons for 3-layer networks, no matter how large $K$ is. Our results recover the known memorization capacity results when $N = K$ as a special case.",
        "_bibtex": "@inproceedings{\nsohn2024memorization,\ntitle={Memorization Capacity for Additive Fine-Tuning with Small Re{LU}s},\nauthor={Jy-yong Sohn and Dohyun Kwon and Seoyeon An and Kangwook Lee},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=x1Y3x79J42}\n}"
    },
    {
        "title": "Revisiting Convergence of AdaGrad with Relaxed Assumptions",
        "authorids": [
            "~Yusu_Hong1",
            "~Junhong_Lin1"
        ],
        "keywords": [
            "convergence theory",
            "high probability",
            "adaptive gradient methods"
        ],
        "abstract": "In this study, we revisit the convergence of AdaGrad with momentum (covering AdaGrad as a special case) on non-convex smooth optimization problems. We consider a general noise model where the noise magnitude is controlled by the function value gap together with the gradient magnitude. This model encompasses a broad range of noises including bounded noise, sub-Gaussian noise, affine variance noise and the expected smoothness, and it has been shown to be more realistic in many practical applications. Our analysis yields a probabilistic convergence rate which, under the general noise, could reach at $\\tilde{\\mathcal{O}}(1/\\sqrt{T})$. This rate does not rely on prior knowledge of problem-parameters and could accelerate to $\\tilde{\\mathcal{O}}(1/T)$ where $T$ denotes the total number iterations, when the noise parameters related to the function value gap and noise level are sufficiently small. The convergence rate thus matches the lower rate for stochastic first-order methods over non-convex smooth landscape up to logarithm terms [Arjevani et al., 2023]. We further derive a convergence bound for AdaGrad with momentum, considering the generalized smoothness where the local smoothness is controlled by a first-order function of the gradient norm.",
        "_bibtex": "@inproceedings{\nhong2024revisiting,\ntitle={Revisiting Convergence of AdaGrad with Relaxed Assumptions},\nauthor={Yusu Hong and Junhong Lin},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=AlYO5cq1fG}\n}"
    },
    {
        "title": "Random Linear Projections Loss for Hyperplane-Based Optimization in Neural Networks",
        "authorids": [
            "~Shyam_Venkatasubramanian1",
            "~Ahmed_Aloui1",
            "~Vahid_Tarokh1"
        ],
        "keywords": [
            "loss functions",
            "deep learning",
            "optimization"
        ],
        "abstract": "Advancing loss function design is pivotal for optimizing neural network training and performance. This work introduces Random Linear Projections (RLP) loss, a novel approach that enhances training efficiency by leveraging geometric relationships within the data. Distinct from traditional loss functions that target minimizing pointwise errors, RLP loss operates by minimizing the distance between sets of hyperplanes connecting fixed-size subsets of feature-prediction pairs and feature-label pairs. Our empirical evaluations, conducted across benchmark datasets and synthetic examples, demonstrate that neural networks trained with RLP loss outperform those trained with traditional loss functions, achieving improved performance with fewer data samples, and exhibiting greater robustness to additive noise. We provide theoretical analysis supporting our empirical findings.",
        "_bibtex": "@inproceedings{\nvenkatasubramanian2024random,\ntitle={Random Linear Projections Loss for Hyperplane-Based Optimization in Neural Networks},\nauthor={Shyam Venkatasubramanian and Ahmed Aloui and Vahid Tarokh},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=ViuCERY1Pn}\n}"
    },
    {
        "title": "Quantum Kernelized Bandits",
        "authorids": [
            "~Yasunari_Hikima1",
            "~Kazunori.Murao1",
            "~Sho_Takemori1",
            "~Yuhei_Umeda1"
        ],
        "keywords": [
            "bandits",
            "quantum",
            "Bayesian optimization",
            "quantum machine learning"
        ],
        "abstract": "We consider the quantum kernelized bandit problem, where the player observes information of rewards through quantum circuits termed the quantum reward oracle, and the mean reward function belongs to a reproducing kernel Hilbert space (RKHS). We propose a UCB-type algorithm that utilizes the quantum Monte Carlo (QMC) method and provide regret bounds in terms of the decay rate of eigenvalues of the Mercer operator of the kernel. Our algorithm achieves $\\widetilde{O}\\left( T^{\\frac{3}{1 + \\beta_p}} \\log\\left(\\frac{1}{\\delta} \\right)\\right)$ and $\\widetilde{O} \\left( \\log^{3(1 + \\beta_e^{-1})/2} (T) \\log\\left(\\frac{1 }{\\delta} \\right) \\right)$ cumulative regret bounds with probability at least $1-\\delta$ if the kernel has a $\\beta_p$-polynomial eigendecay and $\\beta_e$-exponential eigendecay, respectively. In particular, in the case of the exponential eigendecay, our regret bounds exponentially improve that of classical algorithms. Moreover, our results indicate that our regret bound is better than the lower bound in the classical kernelized bandit problem if the rate of decay is sufficiently fast.",
        "_bibtex": "@inproceedings{\nhikima2024quantum,\ntitle={Quantum Kernelized Bandits},\nauthor={Yasunari Hikima and Kazunori.Murao and Sho Takemori and Yuhei Umeda},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=3GtCwa9nky}\n}"
    },
    {
        "title": "EntProp: High Entropy Propagation for Improving Accuracy and Robustness",
        "authorids": [
            "~Shohei_Enomoto1"
        ],
        "keywords": [
            "Entropy",
            "Robustness",
            "Auxiliary Batch Normalization"
        ],
        "abstract": "Deep neural networks (DNNs) struggle to generalize to out-of-distribution domains that are different from those in training despite their impressive performance.\nIn practical applications, it is important for DNNs to have both high standard accuracy and robustness against out-of-distribution domains.\nOne technique that achieves both of these improvements is disentangled learning with mixture distribution via auxiliary batch normalization layers (ABNs).\nThis technique treats clean and transformed samples as different domains, allowing a DNN to learn better features from mixed domains.\nHowever, if we distinguish the domains of the samples based on entropy, we find that some transformed samples are drawn from the same domain as clean samples, and these samples are not completely different domains.\nTo generate samples drawn from a completely different domain than clean samples, we hypothesize that transforming clean high-entropy samples to further increase the entropy generates out-of-distribution samples that are much further away from the in-distribution domain.\nOn the basis of the hypothesis, we propose high entropy propagation~(EntProp), which feeds high-entropy samples to the network that uses ABNs.\nWe introduce two techniques, data augmentation and free adversarial training, that increase entropy and bring the sample further away from the in-distribution domain.\nThese techniques do not require additional training costs.\nOur experimental results show that EntProp achieves higher standard accuracy and robustness with a lower training cost than the baseline methods.\nIn particular, EntProp is highly effective at training on small datasets.",
        "_bibtex": "@inproceedings{\nenomoto2024entprop,\ntitle={EntProp: High Entropy Propagation for Improving Accuracy and Robustness},\nauthor={Shohei Enomoto},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=kp27EuaR22}\n}"
    },
    {
        "title": "Bias-aware Boolean Matrix Factorization Using Disentangled Representation Learning",
        "authorids": [
            "~Xiao_Wang26",
            "~Jia_Wang12",
            "~Tong_Zhao2",
            "~Yijie_Wang2",
            "~Nan_Zhang17",
            "~Yong_Zang1",
            "~Sha_Cao1",
            "~Chi_Zhang18"
        ],
        "keywords": [
            "Boolean matrix factorization",
            "disentangled representation learning",
            "bias-aware",
            "pattern detection"
        ],
        "abstract": "Boolean matrix factorization (BMF) has been widely utilized in fields such as recommendation systems, graph learning, text mining, and -omics data analysis. Traditional BMF methods decompose a binary matrix into the Boolean product of two lower-rank Boolean matrices plus homoscedastic random errors. However, real-world binary data typically involves biases arising from heterogeneous row- and column-wise signal distributions. Such biases can lead to suboptimal fitting and unexplainable predictions if not accounted for. In this study, we reconceptualize the binary data generation as the Boolean sum of three components: a binary pattern matrix, a background bias matrix influenced by heterogeneous row or column distributions, and random flipping errors. We introduce a novel Disentangled Representation Learning for Binary matrices (DRLB) method, which employs a dual auto-encoder network to reveal the true patterns. DRLB can be seamlessly integrated with existing BMF techniques to facilitate bias-aware BMF. Our experiments with both synthetic and real-world datasets show that DRLB significantly enhances the precision of traditional BMF methods while offering high scalability. Moreover, the bias matrix detected by DRLB accurately reflects the inherent biases in synthetic data, and the patterns identified in the bias-corrected real-world data exhibit enhanced interpretability.",
        "_bibtex": "@inproceedings{\nwang2024biasaware,\ntitle={Bias-aware Boolean Matrix Factorization Using Disentangled Representation Learning},\nauthor={Xiao Wang and Jia Wang and Tong Zhao and Yijie Wang and Nan Zhang and Yong Zang and Sha Cao and Chi Zhang},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=2ynAD7mpmR}\n}"
    },
    {
        "title": "Fair Active Learning in Low-Data Regimes",
        "authorids": [
            "~Romain_Camilleri1",
            "~Andrew_Wagenmaker1",
            "~Kevin_Jamieson1",
            "~Lalit_K_Jain1",
            "~Jamie_Heather_Morgenstern1"
        ],
        "keywords": [
            "Fairness",
            "Active Learning"
        ],
        "abstract": "In critical machine learning applications, ensuring fairness is essential to avoid perpetuating social inequities. In this work, we address the challenges of reducing bias and improving accuracy in data-scarce environments, where the cost of collecting labeled data prohibits the use of large, labeled datasets. In such settings, active learning promises to maximize marginal accuracy gains of small amounts of labeled data. However, existing applications of active learning for fairness fail to deliver on this, typically requiring large labeled datasets, or failing to ensure the desired fairness tolerance is met on the population distribution.\n\nTo address such limitations, we introduce an innovative active learning framework that combines an exploration procedure inspired by posterior sampling with a fair classification subroutine. We demonstrate that this framework performs effectively in very data-scarce regimes, maximizing accuracy while satisfying fairness constraints with high probability. We evaluate our proposed approach using well-established real-world benchmark datasets and compare it against state-of-the-art methods, demonstrating its effectiveness in producing fair models, and improvement over existing methods.",
        "_bibtex": "@inproceedings{\ncamilleri2024fair,\ntitle={Fair Active Learning in Low-Data Regimes},\nauthor={Romain Camilleri and Andrew Wagenmaker and Kevin Jamieson and Lalit K Jain and Jamie Heather Morgenstern},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=goREZw6mdK}\n}"
    },
    {
        "title": "GCVR: Reconstruction from Cross-View Enable Sufficient and Robust Graph Contrastive Learning",
        "authorids": [
            "~Qianlong_Wen1",
            "~Zhongyu_Ouyang1",
            "~Chunhui_Zhang1",
            "~Yiyue_Qian2",
            "~Chuxu_Zhang2",
            "~Yanfang_Ye1"
        ],
        "keywords": [
            "Graph Neural Network",
            "Graph Self-supervised Learning"
        ],
        "abstract": "Among the existing self-supervised learning (SSL) methods for graphs, graph contrastive learning (GCL) frameworks usually automatically generate supervision by transforming the same graph into different views through graph augmentation operations. \nThe computation-efficient augmentation techniques enable the prevalent usage of GCL to alleviate the supervision shortage issue.  \nDespite the remarkable performance of those GCL methods, the InfoMax principle used to guide the optimization of GCL has been proven to be insufficient to avoid redundant information without losing important features.  \nIn light of this, we introduce the Graph Contrastive Learning with Cross-View Reconstruction (GCVR), aiming to learn robust and sufficient representation from graph data. Specifically, GCVR introduces a cross-view reconstruction mechanism based on conventional graph contrastive learning to elicit those essential features from raw graphs. Besides, we introduce an extra adversarial view perturbed from the original view in the contrastive loss to pursue the intactness of the graph semantics and strengthen the representation robustness. We empirically demonstrate that our proposed model outperforms the state-of-the-art baselines on graph classification tasks over multiple benchmark datasets.",
        "_bibtex": "@inproceedings{\nwen2024gcvr,\ntitle={{GCVR}: Reconstruction from Cross-View Enable Sufficient and Robust Graph Contrastive Learning},\nauthor={Qianlong Wen and Zhongyu Ouyang and Chunhui Zhang and Yiyue Qian and Chuxu Zhang and Yanfang Ye},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=DA1zd1Qdon}\n}"
    },
    {
        "title": "Decentralized Two-Sided Bandit Learning in Matching Market",
        "authorids": [
            "~YiRui_Zhang1",
            "~Zhixuan_Fang1"
        ],
        "keywords": [
            "Multi-agent Reinforcement Learning",
            "Bandits",
            "Two-sided Matching"
        ],
        "abstract": "Two-sided matching under uncertainty has recently drawn much attention due to its wide applications. \n Existing works in matching bandits mainly focus on the one-sided learning setting and design algorithms with the objective of converging to stable matching with low regret.  In this paper, we consider the more general two-sided learning setting, i.e. participants on both sides have to learn their preferences over the other side through repeated interactions. \n Inspired by the classical result that the optimal matching for the proposing side can be obtained using the Gale-Shapley algorithm, our inquiry stems from the curiosity about whether this result still holds in a two-sided learning setting. To handle this question, we formally introduce the two-sided learning setting, addressing strategies for both the arm and player sides without restrictive assumptions such as special preference structure and observation of winning players. \n Our results not only provide a positive answer to our inquiry but also offer a near-optimal upper bound, achieving $O(\\log T)$ regret.",
        "_bibtex": "@inproceedings{\nzhang2024decentralized,\ntitle={Decentralized Two-Sided Bandit Learning in Matching Market},\nauthor={YiRui Zhang and Zhixuan Fang},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=RUazoBVXIN}\n}"
    },
    {
        "title": "Identifying Homogeneous and Interpretable Groups for Conformal Prediction",
        "authorids": [
            "~Natalia_Martinez1",
            "~Dhaval_C_Patel1",
            "~Chandra_Reddy1",
            "~Giridhar_Ganapavarapu1",
            "~Roman_Vaculin1",
            "~Jayant_Kalagnanam1"
        ],
        "keywords": [
            "Uncertainty Quantification; Group Discovery; Trustworthy AI; Conformal Prediction"
        ],
        "abstract": "Conformal prediction methods are a tool for uncertainty quantification of a model's prediction, providing a model-agnostic and distribution-free  statistical wrapper that generates prediction intervals/sets for a given model with finite sample generalization guarantees. However, these guarantees hold only on average, or conditioned on the output values of the predictor or on a set of predefined groups, which a-priori may not relate to the prediction task at hand. We propose a method to learn a generalizable partition function of the input space (or representation mapping) into interpretable groups of varying sizes where the non-conformity scores - a measure of discrepancy between prediction and target - are as homogeneous as possible when conditioned to the group. The learned partition can be integrated with any of the group conditional conformal approaches to produce conformal sets with group conditional guarantees on the discovered regions. Since these learned groups are expressed as strictly a function of the input, they can be used for downstream tasks such as data collection or model selection. We show the effectiveness of our method in reducing worst case group coverage outcomes in a variety of datasets.",
        "_bibtex": "@inproceedings{\nmartinez2024identifying,\ntitle={Identifying Homogeneous and Interpretable Groups for Conformal Prediction},\nauthor={Natalia Martinez and Dhaval C Patel and Chandra Reddy and Giridhar Ganapavarapu and Roman Vaculin and Jayant Kalagnanam},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=5qW3Ojxt9m}\n}"
    },
    {
        "title": "Pure Exploration in Asynchronous Federated Bandits",
        "authorids": [
            "~Zichen_Wang8",
            "~Chuanhao_Li1",
            "~chenyu_song1",
            "~Lianghui_Wang1",
            "~Quanquan_Gu1",
            "~Huazheng_Wang1"
        ],
        "keywords": [
            "Bandits; Federated learning; Pure exploration"
        ],
        "abstract": "We study the federated pure exploration problem of multi-armed bandits and linear bandits, where $M$ agents cooperatively identify the best arm via communicating with the central server. To enhance the robustness against latency and unavailability of agents that are common in practice, we propose the first federated asynchronous multi-armed bandit and linear bandit algorithms for pure exploration with fixed confidence. Our theoretical analysis shows the proposed algorithms achieve near-optimal sample complexities and efficient communication costs in a fully asynchronous environment. Moreover, experimental results based on synthetic and real-world data empirically elucidate the effectiveness and communication cost-efficiency of the proposed algorithms.",
        "_bibtex": "@inproceedings{\nwang2024pure,\ntitle={Pure Exploration in Asynchronous Federated Bandits},\nauthor={Zichen Wang and Chuanhao Li and chenyu song and Lianghui Wang and Quanquan Gu and Huazheng Wang},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=VqgArMVbZf}\n}"
    },
    {
        "title": "To smooth a cloud or to pin it down: Expressiveness guarantees and insights on score matching in denoising diffusion models",
        "authorids": [
            "~Teodora_Reu1",
            "~Francisco_Vargas1",
            "~Anna_Kerekes1",
            "~Michael_M._Bronstein1"
        ],
        "keywords": [
            "Denoising Diffusion Models",
            "Score Matching",
            "Timereveral",
            "Expressivness",
            "MCMC",
            "Sampling",
            "Generative Modelling",
            "Machine Learning Theory"
        ],
        "abstract": "Denoising diffusion models are a class of generative models that have recently achieved state-of-the-art results across many domains. Gradual noise is added to the data using a diffusion process, which transforms the data distribution into a Gaussian. Samples from the generative model are then obtained by simulating an approximation of the time reversal of this diffusion initialized by Gaussian samples. Recent research has explored the sampling error achieved by diffusion models under the assumption of an absolute error $\\epsilon$ achieved via a neural approximation of the score. To the best of our knowledge, no work formally quantifies the error of such neural approximation to the score. In this paper, we close the gap and present quantitative error bounds for approximating the score of denoising diffusion models using neural networks leveraging ideas from stochastic control. Finally, through simulation, we explore some of the insights that arise from our results confirming that diffusion models based on the Ornstein-Uhlenbeck (OU) process require fewer parameters to better approximate the score than those based on the F\\\"{o}lmer drift / Pinned Brownian Motion.",
        "_bibtex": "@inproceedings{\nreu2024to,\ntitle={To smooth a cloud or to pin it down: Expressiveness guarantees and insights on score matching in denoising diffusion models},\nauthor={Teodora Reu and Francisco Vargas and Anna Kerekes and Michael M. Bronstein},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=BV2STKHKE6}\n}"
    },
    {
        "title": "Discrete Probabilistic Inference as Control in Multi-path Environments",
        "authorids": [
            "~Tristan_Deleu1",
            "~Padideh_Nouri1",
            "~Nikolay_Malkin1",
            "~Doina_Precup1",
            "~Yoshua_Bengio1"
        ],
        "keywords": [
            "maximum entropy reinforcement learning",
            "generative flow networks",
            "discrete probabilistic inference"
        ],
        "abstract": "We consider the problem of sampling from a discrete and structured distribution as a sequential decision problem, where the objective is to find a stochastic policy such that objects are sampled at the end of this sequential process proportionally to some predefined reward. While we could use maximum entropy Reinforcement Learning (MaxEnt RL) to solve this problem for some distributions, it has been shown that in general, the distribution over states induced by the optimal policy may be biased in cases where there are multiple ways to generate the same object. To address this issue, Generative Flow Networks (GFlowNets) learn a stochastic policy that samples objects proportionally to their reward by approximately enforcing a conservation of flows across the whole Markov Decision Process (MDP). In this paper, we extend recent methods correcting the reward in order to guarantee that the marginal distribution induced by the optimal MaxEnt RL policy is proportional to the original reward, regardless of the structure of the underlying MDP. We also prove that some flow-matching objectives found in the GFlowNet literature are in fact equivalent to well-established MaxEnt RL algorithms with a corrected reward. Finally, we study empirically the performance of multiple MaxEnt RL and GFlowNet algorithms on multiple problems involving sampling from discrete distributions.",
        "_bibtex": "@inproceedings{\ndeleu2024discrete,\ntitle={Discrete Probabilistic Inference as Control in Multi-path Environments},\nauthor={Tristan Deleu and Padideh Nouri and Nikolay Malkin and Doina Precup and Yoshua Bengio},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=3C69sU1YkK}\n}"
    },
    {
        "title": "Approximation Algorithms for Observer Aware MDPs",
        "authorids": [
            "~Shuwa_Miura1",
            "~Olivier_Buffet1",
            "~Shlomo_Zilberstein1"
        ],
        "keywords": [
            "MDP",
            "OADMP",
            "legibility",
            "theory of mind",
            "HRI"
        ],
        "abstract": "We present approximation algorithms for Observer-Aware Markov Decision Processes (OAMDPs). OAMDPs model sequential decision-making problems in which rewards depend on the beliefs of an observer about the goals, intentions, or capabilities of the observed agent. The first proposed algorithm is a grid-based value iteration (Grid-VI), which discretizes the observer's belief into regular grids. Based on the same discretization, the second proposed algorithm is a variant of Real-Time Dynamic Programming (RTDP) called Grid-RTDP. Unlike Grid-Vi, Grid-RTDP focuses its updates on promising states using heuristic estimates. \nWe provide theoretical guarantees of the proposed algorithms and demonstrate that Grid-RTDP has a good anytime performance comparable to the existing approach without performance guarantees.",
        "_bibtex": "@inproceedings{\nmiura2024approximation,\ntitle={Approximation Algorithms for Observer Aware {MDP}s},\nauthor={Shuwa Miura and Olivier Buffet and Shlomo Zilberstein},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=UXsERjAZy8}\n}"
    },
    {
        "title": "Multi-fidelity Bayesian Optimization with Multiple Information Sources of Input-dependent Fidelity",
        "authorids": [
            "~Mingzhou_Fan1",
            "~Byung-Jun_Yoon1",
            "~Edward_Dougherty1",
            "~Francis_Alexander1",
            "~Nathan_Urban2",
            "~Raymundo_Arroyave1",
            "~Xiaoning_Qian2"
        ],
        "keywords": [
            "Bayesian Optimization",
            "Multi-Fidelity BO"
        ],
        "abstract": "By querying approximate surrogate models of different fidelity as available information sources, Multi-Fidelity Bayesian Optimization (MFBO) aims at optimizing unknown functions that are costly if not infeasible to evaluate. Existing MFBO methods often assume that approximate surrogates have consistently high/low fidelity across the input domain. However, approximate evaluations from the same surrogate can have different fidelity at different input regions due to data availability and model constraints, especially when considering machine learning surrogates. In this work, we investigate MFBO when multi-fidelity approximations have input-dependent fidelity. By explicitly capturing input dependency for multi-fidelity queries in Gaussian Process (GP), our new input-dependent MFBO~(iMFBO) with learnable noise models better captures the fidelity of each information source in an intuitive way.\nWe further design a new acquisition function for iMFBO and prove that the queries selected by iMFBO have higher quality than those by naive MFBO methods, with the derived sub-linear regret bound. Experiments on both synthetic and real-world data demonstrate its superior empirical performance.",
        "_bibtex": "@inproceedings{\nfan2024multifidelity,\ntitle={Multi-fidelity Bayesian Optimization with Multiple Information Sources of Input-dependent Fidelity},\nauthor={Mingzhou Fan and Byung-Jun Yoon and Edward Dougherty and Francis Alexander and Nathan Urban and Raymundo Arroyave and Xiaoning Qian},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=ZZBxbUMrP6}\n}"
    },
    {
        "title": "Characterizing Data Point Vulnerability as Average-Case Robustness",
        "authorids": [
            "~Tessa_Han1",
            "~Suraj_Srinivas1",
            "~Himabindu_Lakkaraju1"
        ],
        "keywords": [
            "Trustworthy AI",
            "Deep Learning",
            "Robustness",
            "Data-centric Machine Learning"
        ],
        "abstract": "Studying the robustness of machine learning models is important to ensure consistent model behaviour across real-world settings. To this end, adversarial robustness is a standard framework, which views robustness of predictions through a binary lens: either a worst-case adversarial perturbation exists in the local region around an input, or it does not. However, this binary perspective does not account for the degrees of vulnerability, as data points with a larger number of misclassified examples in their neighborhoods are more vulnerable. In this work, we consider a complementary framework for robustness, called average-case robustness, which measures the fraction of points in a local region that provides consistent predictions. However, computing this quantity is hard, as standard Monte Carlo approaches are inefficient especially for high-dimensional inputs. In this work, we propose the first analytical estimators for average-case robustness for multi-class classifiers. We show empirically that our estimators are accurate and efficient for standard deep learning models and demonstrate their usefulness for identifying vulnerable data points, and well as quantifying robustness bias of models. Overall, our tools provide a complementary view to robustness, improving our ability to characterize model behaviour.",
        "_bibtex": "@inproceedings{\nhan2024characterizing,\ntitle={Characterizing Data Point Vulnerability as Average-Case Robustness},\nauthor={Tessa Han and Suraj Srinivas and Himabindu Lakkaraju},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=iLZUHWbDkq}\n}"
    },
    {
        "title": "Conditional Bayesian Quadrature",
        "authorids": [
            "~Zonghao_Chen1",
            "~Masha_Naslidnyk1",
            "~Arthur_Gretton1",
            "~Francois-Xavier_Briol1"
        ],
        "keywords": [
            "Bayesian Quadrature",
            "Probabilistic Numerics",
            "Gaussian Process",
            "Conditional Expectation"
        ],
        "abstract": "We propose a novel approach for estimating conditional or parametric expectations in the setting where obtaining samples or evaluating integrands is costly. Through the framework of probabilistic numerical methods (such as Bayesian quadrature), our novel approach allows to  incorporates prior information about the integrands especially the prior smoothness knowledge about the integrands and the conditional expectation. As a result, our approach provides a way of quantifying uncertainty and leads to a fast convergence rate, which is confirmed both theoretically and empirically on challenging tasks in Bayesian sensitivity analysis, computational finance and decision making under uncertainty.",
        "_bibtex": "@inproceedings{\nchen2024conditional,\ntitle={Conditional Bayesian Quadrature},\nauthor={Zonghao Chen and Masha Naslidnyk and Arthur Gretton and Francois-Xavier Briol},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=x1YfB33Hj6}\n}"
    },
    {
        "title": "Model-Free Robust Reinforcement Learning with Sample Complexity Analysis",
        "authorids": [
            "~Yudan_Wang1",
            "~Shaofeng_Zou1",
            "~Yue_Wang16"
        ],
        "keywords": [
            "robust reinforcement learning",
            "sample complexity",
            "distributionally robust optimization"
        ],
        "abstract": "Distributionally Robust Reinforcement Learning (DR-RL) aims to derive a policy optimizing the worst-case performance within a predefined uncertainty set. Despite extensive research, previous DR-RL algorithms have predominantly favored model-based approaches, with limited availability of model-free methods offering convergence guarantees or sample complexities. This paper proposes a model-free DR-RL algorithm leveraging the Multi-level Monte Carlo (MLMC) technique to close such a gap. Our innovative approach integrates a threshold mechanism that ensures finite sample requirements for algorithmic implementation, a significant departure from previous model-free algorithms. We adapt our algorithm to accommodate uncertainty sets defined by total variation, Chi-square divergence, and KL divergence, and provide finite sample analyses under all three cases. Remarkably, our algorithms represent the first model-free DR-RL approach featuring finite sample complexity for total variation and Chi-square divergence uncertainty sets, while also offering an improved sample complexity and broader applicability compared to existing model-free DR-RL algorithms for the KL divergence model. The complexities of our method establish the tightest results for all three uncertainty models in model-free DR-RL, underscoring the effectiveness and efficiency of our algorithm, and highlighting its potential for practical applications.",
        "_bibtex": "@inproceedings{\nwang2024modelfree,\ntitle={Model-Free Robust Reinforcement Learning with Sample Complexity Analysis},\nauthor={Yudan Wang and Shaofeng Zou and Yue Wang},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=brZRvwK58H}\n}"
    },
    {
        "title": "Neural Optimal Transport with Lagrangian Costs",
        "authorids": [
            "~Aram-Alexandre_Pooladian2",
            "~Carles_Domingo-Enrich1",
            "~Ricky_T._Q._Chen1",
            "~Brandon_Amos1"
        ],
        "keywords": [
            "neural optimal transport",
            "Lagrangian costs",
            "Riemannian geometry",
            "amortized optimization"
        ],
        "abstract": "We investigate the optimal transport problem between probability measures when the underlying cost function is understood to satisfy a least action principle, also known as a Lagrangian cost. These generalizations are useful when connecting observations from a physical system where the transport dynamics are influenced by the geometry of the system, such as obstacles (e.g., incorporating barrier functions in the Lagrangian), and allows practitioners to incorporate a priori knowledge of the underlying system such as non-Euclidean geometries (e.g., paths must be circular). Our contributions are of computational interest, where we demonstrate the ability to efficiently compute geodesics and amortize spline-based paths, which has not been done before, even in low dimensional problems. Unlike prior work, we also output the resulting Lagrangian optimal transport map without requiring an ODE solver. We demonstrate the effectiveness of our formulation on low-dimensional examples taken from  prior work. The source code to reproduce our experiments is available at https://github.com/facebookresearch/lagrangian-ot.",
        "_bibtex": "@inproceedings{\npooladian2024neural,\ntitle={Neural Optimal Transport with Lagrangian Costs},\nauthor={Aram-Alexandre Pooladian and Carles Domingo-Enrich and Ricky T. Q. Chen and Brandon Amos},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=x4paJ2sJyZ}\n}"
    },
    {
        "title": "Adjustment Identification Distance: A gadjid for Causal Structure Learning",
        "authorids": [
            "~Leonard_Henckel1",
            "~Theo_W\u00fcrtzen1",
            "~Sebastian_Weichwald1"
        ],
        "keywords": [
            "Causal Discovery",
            "Structure Learning",
            "Causality",
            "Identification"
        ],
        "abstract": "Evaluating graphs learned by causal discovery algorithms is difficult: The number of edges that differ between two graphs does not reflect how the graphs differ with respect to the identifying formulas they suggest for causal effects. We introduce a framework for developing causal distances between graphs which includes the structural intervention distance for directed acyclic graphs as a special case. We use this framework to develop improved adjustment-based distances as well as extensions to completed partially directed acyclic graphs and causal orders. We develop new reachability algorithms to compute the distances efficiently and to prove their low polynomial time complexity. In our package gadjid (open source at https://github.com/CausalDisco/gadjid), we provide implementations of our distances; they are orders of magnitude faster with proven lower time complexity than the structural intervention distance and thereby provide a success metric for causal discovery that scales to graph sizes that were previously prohibitive.",
        "_bibtex": "@inproceedings{\nhenckel2024adjustment,\ntitle={Adjustment Identification Distance: A gadjid for Causal Structure Learning},\nauthor={Leonard Henckel and Theo W{\\\"u}rtzen and Sebastian Weichwald},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=jO5UNNrjJr}\n}"
    },
    {
        "title": "Center-Based Relaxed Learning Against Membership Inference Attacks",
        "authorids": [
            "~Xingli_Fang1",
            "~Jung-Eun_Kim1"
        ],
        "keywords": [
            "Privacy",
            "Metric Learning",
            "Representation Learning",
            "Deep Learning"
        ],
        "abstract": "Membership inference attacks (MIAs) are currently considered one of the main privacy attack strategies, and their defense mechanisms have also been extensively explored. However, there is still a gap between the existing defense approaches and ideal models in both performance and deployment costs. In particular, we observed that the privacy vulnerability of the model is closely correlated with the gap between the model's data-memorizing ability and generalization ability. To address it, we propose a new architecture-agnostic training paradigm called Center-based Relaxed Learning (CRL), which is adaptive to any classification model and provides privacy preservation by sacrificing a minimal or no loss of model generalizability. We emphasize that CRL can better maintain the model's consistency between member and non-member data. Through extensive experiments on common classification datasets, we empirically show that this approach exhibits comparable performance without requiring additional model capacity or data costs.",
        "_bibtex": "@inproceedings{\nfang2024centerbased,\ntitle={Center-Based Relaxed Learning Against Membership Inference Attacks},\nauthor={Xingli Fang and Jung-Eun Kim},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=unlWrunFjg}\n}"
    },
    {
        "title": "Probabilistic reconciliation of mixed-type hierarchical time series",
        "authorids": [
            "~Lorenzo_Zambon1",
            "~Dario_Azzimonti1",
            "~Nicol\u00f2_Rubattu2",
            "~Giorgio_Corani1"
        ],
        "keywords": [
            "probabilistic forecast reconciliation",
            "hierarchical forecasting",
            "reconciliation via conditioning",
            "top-down"
        ],
        "abstract": "Hierarchical time series are collections of time series that are formed via aggregation, and thus adhere to some linear constraints.\nThe forecasts for hierarchical time series should be coherent, i.e., they should satisfy the same constraints.\nIn a probabilistic setting, forecasts are in the form of predictive distributions. \nProbabilistic reconciliation adjusts the predictive distributions, yielding a joint reconciled distribution that assigns positive probability only to coherent forecasts.\nThere are methods for the reconciliation of hierarchies containing only Gaussian or only discrete predictive distributions; instead, the reconciliation of mixed hierarchies, i.e. mixtures of discrete and continuous time series, is still an open problem. \nWe propose two different approaches to address this problem: mixed conditioning and top-down conditioning.\nWe discuss their properties and we present experiments with datasets containing up to thousands of time series.",
        "_bibtex": "@inproceedings{\nzambon2024probabilistic,\ntitle={Probabilistic reconciliation of mixed-type hierarchical time series},\nauthor={Lorenzo Zambon and Dario Azzimonti and Nicol{\\`o} Rubattu and Giorgio Corani},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=KmbmBlrQkr}\n}"
    },
    {
        "title": "Bootstrap Your Conversions: Thompson Sampling for Partially Observable Delayed Rewards",
        "authorids": [
            "~Marco_Gigli1",
            "~Fabio_Stella1"
        ],
        "keywords": [
            "stochastic bandits",
            "delayed feedback",
            "partially observable feedback",
            "Thompson sampling"
        ],
        "abstract": "This paper presents a novel approach to address contextual bandit problems with partially observable, delayed feedback by introducing an approximate Thompson sampling technique. This is a common setting, with applications ranging from online marketing to vaccine trials. Leveraging Bootstrapped Thompson sampling (BTS), we obtain an approximate posterior distribution over delay distributions and conversion probabilities, thereby extending an Expectation-Maximisation (EM) model to the Bayesian domain. Unlike prior methodologies, our approach does not overlook uncertainty on delays. Within the EM framework, we employ the Kaplan-Meier estimator to place no restriction on delay distributions. Through extensive benchmarking against state-of-the-art techniques, our approach demonstrates superior performance across the majority of tested environments, with comparable performance in the remaining cases. Furthermore, our method offers practical implementation using off-the-shelf libraries, facilitating broader adoption. Our technique lays a foundation for extending to other bandit settings, such as non-contextual bandits or action-dependent delay distributions, promising wider applicability and versatility in real-world applications.",
        "_bibtex": "@inproceedings{\ngigli2024bootstrap,\ntitle={Bootstrap Your Conversions: Thompson Sampling for Partially Observable Delayed Rewards},\nauthor={Marco Gigli and Fabio Stella},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=BhFp6cFwDq}\n}"
    },
    {
        "title": "Transductive and Inductive Outlier Detection with Robust Autoencoders",
        "authorids": [
            "~Ofir_Lindenbaum1",
            "~Yariv_Aizenbud1",
            "~Yuval_Kluger1"
        ],
        "keywords": [
            "Outlier detection",
            "Anomaly detection",
            "Robust Autoencoders"
        ],
        "abstract": "Accurate detection of outliers is crucial for the success of numerous data analysis tasks. In this context, we propose the Probabilistic Robust AutoEncoder (PRAE) that can simultaneously remove outliers during training (transductive) and learn a mapping that can be used to detect outliers in new data (inductive). We first present the Robust AutoEncoder (RAE) objective that excludes outliers while including a subset of samples (inliers) that can be effectively reconstructed using an AutoEncoder (AE). RAE minimizes the autoencoder's reconstruction error while incorporating as many samples as possible. This could be formulated via regularization by subtracting an $\\ell_0$ norm, counting the number of selected samples from the reconstruction term. As this leads to an intractable combinatorial problem, we propose two probabilistic relaxations of RAE, which are differentiable and alleviate the need for a combinatorial search. We prove that the solution to the PRAE problem is equivalent to the solution of RAE. We then use synthetic data to demonstrate that PRAE can accurately remove outliers in various contamination levels. Finally, we show that using PRAE for outlier detection leads to state-of-the-art results for inductive and transductive outlier detection.",
        "_bibtex": "@inproceedings{\nlindenbaum2024transductive,\ntitle={Transductive and Inductive Outlier Detection with Robust Autoencoders},\nauthor={Ofir Lindenbaum and Yariv Aizenbud and Yuval Kluger},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=UkA5dZs5mP}\n}"
    },
    {
        "title": "On Hardware-efficient Inference in Probabilistic Circuits",
        "authorids": [
            "~Lingyun_Yao1",
            "~Martin_Trapp2",
            "~Jelin_Leslin1",
            "~Gaurav_Singh4",
            "~Peng_Zhang49",
            "~Karthekeyan_Periasamy1",
            "~Martin_Andraud1"
        ],
        "keywords": [
            "Probabilistic Circuits",
            "Hardware-efficient Inference",
            "Approximate computing"
        ],
        "abstract": "Probabilistic circuits (PCs) offer a promising avenue to perform embedded reasoning under uncertainty. They support efficient and exact computation of various probabilistic inference tasks by design. Hence, hardware-efficient computation of PCs is highly interesting for edge computing applications. As computations in PCs are based on arithmetic with probability values, they are typically performed in the log domain to avoid underflow. Unfortunately, performing the log operation on hardware is costly. Hence, prior work has focused on computations in the linear domain, resulting in high resolution and energy requirements. This work proposes the first dedicated approximate computing framework for PCs that allows for low-resolution logarithm computations. We leverage Addition As Int, resulting in linear PC computation with simple hardware elements. Further, we provide a theoretical approximation error analysis and present an error compensation mechanism. Empirically, our method obtains up to 357\u00d7 and 649\u00d7 energy reduction on custom hardware for evidence and MAP queries respectively with little or no computational error.",
        "_bibtex": "@inproceedings{\nyao2024on,\ntitle={On Hardware-efficient Inference in Probabilistic Circuits},\nauthor={Lingyun Yao and Martin Trapp and Jelin Leslin and Gaurav Singh and Peng Zhang and Karthekeyan Periasamy and Martin Andraud},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=XOlD2SyjQl}\n}"
    },
    {
        "title": "Adaptive Time-Stepping Schedules for Diffusion Models",
        "authorids": [
            "~Yuzhu_Chen1",
            "~Fengxiang_He1",
            "~Shi_Fu1",
            "~Xinmei_Tian1",
            "~Dacheng_Tao1"
        ],
        "keywords": [
            "diffusion models",
            "stepping schedule",
            "generative model"
        ],
        "abstract": "This paper studies how to tune the stepping schedule in diffusion models, which is mostly fixed in current practice, lacking theoretical foundations and assurance of optimal performance at the chosen discretization points. In this paper, we advocate the use of adaptive time-stepping schedules and design two algorithms with an optimized sampling error bound $EB$: (1) for continuous diffusion, we treat $EB$ as the loss function to discretization points and run gradient descent to adjust them; and (2) for discrete diffusion, we propose a greedy algorithm that adjusts only one discretization point to its best position in each iteration. We conducted extensive experiments that show (1) improved generation ability in well-trained models, and (2) premature though usable generation ability in under-trained models. The code is submitted and will be released publicly.",
        "_bibtex": "@inproceedings{\nchen2024adaptive,\ntitle={Adaptive Time-Stepping Schedules for Diffusion Models},\nauthor={Yuzhu Chen and Fengxiang He and Shi Fu and Xinmei Tian and Dacheng Tao},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=lZzriJH2DC}\n}"
    },
    {
        "title": "Posterior Inference on Shallow Infinitely Wide Bayesian Neural Networks under Weights with Unbounded Variance",
        "authorids": [
            "~Jorge_Loria1",
            "~Anindya_Bhadra1"
        ],
        "keywords": [
            "Bayesian Methods",
            "Gaussian Process",
            "Probabilistic Machine Learning",
            "Uncertainty Quantification"
        ],
        "abstract": "From the classical and influential works of Neal (1996), it is known that the infinite width scaling limit of a Bayesian neural network with one hidden layer is a Gaussian process, when the network weights have bounded prior variance. Neal's result has been extended to networks with multiple hidden layers and to convolutional neural networks, also with Gaussian process scaling limits. The tractable properties of Gaussian processes then allow straightforward posterior inference and uncertainty quantification, considerably simplifying the study of the limit process compared to a network of finite width. Neural network weights with unbounded variance, however, pose unique challenges. In this case, the classical central limit theorem breaks down and it is well known that the scaling limit is an $\\alpha$-stable process under suitable conditions. However, current literature is primarily limited to forward simulations under these processes and the problem of posterior inference under such a scaling limit remains largely unaddressed, unlike in the Gaussian process case. To this end, our contribution is an interpretable and computationally efficient procedure for posterior inference, using a conditionally Gaussian representation, that then allows full use of the Gaussian process machinery for tractable posterior inference and uncertainty quantification in the non-Gaussian regime.",
        "_bibtex": "@inproceedings{\nloria2024posterior,\ntitle={Posterior Inference on Shallow Infinitely Wide Bayesian Neural Networks under Weights with Unbounded Variance},\nauthor={Jorge Loria and Anindya Bhadra},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=J97bdMR7Lv}\n}"
    },
    {
        "title": "Learning relevant contextual variables within Bayesian optimization",
        "authorids": [
            "~Julien_Martinelli1",
            "~Ayush_Bharti1",
            "~Armi_Tiihonen1",
            "~S._T._John1",
            "~Louis_Filstroff1",
            "~Sabina_J._Sloman1",
            "~Patrick_Rinke1",
            "~Samuel_Kaski1"
        ],
        "keywords": [
            "Bayesian Optimization",
            "Variable selection",
            "Contextual BO"
        ],
        "abstract": "Contextual Bayesian Optimization (CBO) efficiently optimizes black-box functions with respect\n    to design variables, while simultaneously integrating _contextual_ information regarding\n    the environment, such as experimental conditions.\n    However, the relevance of contextual variables is not necessarily known beforehand.\n    Moreover, contextual variables can sometimes be optimized themselves at additional cost, a setting overlooked by current CBO\n    algorithms.\n    Cost-sensitive CBO would simply include optimizable contextual variables as part of the design variables based on their cost. Instead, we adaptively select a subset of contextual variables to include in the optimization, based on the trade-off between their _relevance_ and the additional cost incurred by optimizing them compared to leaving them to be determined by the environment.\n    We learn the relevance of contextual variables by sensitivity analysis of the posterior surrogate model while\n    minimizing the cost of optimization by leveraging recent developments on early stopping for BO.\n    We empirically evaluate our proposed Sensitivity-Analysis-Driven Contextual BO (_SADCBO_) method against alternatives on both synthetic and real-world\n    experiments, together with extensive ablation studies, and demonstrate a consistent improvement\n    across examples.",
        "_bibtex": "@inproceedings{\nmartinelli2024learning,\ntitle={Learning relevant contextual variables within Bayesian optimization},\nauthor={Julien Martinelli and Ayush Bharti and Armi Tiihonen and S. T. John and Louis Filstroff and Sabina J. Sloman and Patrick Rinke and Samuel Kaski},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=qb33ha9frm}\n}"
    },
    {
        "title": "Anomaly Detection with Variance Stabilized Density Estimation",
        "authorids": [
            "~Amit_Rozner1",
            "~Barak_Battash1",
            "~Henry_Li2",
            "~Lior_Wolf1",
            "~Ofir_Lindenbaum1"
        ],
        "keywords": [
            "Anomaly Detection",
            "Outlier Detection",
            "Density Estimation"
        ],
        "abstract": "We propose a modified density estimation problem that is highly effective for detecting anomalies in tabular data. Our approach assumes that the density function is relatively stable (with lower variance) around normal samples. We have verified this hypothesis empirically using a wide range of real-world data. Then, we present a variance-stabilized density estimation problem for maximizing the likelihood of the observed samples while minimizing the variance of the density around normal samples. To obtain a reliable anomaly detector, we introduce a spectral ensemble of autoregressive models for learning the variance-stabilized distribution. We have conducted an extensive benchmark with 52 datasets, demonstrating that our method leads to state-of-the-art results while alleviating the need for data-specific hyperparameter tuning. Finally, we have used an ablation study to demonstrate the importance of each of the proposed components, followed by a stability analysis evaluating the robustness of our model.",
        "_bibtex": "@inproceedings{\nrozner2024anomaly,\ntitle={Anomaly Detection with Variance Stabilized Density Estimation},\nauthor={Amit Rozner and Barak Battash and Henry Li and Lior Wolf and Ofir Lindenbaum},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=QWnjmAggVj}\n}"
    },
    {
        "title": "Base Models for Parabolic Partial Differential Equations",
        "authorids": [
            "~Xingzi_Xu2",
            "~Ali_Hasan1",
            "~Jie_Ding2",
            "~Vahid_Tarokh1"
        ],
        "keywords": [
            "partial differential equations",
            "feynman-kac",
            "parabolic",
            "meta-learned"
        ],
        "abstract": "Parabolic partial differential equations (PDEs) appear in many disciplines to model the evolution of various mathematical objects, such as probability flows, value functions in control theory, and derivative prices in finance. \nIt is often necessary to compute the solutions or a function of the solutions to a parametric PDE in multiple scenarios corresponding to different parameters of this PDE.\nThis process often requires resolving the PDEs from scratch, which is time-consuming.\nTo better employ existing simulations for the PDEs, we propose a framework for finding solutions to parabolic PDEs across different scenarios by meta-learning an underlying base distribution.%tasks.\nWe build upon this base distribution to propose a method for computing solutions to parametric PDEs under different parameter settings.\nFinally, we illustrate the application of the proposed methods through extensive experiments in generative modeling, stochastic control, and finance.\nThe empirical results suggest that the proposed approach improves generalization to solving new PDEs.",
        "_bibtex": "@inproceedings{\nxu2024base,\ntitle={Base Models for Parabolic Partial Differential Equations},\nauthor={Xingzi Xu and Ali Hasan and Jie Ding and Vahid Tarokh},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=TLYNkysFB3}\n}"
    },
    {
        "title": "Cooperative Meta-Learning with Gradient Augmentation",
        "authorids": [
            "~Jongyun_Shin1",
            "~seungjin_Han1",
            "~Jangho_Kim1"
        ],
        "keywords": [
            "deep learning",
            "meta-learning",
            "few-shot learning"
        ],
        "abstract": "Model agnostic meta-learning (MAML) is one of the most widely used gradient-based meta-learning, consisting of two optimization loops: an inner loop and outer loop. MAML learns the new task from meta-initialization parameters with an inner update and finds the meta-initialization parameters in the outer loop. In general, the injection of noise into the gradient of the model for augmenting the gradient is one of the widely used regularization methods. In this work, we propose a novel cooperative meta-learning framework dubbed CML which leverages gradient-level regularization with gradient augmentation. We inject learnable noise into the gradient of the model for the model generalization. The key idea of CML is introducing the co-learner which has no inner update but the outer loop update to augment gradients for finding better meta-initialization parameters. Since the co-learner does not update in the inner loop, it can be easily deleted after meta-training. Therefore, CML infers with only meta-learner without additional cost and performance degradation. We demonstrate that CML is easily applicable to gradient-based meta-learning methods and CML leads to increased performance in few-shot regression, few-shot image classification and few-shot node classification tasks. Our codes are at https://github.com/JJongyn/CML.",
        "_bibtex": "@inproceedings{\nshin2024cooperative,\ntitle={Cooperative Meta-Learning with Gradient Augmentation},\nauthor={Jongyun Shin and seungjin Han and Jangho Kim},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=ymjADRNzvu}\n}"
    },
    {
        "title": "Label Consistency-based Worker Filtering for Crowdsourcing",
        "authorids": [
            "~Jiao_Li4",
            "~Liangxiao_Jiang1",
            "~Chaoqun_Li1",
            "~Wenjun_Zhang4"
        ],
        "keywords": [
            "Crowdsourcing learning",
            "Label integration",
            "Worker filtering",
            "Label consistency"
        ],
        "abstract": "In crowdsourcing scenarios, we can obtain multiple noisy labels from different crowd workers on the Internet for each instance and then infer its unknown true label via a label integration method. However, noisy labels often have a serious negative impact on label integration. In this case, most existing works always focus on designing more complex label integration methods to infer unknown true labels more accurately from multiple noisy labels, but little attention has been paid to another perspective, i.e., purifying noisy labels before label integration. In this paper, we aim to purify noisy labels for existing label integration methods and propose a label consistency-based worker filtering (LCWF) algorithm. In LCWF, we consider that if all low-quality workers are filtered out and only high-quality workers remain, the label consistency should be high. Therefore, we utilize label consistency to filter out low-quality workers. Firstly, we directly transform the worker filtering problem into a discrete optimization problem and utilize label consistency to define the fitness function for this problem. Then, we search for the optimal solution to this problem by a genetic algorithm. Finally, we filter out all labels from low-quality workers according to the optimal solution we obtained. Experimental results on simulated and real-world datasets demonstrate that LCWF can effectively purify noisy labels and improve the integration accuracy of existing label integration methods.",
        "_bibtex": "@inproceedings{\nli2024label,\ntitle={Label Consistency-based Worker Filtering for Crowdsourcing},\nauthor={Jiao Li and Liangxiao Jiang and Chaoqun Li and Wenjun Zhang},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=eOAkWHM4M8}\n}"
    },
    {
        "title": "Patch-Prompt Aligned Bayesian Prompt Tuning for Vision-Language Models",
        "authorids": [
            "~Xinyang_Liu4",
            "~Dongsheng_Wang4",
            "~Bowei_Fang2",
            "~Miaoge_Li1",
            "~Yishi_Xu2",
            "~Zhibin_Duan1",
            "~Bo_Chen1",
            "~Mingyuan_Zhou1"
        ],
        "keywords": [
            "Bayesian prompt learning; pre-trained vision language model"
        ],
        "abstract": "For downstream applications of vision-language pre-trained models, there has been significant interest in constructing effective prompts. Existing works on prompt engineering, which either require laborious manual designs or optimize the prompt tuning as a point estimation problem, may fail to describe diverse characteristics of categories and limit their applications. We introduce a Bayesian probabilistic resolution to prompt tuning, where the label-specific stochastic prompts are generated hierarchically by first sampling a latent vector from an underlying distribution and then employing a lightweight generative model. Importantly, we semantically regularize the tuning process by minimizing the statistic distance between the visual patches and linguistic prompts, which pushes the stochastic label representations to faithfully capture diverse visual concepts, instead of overfitting the training categories. We evaluate the effectiveness of our approach on four tasks: few-shot image recognition, base-to-new generalization, dataset transfer learning, and domain shifts. Extensive results on over 15 datasets show promising transferability and generalization performance of our proposed model, both quantitatively and qualitatively.",
        "_bibtex": "@inproceedings{\nliu2024patchprompt,\ntitle={Patch-Prompt Aligned Bayesian Prompt Tuning for Vision-Language Models},\nauthor={Xinyang Liu and Dongsheng Wang and Bowei Fang and Miaoge Li and Yishi Xu and Zhibin Duan and Bo Chen and Mingyuan Zhou},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=DJa8rxF67Y}\n}"
    },
    {
        "title": "Distributionally Robust Optimization as a Scalable Framework to Characterize Extreme Value Distributions",
        "authorids": [
            "~Patrick_Kendal_Kuiper1",
            "~Ali_Hasan1",
            "~Wenhao_Yang2",
            "~Jose_Blanchet1",
            "~Vahid_Tarokh1",
            "~Yuting_Ng1",
            "~Hoda_Bidkhori1"
        ],
        "keywords": [
            "extreme value theory",
            "distributionally robust optimization",
            "neural networks",
            "generative models"
        ],
        "abstract": "The goal of this paper is to develop distributionally robust optimization (DRO) estimators, specifically for multidimensional Extreme Value Theory (EVT) statistics. EVT supports using semi-parametric models called max-stable distributions built from spatial Poisson point processes. While powerful, these models are only asymptotically valid for large samples. However, since extreme data is by definition scarce, the potential for model misspecification error is inherent to these applications, thus DRO estimators are natural. In order to mitigate over-conservative estimates while enhancing out-of-sample performance, we study DRO estimators informed by semi-parametric max-stable constraints in the space of point processes. We study both tractable convex formulations for some problems of interest (e.g. CVaR) and more general neural network based estimators. Both approaches are validated using synthetically generated data, recovering prescribed characteristics, and verifying the efficacy of the proposed techniques. Additionally, the proposed method is applied to a real data set of financial returns for comparison to a previous analysis. We established the proposed model as a novel formulation in the multivariate EVT domain, and innovative with respect to performance when compared to relevant alternate proposals.",
        "_bibtex": "@inproceedings{\nkuiper2024distributionally,\ntitle={Distributionally Robust Optimization as a Scalable Framework to Characterize Extreme Value Distributions},\nauthor={Patrick Kendal Kuiper and Ali Hasan and Wenhao Yang and Jose Blanchet and Vahid Tarokh and Yuting Ng and Hoda Bidkhori},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=h28p1SR7Gw}\n}"
    },
    {
        "title": "Mitigating Overconfidence in Out-of-Distribution Detection by Capturing Extreme Activations",
        "authorids": [
            "~Mohammad_Azizmalayeri1",
            "~Ameen_Abu-Hanna1",
            "~Giovanni_Cin\u00e01"
        ],
        "keywords": [
            "OOD Detection",
            "Overconfidence",
            "Data Shift"
        ],
        "abstract": "Detecting out-of-distribution (OOD) instances is crucial for the reliable deployment of machine learning models in real-world scenarios.\nOOD inputs are commonly expected to cause a more uncertain prediction in the primary task; however, there are OOD cases for which the model returns a highly confident prediction. This phenomenon, denoted as \"overconfidence\", presents a challenge to OOD detection. \nSpecifically, theoretical evidence indicates that overconfidence is an intrinsic property of certain neural network architectures, leading to poor OOD detection. In this work, we address this issue by measuring extreme activation values in the penultimate layer of neural networks and then leverage this proxy of overconfidence to improve on several OOD detection baselines. We test our method on a wide array of experiments spanning synthetic data and real-world data, tabular and image datasets, multiple architectures such as ResNet and Transformer, different training loss functions, and include the scenarios examined in previous theoretical work. Compared to the baselines, our method often grants substantial improvements, with double-digit increases in OOD detection AUC, and it does not damage performance in any scenario.",
        "_bibtex": "@inproceedings{\nazizmalayeri2024mitigating,\ntitle={Mitigating Overconfidence in Out-of-Distribution Detection by Capturing Extreme Activations},\nauthor={Mohammad Azizmalayeri and Ameen Abu-Hanna and Giovanni Cin{\\`a}},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=nwf2mKQVhP}\n}"
    },
    {
        "title": "Graph Contrastive Learning under Heterophily via Graph Filters",
        "authorids": [
            "~Wenhan_Yang5",
            "~Baharan_Mirzasoleiman1"
        ],
        "keywords": [
            "Graph Representation Learning",
            "Contrastive Learning"
        ],
        "abstract": "Graph contrastive learning (CL) methods learn node representations in a self-supervised manner by maximizing the similarity between the augmented node representations obtained via a GNN-based encoder. However, CL methods perform poorly on graphs with heterophily, where connected nodes tend to belong to different classes. In this work, we address this problem by proposing an effective graph CL method, namely HLCL, for learning graph representations under heterophily. HLCL first identifies a homophilic and a heterophilic subgraph based on the cosine similarity of node features. It then uses a low-pass and a high-pass graph filter to aggregate representations of nodes connected in the homophilic subgraph and differentiate representations of nodes in the heterophilic subgraph. The final node representations are learned by contrasting both the augmented high-pass filtered views and the augmented low-pass filtered node views. Our extensive experiments show that HLCL outperforms state-of-the-art graph CL methods on benchmark datasets with heterophily, as well as large-scale real-world graphs, by up to 7%, and outperforms graph supervised learning methods on datasets with heterophily by up to 10%.",
        "_bibtex": "@inproceedings{\nyang2024graph,\ntitle={Graph Contrastive Learning under Heterophily via Graph Filters},\nauthor={Wenhan Yang and Baharan Mirzasoleiman},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=khvJM3uFk8}\n}"
    },
    {
        "title": "Functional Wasserstein Variational Policy Optimization",
        "authorids": [
            "~Junyu_Xuan1",
            "~Mengjing_Wu1",
            "~Zihe_Liu1",
            "~Jie_Lu4"
        ],
        "keywords": [
            "policy optimization",
            "uncertainty"
        ],
        "abstract": "Variational policy optimization has become increasingly attractive to the reinforcement learning community because of its strong capability in uncertainty modeling and environment generalization. However, almost all existing studies in this area rely on Kullback\u2013Leibler (KL) divergence which is unfortunately ill-defined in several situations. In addition, the policy is parameterized and optimized in weight space, which may not only bring additional unnecessary bias but also make the policy learning harder due to the complicatedly dependent weight posterior. In the paper, we design a novel functional Wasserstein variational policy optimization (FWVPO) based on the Wasserstein distance between function distributions. Specifically, we firstly parameterize policy as a Bayesian neural network but from a function-space view rather than a weight-space view and then propose FWVPO to optimize and explore the functional policy posterior. We prove that our FWVPO is a valid variational Bayesian objective and also guarantees the monotonic expected reward improvement under certain conditions. Experimental results on multiple reinforcement learning tasks demonstrate the efficiency of our new algorithm in terms of both cumulative rewards and uncertainty modeling capability.",
        "_bibtex": "@inproceedings{\nxuan2024functional,\ntitle={Functional Wasserstein Variational Policy Optimization},\nauthor={Junyu Xuan and Mengjing Wu and Zihe Liu and Jie Lu},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=8m7MSD7dEF}\n}"
    },
    {
        "title": "How to Fix a Broken Confidence Estimator: Evaluating Post-hoc Methods for Selective Classification with Deep Neural Networks",
        "authorids": [
            "~Lu\u00eds_Felipe_Prates_Cattelan1",
            "~Danilo_Silva1"
        ],
        "keywords": [
            "Selective classification",
            "deep learning",
            "uncertainty estimation",
            "failure prediction",
            "misclassification detection",
            "reject option",
            "neural networks",
            "distribution shift"
        ],
        "abstract": "This paper addresses the problem of selective classification for deep neural networks, where a model is allowed to abstain from low-confidence predictions to avoid potential errors. We focus on so-called post-hoc methods, which replace the confidence estimator of a given classifier without modifying or retraining it, thus being practically appealing. Considering neural networks with softmax outputs, our goal is to identify the best confidence estimator that can be computed directly from the unnormalized logits. This problem is motivated by the intriguing observation in recent work that many classifiers appear to have a ``broken'' confidence estimator, in the sense that their selective classification performance is much worse than what could be expected by their corresponding accuracies. We perform an extensive experimental study of many existing and proposed confidence estimators applied to 84 pretrained ImageNet classifiers available from popular repositories. Our results show that a simple $p$-norm normalization of the logits, followed by taking the maximum logit as the confidence estimator, can lead to considerable gains in selective classification performance, completely fixing the pathological behavior observed in many classifiers. As a consequence, the selective classification performance of any classifier becomes almost entirely determined by its corresponding accuracy. Moreover, these results are shown to be consistent under distribution shift.",
        "_bibtex": "@inproceedings{\ncattelan2024how,\ntitle={How to Fix a Broken Confidence Estimator: Evaluating Post-hoc Methods for Selective Classification with Deep Neural Networks},\nauthor={Lu{\\'\\i}s Felipe Prates Cattelan and Danilo Silva},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=IJBWLRCvYX}\n}"
    },
    {
        "title": "AutoDrop: Training Deep Learning Models with Automatic Learning Rate Drop",
        "authorids": [
            "~Jing_Wang24",
            "~Yunfei_Teng1",
            "~Anna_Ewa_Choromanska1"
        ],
        "keywords": [
            "optimization in deep learning",
            "automatic learning rate"
        ],
        "abstract": "Modern deep learning (DL) architectures are trained using variants of the SGD algorithm and typically rely on the user to manually drop the learning rate when the training curve saturates. In this paper, we develop an algorithm, that we call AutoDrop, that realizes the learning rate drop automatically and stems from the properties of the learning dynamics of DL systems. Specifically, it is motivated by the observation that the angular velocity of the model parameters, i.e., the velocity of the changes of the convergence direction, for a fixed learning rate initially increases rapidly and then progresses towards soft saturation. At saturation, the optimizer slows down thus the angular velocity saturation is a good indicator for dropping the learning rate. After the drop, the angular velocity \u201cresets\u201d and follows the pattern described above, increasing again until saturation. AutoDrop is built on this idea and drops the learning rate whenever the angular velocity saturates. The method is simple to implement, computationally cheap, and by design avoids the short-horizon bias problem. We show that AutoDrop achieves favorable performance compared to many different baseline manual and automatic learning rate schedulers, and matches the SOTA performance on all our experiments. On the theoretical front, we claim two contributions: we formulate the learning rate behavior based on the angular velocity and provide general convergence theory for the learning rate schedulers that decrease the learning rate step-wise, rather than continuously as is commonly analyzed.",
        "_bibtex": "@inproceedings{\nwang2024autodrop,\ntitle={AutoDrop: Training Deep Learning Models with Automatic Learning Rate Drop},\nauthor={Jing Wang and Yunfei Teng and Anna Ewa Choromanska},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=OrvFCwvmHd}\n}"
    },
    {
        "title": "Robust Entropy Search for Safe Efficient Bayesian Optimization",
        "authorids": [
            "~Dorina_Weichert1",
            "~Alexander_Kister1",
            "~Sebastian_Houben1",
            "~Patrick_Link1",
            "~Gunar_Ernis1"
        ],
        "keywords": [
            "Bayesian Optimization",
            "Robustness",
            "Information-Based Acquisition Functions"
        ],
        "abstract": "The practical use of Bayesian Optimization (BO) in engineering applications imposes special requirements: high sampling efficiency on the one hand and finding a robust solution on the other hand. We address the case of adversarial robustness, where all parameters are controllable during the optimization process, but a subset of them is uncontrollable or even adversely perturbed at the time of application. To this end, we develop an efficient information-based acquisition function that we call Robust Entropy Search (RES). We empirically demonstrate its benefits in experiments on synthetic and real-life data. The results show that RES reliably finds robust optima, outperforming state-of-the-art algorithms.",
        "_bibtex": "@inproceedings{\nweichert2024robust,\ntitle={Robust Entropy Search for Safe Efficient Bayesian Optimization},\nauthor={Dorina Weichert and Alexander Kister and Sebastian Houben and Patrick Link and Gunar Ernis},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=MIznZNiEAm}\n}"
    },
    {
        "title": "Early-Exit Neural Networks with Nested Prediction Sets",
        "authorids": [
            "~Metod_Jazbec1",
            "~Patrick_Forr\u00e91",
            "~Stephan_Mandt1",
            "~Dan_Zhang1",
            "~Eric_Nalisnick1"
        ],
        "keywords": [
            "early-exit neural networks",
            "uncertainty quantification",
            "anytime-valid confidence sequence"
        ],
        "abstract": "Early-exit neural networks (EENNs) facilitate adaptive inference by producing predictions at multiple stages of the forward pass. In safety-critical applications, these predictions are only meaningful when complemented with reliable uncertainty estimates.  Yet, due to their sequential structure, an EENN's uncertainty estimates should also be *consistent*: labels that are deemed improbable at one exit should not reappear within the confidence interval / set of later exits.  We show that standard uncertainty quantification techniques, like Bayesian methods or conformal prediction, can lead to inconsistency across exits. We address this problem by applying anytime-valid confidence sequences (AVCSs) to the exits of EENNs.  By design, AVCSs maintain consistency across exits. We examine the theoretical and practical challenges of applying AVCSs to EENNs and empirically validate our approach on both regression and classification tasks.",
        "_bibtex": "@inproceedings{\njazbec2024earlyexit,\ntitle={Early-Exit Neural Networks with Nested Prediction Sets},\nauthor={Metod Jazbec and Patrick Forr{\\'e} and Stephan Mandt and Dan Zhang and Eric Nalisnick},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=mAjIKFHa2P}\n}"
    },
    {
        "title": "Localised Natural Causal Learning Algorithms for Weak Consistency Conditions",
        "authorids": [
            "~Kai_Z._Teh1",
            "~Kayvan_Sadeghi1",
            "~Terry_Soo1"
        ],
        "keywords": [
            "Causal Discovery",
            "Graphical Models"
        ],
        "abstract": "By relaxing conditions for \u201cnatural\u201d structure learning algorithms, a family of constraint-based algorithms containing all exact structure learning algorithms under the faithfulness assumption, we define localised natural structure learning algorithms (LoNS). We also provide a set of necessary and sufficient assumptions for consistency of LoNS, which can be thought of as a strict relaxation of the restricted faithfulness assumption. We provide a practical LoNS algorithm that runs in exponential time, which is then compared with related existing structure learning algorithms, namely PC/SGS and the relatively recent Sparsest Permutation algorithm. Simulation studies are also provided.",
        "_bibtex": "@inproceedings{\nteh2024localised,\ntitle={Localised Natural Causal Learning Algorithms for Weak Consistency Conditions},\nauthor={Kai Z. Teh and Kayvan Sadeghi and Terry Soo},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=aq03WSU3iJ}\n}"
    },
    {
        "title": "Exploring High-dimensional Search Space via Voronoi Graph Traversing",
        "authorids": [
            "~Aidong_Zhao1",
            "~Xuyang_Zhao4",
            "~Tianchen_Gu1",
            "~zhaori_bi1",
            "~Xinwei_Sun1",
            "~Changhao_Yan1",
            "~Fan_Yang31",
            "~Dian_Zhou1",
            "~Xuan_Zeng1"
        ],
        "keywords": [
            "High-dimensional Bayesian optimization",
            "black-box optimization",
            "Voronoi diagram"
        ],
        "abstract": "Bayesian optimization (BO) is a well-established methodology for optimizing costly black-box functions. However, the sparse observations in the high-dimensional search space pose challenges in constructing reliable Gaussian Process (GP) models, which leads to blind exploration of the search space. We propose a novel Voronoi Graph Traversing (VGT) algorithm to extend BO to ultra high-dimensional problems. VGT employs a Voronoi diagram to mesh the design space and transform it into an undirected Voronoi graph. VGT explores the search space by iteratively performing path selection, promising cell sampling, and graph expansion operations. We introduce a UCB-based global traversal strategy to select the path towards promising Voronoi cells. Then we perform local BO within the promising cell and train local GP with a neighboring subset. The intrinsic geometric boundaries and adjacency of the Voronoi graph assist in fine-tuning the trajectory of local BO sampling. We also present a subspace enhancement approach for the intrinsic low-dimensional problems. Experimental results, including both synthetic benchmarks and real-world applications, demonstrate the proposed approach's state-of-the-art performance for tackling ultra high-dimensional problems ranging from hundreds to one thousand dimensions.",
        "_bibtex": "@inproceedings{\nzhao2024exploring,\ntitle={Exploring High-dimensional Search Space via Voronoi Graph Traversing},\nauthor={Aidong Zhao and Xuyang Zhao and Tianchen Gu and zhaori bi and Xinwei Sun and Changhao Yan and Fan Yang and Dian Zhou and Xuan Zeng},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=Phyo9GzgWd}\n}"
    },
    {
        "title": "On the Convergence of Hierarchical Federated Learning with Partial Worker Participation",
        "authorids": [
            "~Xiaohan_Jiang1",
            "~Hongbin_Zhu1"
        ],
        "keywords": [
            "Hierarchical Federated Learning",
            "Partial Worker Participation",
            "Convergence Analysis"
        ],
        "abstract": "Hierarchical federated learning (HFL) has emerged as the architecture of choice for multi-level communication networks, mainly because of its data privacy protection and low communication cost.\nHowever, existing studies on the convergence analysis for HFL are limited to the assumptions of full worker participation and/or i.i.d. datasets across workers, both of which rarely hold in practice.\nMotivated by this, we in this work propose a unified convergence analysis framework for HFL covering both full and partial worker participation with non-i.i.d. data, non-convex objective function and stochastic gradient.\nWe correspondingly develop a three-sided learning rates algorithm to mitigate data divergences issue, thereby realizing better convergence performance. \nOur theoretical results provide key insights of why partial participation of HFL is beneficial in significantly reducing the data divergences compared to standard FL.\nBesides, the convergence analysis allows certain individualization for each cluster in HFL indicating that adjusting the worker sampling ratio and round period can improve the convergence behavior.",
        "_bibtex": "@inproceedings{\njiang2024on,\ntitle={On the Convergence of Hierarchical Federated Learning with Partial Worker Participation},\nauthor={Xiaohan Jiang and Hongbin Zhu},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=fbOg2MB3Uy}\n}"
    },
    {
        "title": "Publishing Number of Walks and Katz Centrality under Local Differential Privacy",
        "authorids": [
            "~Louis_BETZER1",
            "~Vorapong_Suppakitpaisarn1",
            "~Quentin_Hillebrand1"
        ],
        "keywords": [
            "AI under privacy constraints",
            "local differential privacy",
            "social network analysis and graph algorithms",
            "Katz centrality"
        ],
        "abstract": "In our study, we present an algorithm for publishing the count of walks and Katz centrality under local differential privacy (LDP), complemented by a comprehensive theoretical analysis. While previous research in LDP has predominantly focused on counting subgraphs with a maximum of five nodes, our work extends this to larger subgraphs. The primary challenge in such an extension lies in managing the exponentially increasing noise associated with LDP as the size of the subgraph grows. Our solution involves an algorithm for publishing the count of walks originating from each node in the graph, which subsequently enables us to publish the Katz centrality of all nodes. This algorithm incorporates multiple communication rounds and employs a clipping technique. Through both theoretical and empirical evaluation, we demonstrate that our algorithm achieves has a relatively small bias and variance, showing significant improvements over both the randomized response method and non-clipping algorithms. Additionally, our approach to estimating Katz centrality successfully identifies up to 90\\% of the nodes with the highest centrality values.",
        "_bibtex": "@inproceedings{\nbetzer2024publishing,\ntitle={Publishing Number of Walks and Katz Centrality under Local Differential Privacy},\nauthor={Louis BETZER and Vorapong Suppakitpaisarn and Quentin Hillebrand},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=76UkTmdmkB}\n}"
    },
    {
        "title": "QuantProb: Generalizing Probabilities along with Predictions for a Pre-trained Classifier",
        "authorids": [
            "~Aditya_Challa1",
            "~Soma_S_Dhavala1",
            "~Snehanshu_Saha1"
        ],
        "keywords": [
            "Calibration",
            "Quantile Regression",
            "Robust Probabilities"
        ],
        "abstract": "Quantification of Uncertainty in predictions is a challenging problem. In the classification settings, although deep learning based models generalize well, class probabilities often lack reliability. Calibration errors are used to quantify uncertainty, and several methods exist to minimize calibration error. We argue that between the choice of having a minimum calibration error on original distribution which increases across distortions or having a (possibly slightly higher) calibration error which is constant across distortions, we prefer the latter\n \n We hypothesize that the reason for unreliability of deep networks is - The way neural networks are currently trained, the probabilities do not generalize across small distortions. We observe that quantile based approaches can potentially solve this problem. We propose an innovative approach to decouple the construction of quantile representations from the loss function allowing us to compute quantile based probabilities without disturbing the original network. We achieve this by establishing a novel duality property between quantiles and probabilities, and an ability to obtain quantile probabilities from any pre-trained classifier.\n \n While post-hoc calibration techniques successfully minimize calibration errors, they do not preserve robustness to distortions. We show that, Quantile probabilities (QuantProb), obtained from Quantile representations, preserve the calibration errors across distortions, since quantile probabilities generalize better than the naive Softmax probabilities.",
        "_bibtex": "@inproceedings{\nchalla2024quantprob,\ntitle={QuantProb: Generalizing Probabilities along with Predictions for a Pre-trained Classifier},\nauthor={Aditya Challa and Soma S Dhavala and Snehanshu Saha},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=FBQlYI4LVM}\n}"
    },
    {
        "title": "How Inverse Conditional Flows Can Serve as a Substitute for Distributional Regression",
        "authorids": [
            "~Lucas_Kook1",
            "~Chris_Kolb1",
            "~Philipp_Schiele1",
            "~Daniel_Dold1",
            "~Marcel_Arpogaus2",
            "~Cornelius_Fritz1",
            "~Philipp_F._M._Baumann1",
            "~Philipp_Kopper1",
            "~Tobias_Pielok1",
            "~Emilio_Dorigatti1",
            "~David_R\u00fcgamer1"
        ],
        "keywords": [
            "Normalizing Flows",
            "Distribution Regression",
            "Transformation Models",
            "Aleatoric Uncertainty"
        ],
        "abstract": "Neural network representations of simple models, such as linear regression, are being studied increasingly to better understand the underlying principles of deep learning algorithms. However, neural representations of distributional regression models, such as the Cox model, have received little attention so far. We close this gap by proposing a framework for distributional regression using inverse flow transformations (DRIFT), which includes neural representations of the aforementioned models. We empirically demonstrate that the neural representations of models in DRIFT can serve as a substitute for their classical statistical counterparts in several applications involving continuous, ordered, time-series, and survival outcomes. We confirm that models in DRIFT empirically match the performance of several statistical methods in terms of estimation of partial effects, prediction, and aleatoric uncertainty quantification. DRIFT covers both interpretable statistical models and flexible neural networks opening up new avenues in both statistical modeling and deep learning.",
        "_bibtex": "@inproceedings{\nkook2024how,\ntitle={How Inverse Conditional Flows Can Serve as a Substitute for Distributional Regression},\nauthor={Lucas Kook and Chris Kolb and Philipp Schiele and Daniel Dold and Marcel Arpogaus and Cornelius Fritz and Philipp F. M. Baumann and Philipp Kopper and Tobias Pielok and Emilio Dorigatti and David R{\\\"u}gamer},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=jd5DhbTsde}\n}"
    },
    {
        "title": "Bayesian Pseudo-Coresets via Contrastive Divergence",
        "authorids": [
            "~Piyush_Tiwary1",
            "~Kumar_Shubham1",
            "~Vivek_V_Kashyap1",
            "~Prathosh_AP1"
        ],
        "keywords": [
            "Data Efficient ML",
            "Energy-based Models"
        ],
        "abstract": "Bayesian methods provide an elegant framework for estimating parameter posteriors and quantification of uncertainty associated with probabilistic models. However, they often suffer from slow inference times. To address this challenge, Bayesian Pseudo-Coresets (BPC) have emerged as a promising solution. BPC methods aim to create a small synthetic dataset, known as pseudo-coresets, that approximates the posterior inference achieved with the original dataset. This approximation is achieved by optimizing a divergence measure between the true posterior and the pseudo-coreset posterior. Various divergence measures have been proposed for constructing pseudo-coresets, with forward Kullback-Leibler (KL) divergence being the most successful. However, using forward KL divergence necessitates sampling from the pseudo-coreset posterior, often accomplished through approximate Gaussian variational distributions. Alternatively, one could employ Markov Chain Monte Carlo (MCMC) methods for sampling, but this becomes challenging in high-dimensional parameter spaces due to slow mixing.\nIn this study, we introduce a novel approach for constructing pseudo-coresets by utilizing contrastive divergence. Importantly, optimizing contrastive divergence eliminates the need for approximations in the pseudo-coreset construction process. Furthermore, it enables the use of finite-step MCMC methods, alleviating the requirement for extensive mixing to reach a stationary distribution. To validate our method's effectiveness, we conduct extensive experiments on multiple datasets, demonstrating its superiority over existing BPC techniques. \nOur implementation is available at https://github.com/backpropagator/BPC-CD .",
        "_bibtex": "@inproceedings{\ntiwary2024bayesian,\ntitle={Bayesian Pseudo-Coresets via Contrastive Divergence},\nauthor={Piyush Tiwary and Kumar Shubham and Vivek V Kashyap and Prathosh AP},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=SHsR2VOOKv}\n}"
    },
    {
        "title": "Metric Learning from Limited Pairwise Preference Comparisons",
        "authorids": [
            "~Zhi_Wang3",
            "~Geelon_So1",
            "~Ramya_Korlakai_Vinayak1"
        ],
        "keywords": [
            "metric learning",
            "preference comparisons",
            "crowdsourced data",
            "subspace structure",
            "ideal point model"
        ],
        "abstract": "We study metric learning from preference comparisons under the ideal point model, in which a user prefers an item over another if it is closer to their latent ideal item. These items are embedded into $\\mathbb{R}^d$ equipped with an unknown Mahalanobis distance shared across users. While recent work shows that it is possible to simultaneously recover the metric and ideal items given $\\mathcal{O}(d)$ pairwise comparisons per user, in practice we often have a limited budget of $o(d)$ comparisons. We study whether the metric can still be recovered, even though learning individual ideal items is now no longer possible. We show that, on the one hand, $o(d)$ comparisons may not reveal any information about the metric, even with infinitely many users. On the other hand, when comparisons are made over items that exhibit low-dimensional structure, each user can contribute to learning the metric restricted to a low-dimensional subspace so that the metric can be jointly identified. We present a divide-and-conquer approach that achieves this, and provide theoretical recovery guarantees and empirical validation.",
        "_bibtex": "@inproceedings{\nwang2024metric,\ntitle={Metric Learning from Limited Pairwise Preference Comparisons},\nauthor={Zhi Wang and Geelon So and Ramya Korlakai Vinayak},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=VFf9pwPYeX}\n}"
    },
    {
        "title": "GeONet: a neural operator for learning the Wasserstein geodesic",
        "authorids": [
            "~Andrew_Gracyk1",
            "~Xiaohui_Chen3"
        ],
        "keywords": [
            "neural operator",
            "Wasserstein",
            "geodesic",
            "optimal transport",
            "physics-informed"
        ],
        "abstract": "Optimal transport (OT) offers a versatile framework to compare complex data distributions in a geometrically meaningful way. Traditional methods for computing the Wasserstein distance and geodesic between probability measures require mesh-specific domain discretization and suffer from the curse-of-dimensionality. We present GeONet, a mesh-invariant deep neural operator network that learns the non-linear mapping from the input pair of initial and terminal distributions to the Wasserstein geodesic connecting the two endpoint distributions. In the offline training stage, GeONet learns the saddle point optimality conditions for the dynamic formulation of the OT problem in the primal and dual spaces that are characterized by a coupled PDE system. The subsequent inference stage is instantaneous and can be deployed for real-time predictions in the online learning setting. We demonstrate that GeONet achieves comparable testing accuracy to the standard OT solvers on simulation examples and the MNIST dataset with considerably reduced inference-stage computational cost by orders of magnitude.",
        "_bibtex": "@inproceedings{\ngracyk2024geonet,\ntitle={Ge{ON}et: a neural operator for learning the Wasserstein geodesic},\nauthor={Andrew Gracyk and Xiaohui Chen},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=huNOnN0UK5}\n}"
    },
    {
        "title": "Can we Defend Against the Unknown? An Empirical Study About Threshold Selection for Neural Network Monitoring",
        "authorids": [
            "~Khoi_Tran_DANG2",
            "~Kevin_Delmas1",
            "~Jeremie_Guiochet1",
            "~Joris_Guerin1"
        ],
        "keywords": [
            "Neural Network Runtime Monitoring",
            "Machine Learning Safety",
            "Threshold Optimization"
        ],
        "abstract": "With the increasing use of neural networks in critical systems, runtime monitoring becomes essential to reject unsafe predictions during inference. Various techniques have emerged to establish rejection scores that maximize the separability between the distributions of safe and unsafe predictions. The efficacy of these approaches is mostly evaluated using threshold-agnostic metrics, such as the area under the receiver operating characteristic curve. However, in real-world applications, an effective monitor also requires identifying a good threshold to transform these scores into meaningful binary decisions. Despite the pivotal importance of threshold optimization, this problem has received little attention. A few studies touch upon this question, but they typically assume that the runtime data distribution mirrors the training distribution, which is a strong assumption as monitors are supposed to safeguard a system against potentially unforeseen threats. In this work, we present rigorous experiments on various image datasets to investigate: 1. The effectiveness of monitors in handling unforeseen threats, which are not available during threshold adjustments. 2. Whether integrating generic threats into the threshold optimization scheme can enhance the robustness of monitors.",
        "_bibtex": "@inproceedings{\ndang2024can,\ntitle={Can we Defend Against the Unknown? An Empirical Study About Threshold Selection for Neural Network Monitoring},\nauthor={Khoi Tran DANG and Kevin Delmas and Jeremie Guiochet and Joris Guerin},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=nzxnE8t2Sz}\n}"
    },
    {
        "title": "Guaranteeing Robustness Against Real-World Perturbations In Time Series Classification Using Conformalized Randomized Smoothing",
        "authorids": [
            "~Nicola_Franco1",
            "~Jakob_Spiegelberg1",
            "~Jeanette_Miriam_Lorenz1",
            "~Stephan_G\u00fcnnemann1"
        ],
        "keywords": [
            "Randomized smoothing",
            "Conformal Prediction",
            "Uncertainty Quantification",
            "Robust Machine Learning",
            "Domain Shifts"
        ],
        "abstract": "Certifying the robustness of machine learning models against domain shifts and input space perturbations is crucial for many applications, where high risk decisions are based on the model's predictions. Techniques such as randomized smoothing have partially addressed this issues with a focus on adversarial attacks in the past. In this paper, we generalize randomized smoothing to arbitrary transformations and extend it to conformal prediction. The proposed ansatz is demonstrated on a time series classifier connected to an automotive use case. We meticulously assess the robustness of smooth classifiers in environments subjected to various degrees and types of time series native perturbations and compare it against standard conformal predictors. The proposed method consistently offers superior resistance to perturbations, maintaining high classification accuracy and reliability. Additionally, we are able to bound the performance on new domains via calibrating generalization with configuration shifts in the training data. In combination, conformalized randomized smoothing may offer a model agnostic approach to construct robust classifiers tailored to perturbations in their respective applications - a crucial capability for AI assurance argumentation.",
        "_bibtex": "@inproceedings{\nfranco2024guaranteeing,\ntitle={Guaranteeing Robustness Against Real-World Perturbations In Time Series Classification Using Conformalized Randomized Smoothing},\nauthor={Nicola Franco and Jakob Spiegelberg and Jeanette Miriam Lorenz and Stephan G{\\\"u}nnemann},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=wu3JIjKmXQ}\n}"
    },
    {
        "title": "Two Facets of SDE Under an Information-Theoretic Lens: Generalization of SGD via Training Trajectories and via Terminal States",
        "authorids": [
            "~Ziqiao_Wang1",
            "~Yongyi_Mao2"
        ],
        "keywords": [
            "Generalization",
            "information-theoretic generalization bound",
            "SGD",
            "SDE"
        ],
        "abstract": "Stochastic differential equations (SDEs) have been shown recently to characterize well the dynamics of training machine learning models with SGD. When the generalization error of the SDE approximation closely aligns with that of SGD in expectation, it provides two opportunities for understanding better the generalization behaviour of SGD through its SDE approximation. Firstly, viewing SGD as full-batch gradient descent with Gaussian gradient noise allows us to obtain trajectory-based generalization bound using the information-theoretic bound from Xu and Raginsky [2017]. Secondly, assuming mild conditions, we estimate the steady-state weight distribution of SDE and use information-theoretic bounds from Xu and Raginsky [2017] and Negrea et al. [2019] to establish terminal-state-based generalization bounds. Our proposed bounds have some advantages, notably the trajectory-based bound outperforms results in Wang and Mao [2022], and the terminal-state-based bound exhibits a fast decay rate comparable to stability-based bounds.",
        "_bibtex": "@inproceedings{\nwang2024two,\ntitle={Two Facets of {SDE} Under an Information-Theoretic Lens: Generalization of {SGD} via Training Trajectories and via Terminal States},\nauthor={Ziqiao Wang and Yongyi Mao},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=ZhMkLTWRyh}\n}"
    },
    {
        "title": "ContextFlow++: Generalist-Specialist Flow-based Generative Models with Mixed-variable Context Encoding",
        "authorids": [
            "~Denis_A_Gudovskiy1",
            "~Tomoyuki_Okuno1",
            "~Yohei_Nakata1"
        ],
        "keywords": [
            "normalizing flows",
            "contexts",
            "discrete",
            "conditioning",
            "anomaly detection",
            "failure prediction"
        ],
        "abstract": "Normalizing flow-based generative models have been widely used in applications where the exact density estimation is of major importance. Recent research proposes numerous methods to improve their expressivity. \nHowever, conditioning on a context is largely overlooked area in the bijective flow research. Conventional conditioning with the vector concatenation is limited to only a few flow types. \nMore importantly, this approach cannot support a practical setup where a set of context-conditioned (*specialist*) models are trained with the fixed pretrained general-knowledge (*generalist*) model. We propose ContextFlow++ approach to overcome these limitations using an additive conditioning with explicit generalist-specialist knowledge decoupling. Furthermore, we support discrete contexts by the proposed mixed-variable architecture with context encoders. Particularly, our context encoder for discrete variables is a surjective flow from which the context-conditioned continuous variables are sampled. Our experiments on rotated MNIST-R, corrupted CIFAR-10C, real-world ATM predictive maintenance and SMAP unsupervised anomaly detection benchmarks show that the proposed ContextFlow++ offers faster stable training and achieves higher performance metrics. Our code is publicly available at [github.com/gudovskiy/contextflow](https://github.com/gudovskiy/contextflow).",
        "_bibtex": "@inproceedings{\ngudovskiy2024contextflow,\ntitle={ContextFlow++: Generalist-Specialist Flow-based Generative Models with Mixed-variable Context Encoding},\nauthor={Denis A Gudovskiy and Tomoyuki Okuno and Yohei Nakata},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=06nlLSkuuu}\n}"
    },
    {
        "title": "Online Policy Optimization for Robust Markov Decision Process",
        "authorids": [
            "~Jing_Dong3",
            "~Jingwei_Li2",
            "~Baoxiang_Wang1",
            "~Jingzhao_Zhang2"
        ],
        "keywords": [
            "reinforcement learning"
        ],
        "abstract": "Reinforcement learning (RL) has exceeded human performance in many synthetic settings such as video games and Go. However, real-world deployment of end-to-end RL models is less common, as RL models can be very sensitive to perturbations in the environment. The robust Markov decision process (MDP) framework---in which the transition probabilities belong to an uncertainty set around a nominal model---provides one way to develop robust models. While previous analysis for robust MDP shows RL algorithms are effective assuming access to a generative model, it remains unclear whether RL can be efficient under a more realistic online setting, which requires a careful balance between exploration and exploitation. In this work, we consider online robust MDP by interacting with an unknown nominal system. We propose a robust optimistic policy optimization algorithm that is provably efficient. To address the additional uncertainty caused by an adversarial environment, our model features a new optimistic update rule derived via Fenchel conjugates. Our analysis establishes the first regret bound for online robust MDPs.",
        "_bibtex": "@inproceedings{\ndong2024online,\ntitle={Online Policy Optimization for Robust Markov Decision Process},\nauthor={Jing Dong and Jingwei Li and Baoxiang Wang and Jingzhao Zhang},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=VS8EPaCSY1}\n}"
    },
    {
        "title": "Hybrid CtrlFormer: Learning Adaptive Search Space Partition for Hybrid Action Control via Transformer-based Monte Carlo Tree Search",
        "authorids": [
            "~Jiashun_Liu1",
            "~Xiaotian_Hao1",
            "~Jianye_HAO1",
            "~YAN_ZHENG1",
            "~Yujing_Hu2",
            "~Changjie_Fan1",
            "~Tangjie_Lv1",
            "~Zhipeng_Hu1"
        ],
        "keywords": [
            "Deep Reinforcement Learning",
            "Hybrid Action Space Control",
            "Transformer",
            "Monte Carlo Tree Search"
        ],
        "abstract": "Hybrid action control tasks are common in the real world, which require controlling some discrete and continuous actions simultaneously. To solve these tasks, existing Deep Reinforcement learning (DRL) methods either directly build a separate policy for each type of action or simplify the hybrid action space into a discrete or continuous action control problem. However, these methods neglect the challenge of exploration resulting from the complexity of the hybrid action space. Thus, it is necessary to design more sample efficient algorithms. To this end, we propose a novel Hybrid Control Transformer (Hybrid CtrlFormer), to achieve better exploration and exploitation for the hybrid action control problems. The core idea is: 1) we construct a hybrid action space tree with the discrete actions at the higher level and the continuous parameter space at the lower level. Each parameter space is split into multiple subregions. 2) To simplify the exploration space, a Transformer-based Monte-Carlo tree search method is designed to efficiently evaluate and partition the hybrid action space into good and bad subregions along the tree. Our method achieves state-of-the-art performance and sample efficiency in a variety of environments with discrete-continuous action space.",
        "_bibtex": "@inproceedings{\nliu2024hybrid,\ntitle={Hybrid CtrlFormer: Learning Adaptive Search Space Partition for Hybrid Action Control via Transformer-based Monte Carlo Tree Search},\nauthor={Jiashun Liu and Xiaotian Hao and Jianye HAO and YAN ZHENG and Yujing Hu and Changjie Fan and Tangjie Lv and Zhipeng Hu},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=EURpNaioiP}\n}"
    },
    {
        "title": "Calibrated and Conformal Propensity Scores for Causal Effect Estimation",
        "authorids": [
            "~Shachi_Deshpande1",
            "~Volodymyr_Kuleshov1"
        ],
        "keywords": [
            "Causal Inference",
            "Conformal Prediction",
            "Propensity Scores",
            "Uncertainties",
            "Calibration",
            "Causal Machine Learning"
        ],
        "abstract": "Propensity scores are commonly used to balance observed covariates while estimating treatment effects. We argue that the probabilistic output of a learned propensity score model should be calibrated, i.e. a predictive treatment probability of 90% should correspond to 90% of individuals being assigned the treatment group. We propose simple recalibration techniques to ensure this property. We prove that calibration is a necessary condition for unbiased treatment effect estimation when using popular inverse propensity weighted and doubly robust estimators. We derive error bounds on causal effect estimates that directly relate to the quality of uncertainties provided by the probabilistic propensity score model and show that calibration strictly improves this error bound while also avoiding extreme propensity weights. We demonstrate improved causal effect estimation with calibrated propensity scores in several tasks including high-dimensional image covariates and genome-wide association studies (GWASs). Calibrated propensity scores improve the speed of GWAS analysis by more than two-fold by enabling the use of simpler models that are faster to train.",
        "_bibtex": "@inproceedings{\ndeshpande2024calibrated,\ntitle={Calibrated and Conformal Propensity Scores for Causal Effect Estimation},\nauthor={Shachi Deshpande and Volodymyr Kuleshov},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=xgs9VpvVAC}\n}"
    },
    {
        "title": "Differentiable Pareto-Smoothed Weighting for High-Dimensional Heterogeneous Treatment Effect Estimation",
        "authorids": [
            "~Yoichi_Chikahara1",
            "~Kansei_Ushiyama1"
        ],
        "keywords": [
            "treatment effect estimation",
            "Pareto smoothing"
        ],
        "abstract": "There is a growing interest in estimating heterogeneous treatment effects across individuals using their high-dimensional feature attributes. Achieving high performance in such high-dimensional heterogeneous treatment effect estimation is challenging because in this setup, it is usual that some features induce sample selection bias while others do not but are predictive of potential outcomes. To avoid losing such predictive feature information, existing methods learn separate feature representations using inverse probability weighting (IPW). However, due to their numerically unstable IPW weights, these methods suffer from estimation bias under a finite sample setup. To develop a numerically robust estimator by weighted representation learning, we propose a differentiable Pareto-smoothed weighting framework that replaces extreme weight values in an end-to-end fashion. Our experimental results show that by effectively correcting the weight values, our proposed method outperforms the existing ones, including traditional weighting schemes. Our code is available at [this https URL](https://github.com/ychika/DPSW).",
        "_bibtex": "@inproceedings{\nchikahara2024differentiable,\ntitle={Differentiable Pareto-Smoothed Weighting for High-Dimensional Heterogeneous Treatment Effect Estimation},\nauthor={Yoichi Chikahara and Kansei Ushiyama},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=o85WSGg0oB}\n}"
    },
    {
        "title": "Neighbor Similarity and Multimodal Alignment based Product Recommendation Study",
        "authorids": [
            "~Zhiqiang_Zhang10",
            "~Yongqiang_Jiang1",
            "~Qian_Gao3",
            "~Zhipeng_Wang5"
        ],
        "keywords": [
            "Neighbor similarity graph convolutional network; Multimodal alignment and fusion; User preference information enhancement; Multimodal recommendation"
        ],
        "abstract": "Existing multimodal recommendation research still faces some challenges, such as not being able to fully mine the implicit relevance information of neighbor nodes, and the unreasonable weight allocation to imbalanced nodes. To address the aforementioned challenges, this paper introduces a new multimodal recommendation model called NSMAR+. Specifically, the model firstly constructs a neighbor similarity graph convolutional network to capture the implicit relevance information and reasonably assigns the attention weights through the graph attention mechanism. Secondly, the model introduces a modal alignment and fusion mechanism by using a multilayer perceptron (MLP) to map image and text features into a shared space for comparison and fusion. In addition, the model constructs a user co-interaction graph and an item semantic graph based on the original information and performs graph convolution operations to enhance the preference information of users and items, and to better capture the interactions and internal features between users and items. Finally, MLP is employed to aggregate user and item representations and predict personalized recommendation rankings. To validate the experiment\u2019s efficacy, this paper compares with several leading multimodal recommendation models on public datasets with a performance improvement of 1% to 3%. The experimental outcomes indicate that the model in this paper has good superiority and accuracy in multimodal recommendation tasks.",
        "_bibtex": "@inproceedings{\nzhang2024neighbor,\ntitle={Neighbor Similarity and Multimodal Alignment based Product Recommendation Study},\nauthor={Zhiqiang Zhang and Yongqiang Jiang and Qian Gao and Zhipeng Wang},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=1jV9k56mme}\n}"
    },
    {
        "title": "BanditQ:Fair Bandits with Guaranteed Rewards",
        "authorids": [
            "~Abhishek_Sinha3"
        ],
        "keywords": [
            "Bandits",
            "fairness",
            "regret bounds"
        ],
        "abstract": "Classic no-regret multi-armed bandit algorithms, including the Upper Confidence Bound (UCB), Hedge, and EXP3, are inherently unfair by design. Their unfairness stems from their objective of playing the most rewarding arm as frequently as possible while ignoring the rest. In this paper, we consider a fair prediction problem in the stochastic setting with a guaranteed minimum rate of accrual of rewards for each arm. We study the problem in both full-information and bandit feedback settings. Combining queueing-theoretic techniques with adversarial bandits, we propose a new online policy called BanditQ that achieves the target reward rates while conceding a regret and target rate violation penalty of at most $O(T^{\\frac{3}{4}}).$ The regret bound in the full-information setting can be further improved to $O(\\sqrt{T})$ under either a monotonicity assumption or when considering time-averaged regret. The proposed policy is efficient and admits a black-box reduction from the fair prediction problem to the standard adversarial MAB problem. The analysis of the BanditQ policy involves a new self-bounding inequality, which might be of independent interest.",
        "_bibtex": "@inproceedings{\nsinha2024banditqfair,\ntitle={BanditQ:Fair Bandits with Guaranteed Rewards},\nauthor={Abhishek Sinha},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=ISp44jGVQX}\n}"
    },
    {
        "title": "Learning from Crowds with Dual-View K-Nearest Neighbor",
        "authorids": [
            "~Jiao_Li4",
            "~Liangxiao_Jiang1",
            "~xuewu1",
            "~Wenjun_Zhang4"
        ],
        "keywords": [
            "Crowdsourcing",
            "Label integration",
            "K-Nearest Neighbor",
            "Multi-view learning"
        ],
        "abstract": "In crowdsourcing scenarios, we can obtain multiple noisy labels from different crowd workers for each instance and then infer its integrated label via label integration. To achieve better performance, some recently published label integration methods have attempted to exploit the multiple noisy labels of inferred instances\u2019 nearest neighbors via the K-nearest neighbor (KNN) algorithm. However, the used KNN algorithm searches inferred instances\u2019 nearest neighbors only relying on the defined distance functions in the original attribute view and totally ignoring the valuable information hidden in the multiple noisy labels, which limits their performance. Motivated by multi-view learning, we define the multiple noisy labels as another label view of instances and propose to search inferred instances\u2019 nearest neighbors using the joint information from both the original attribute view and the multiple noisy label view. To this end, we propose a novel label integration method called dual-view K-nearest neighbor (DVKNN). In DVKNN, we first define a new distance function to search the K-nearest neighbors of an inferred instance. Then, we define a fine-grained weight for each noisy label from each neighbor. Finally, we perform weighted majority voting (WMV) on all these noisy labels to obtain the integrated label of the inferred instance. Extensive experiments validate the effectiveness and rationality of DVKNN.",
        "_bibtex": "@inproceedings{\nli2024learning,\ntitle={Learning from Crowds with Dual-View K-Nearest Neighbor},\nauthor={Jiao Li and Liangxiao Jiang and xuewu and Wenjun Zhang},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=481kS0WCjP}\n}"
    },
    {
        "title": "Fast Reliability Estimation for Neural Networks with Adversarial Attack-Driven Importance Sampling",
        "authorids": [
            "~Karim_TIT1",
            "~Teddy_Furon1"
        ],
        "keywords": [
            "Deep Neural Networks",
            "Reliability",
            "Adversarial Attacks",
            "Importance Sampling"
        ],
        "abstract": "This paper introduces a novel approach to evaluate the reliability of Neural Networks (NNs) by integrating adversarial attacks with Importance Sampling (IS), enhancing the assessment's precision and efficiency. Leveraging adversarial attacks to guide IS, our method efficiently identifies vulnerable input regions, offering a more directed alternative to traditional Monte Carlo methods. While comparing our approach with classical reliability techniques like FORM and SORM, and with classical rare event simulation methods such as Cross-Entropy IS, we acknowledge its reliance on the effectiveness of adversarial attacks and its inability to handle very high-dimensional data such as ImageNet. Despite these challenges, our comprehensive empirical validations on the datasets the MNIST and CIFAR10 demonstrate the method's capability to accurately estimate NN reliability for a variety of models. Our research not only presents an innovative strategy for reliability assessment in NNs but also sets the stage for further work exploiting the connection between adversarial robustness and the field of statistical reliability engineering.",
        "_bibtex": "@inproceedings{\ntit2024fast,\ntitle={Fast Reliability Estimation for Neural Networks with Adversarial Attack-Driven Importance Sampling},\nauthor={Karim TIT and Teddy Furon},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=0usGhloD12}\n}"
    },
    {
        "title": "SMuCo: Reinforcement Learning for Visual Control via Sequential Multi-view Total Correlation",
        "authorids": [
            "~Tong_Cheng2",
            "~Hang_Dong3",
            "~Lu_Wang11",
            "~Bo_Qiao1",
            "~Qingwei_Lin1",
            "~Saravan_Rajmohan2",
            "~Thomas_Moscibroda1"
        ],
        "keywords": [
            "reinforcement learning",
            "visual control",
            "multi-view total correlation"
        ],
        "abstract": "The advent of abundant image data has catalyzed the advancement of visual control in reinforcement learning (RL) systems, leveraging multiple view- points to capture the same physical states, which could enhance control performance theoretically. However, integrating multi-view data into representation learning remains challenging. In this paper, we introduce SMuCo, an innovative multi-view reinforcement learning algorithm that constructs robust latent representations by optimizing multi- view sequential total correlation. This technique effectively captures task-relevant information and temporal dynamics while filtering out irrelevant data. Our method supports an unlimited number of views and demonstrates superior performance over leading model-free and model-based RL algorithms. Empirical results from the DeepMind Control Suite and the Sapien Basic Manipulation Task confirm SMuCo\u2019s enhanced efficacy, significantly improving task performance across diverse scenarios and views.",
        "_bibtex": "@inproceedings{\ncheng2024smuco,\ntitle={{SM}uCo: Reinforcement Learning for Visual Control via Sequential Multi-view Total Correlation},\nauthor={Tong Cheng and Hang Dong and Lu Wang and Bo Qiao and Qingwei Lin and Saravan Rajmohan and Thomas Moscibroda},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=Eg5AmHPQKe}\n}"
    },
    {
        "title": "Partial Identification with Proxy of Latent Confoundings via Sum-of-ratios Fractional Programming",
        "authorids": [
            "~Zhiheng_Zhang1",
            "~Xinyan_Su1"
        ],
        "keywords": [
            "causal effect;"
        ],
        "abstract": "Causal effect estimation is a crucial theoretical tool\nin uncertainty analysis. The challenge of unobservable confoundings has raised concerns regarding\nquantitative causality computation. To address this\nissue, proxy control has become popular, employing auxiliary variables W as proxies for the confounding variables U. However, proximal methods\nrely on strong assumptions, such as reversibility\nand completeness, that are challenging to interpret\nempirically and verify. Consequently, their applicability in real-world scenarios is limited, particularly when the proxies lack informativeness. In\nour paper, we have developed a novel optimization\nmethod named Partial Identification with Proxy of\nLatent Confoundings via Sum-of-Ratios Fractional\nProgramming (PI-SFP). This method does not impose any additional restrictions upon proxies and\nonly assumes the mild partial observability of the\ntransition matrix P(W | U). We have theoretically proven the global convergence of PI-SFP to\nthe valid bound of the causal effect and analyzed\nthe conditions under which the bounds could be\ntight. Our synthetic and real-world experiments\nvalidate our theoretical framework.",
        "_bibtex": "@inproceedings{\nzhang2024partial,\ntitle={Partial Identification with Proxy of Latent Confoundings via Sum-of-ratios Fractional Programming},\nauthor={Zhiheng Zhang and Xinyan Su},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=SSVaDQPqVA}\n}"
    },
    {
        "title": "Trusted re-weighting for label distribution learning",
        "authorids": [
            "~Zhuoran_Zheng1",
            "~Chen_Wu8",
            "~YEYING_JIN1",
            "~Xiuyi_Jia1"
        ],
        "keywords": [
            "label distribution learning"
        ],
        "abstract": "Label distribution learning (LDL) is a novel machine learning paradigm that aims to shift 0/1 labels into descriptive degrees to characterize the polysemy of instances. Since the description degree takes a value between 0 \u223c 1, it is difficult for the annotator to accurately annotate each label. Therefore, the predictive ability of numerous LDL algorithms may be degraded by the presence of noise in the label space. To address this problem, we propose a novel stability-trust LDL framework that aims to reconstruct the feature space of an arbitrary LDL dataset by using feature decoupling and prototype guidance. Specifically, first, we use prototype learning to select reliable cluster centers (representative vectors of label distributions) to filter out a set of clean samples (with labeled noise) on the original dataset. Then, we decouple the feature space (eliminating correlations among features) by modeling a weight assigner that is learned on this clean sample set, thus assigning weights to each sample of the original dataset. Finally, all existing LDL algorithms can be trained on this new re-weighted dataset for the goal of robust modeling. In addition, we create a new image dataset to support the training and testing of compared models. Experimental results demonstrate that the proposed framework boosts the performance of the LDL algorithm on datasets with label noise.",
        "_bibtex": "@inproceedings{\nzheng2024trusted,\ntitle={Trusted re-weighting for label distribution learning},\nauthor={Zhuoran Zheng and Chen Wu and YEYING JIN and Xiuyi Jia},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=TAgFAu9rSz}\n}"
    },
    {
        "title": "Low-rank Matrix Bandits with Heavy-tailed Rewards",
        "authorids": [
            "~Yue_Kang1",
            "~Cho-Jui_Hsieh1",
            "~Thomas_Lee2"
        ],
        "keywords": [
            "contextual bandit"
        ],
        "abstract": "In stochastic low-rank matrix bandit, the expected reward of an arm is equal to the inner product between its feature matrix and some unknown $d_1$ by $d_2$ low-rank parameter matrix $\\Theta^*$ with rank $r \\ll d_1\\wedge d_2$. While all prior studies assume the payoffs are mixed with sub-Gaussian noises, in this work we loosen this strict assumption and consider the new problem of low-rank matrix bandit with heavy-tailed rewards (LowHTR), where the rewards only have finite $(1+\\delta)$ moment for some $\\delta \\in (0,1]$. By utilizing the truncation on observed payoffs and the dynamic exploration, we propose a novel algorithm called LOTUS attaining the regret bound of order $\\tilde O(d^\\frac{3}{2}r^\\frac{1}{2}T^\\frac{1}{1+\\delta}/\\tilde{D}_{rr})$ without knowing $T$, which matches the state-of-the-art regret bound under sub-Gaussian noises~\\citep{lu2021low,kang2022efficient} with $\\delta = 1$. Moreover, we establish a lower bound of the order $\\Omega(d^\\frac{\\delta}{1+\\delta} r^\\frac{\\delta}{1+\\delta} T^\\frac{1}{1+\\delta}) = \\Omega(T^\\frac{1}{1+\\delta})$ for LowHTR, which indicates our LOTUS is nearly optimal in the order of $T$. In addition, we improve LOTUS so that it does not require knowledge of the rank $r$ with $\\tilde O(dr^\\frac{3}{2}T^\\frac{1+\\delta}{1+2\\delta})$ regret bound, and it is efficient under the high-dimensional scenario. We also conduct simulations to demonstrate the practical superiority of our algorithm.",
        "_bibtex": "@inproceedings{\nkang2024lowrank,\ntitle={Low-rank Matrix Bandits with Heavy-tailed Rewards},\nauthor={Yue Kang and Cho-Jui Hsieh and Thomas Lee},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=TG4fKjfvxC}\n}"
    },
    {
        "title": "Decentralized Online Learning in General-Sum Stackelberg Games",
        "authorids": [
            "~Yaolong_Yu1",
            "~Haipeng_Chen1"
        ],
        "keywords": [
            "Stackelberg games; Bandits; Online learning;"
        ],
        "abstract": "We study an online learning problem in general-sum Stackelberg games, where players act in a decentralized and strategic manner. We study two settings depending on the type of information for the follower: (1) the $\\textit{limited information}$ setting where the follower only observes its own reward, and (2) the $\\textit{side information}$ setting where the follower has extra side information about the leader's reward. We show that for the follower, myopically best responding to the leader's action is the best strategy for the limited information setting, but not necessarily so for the side information setting -- the follower can manipulate the leader's reward signals with strategic actions, and hence induce the leader's strategy to converge to an equilibrium that is better off for itself. Based on these insights, we study decentralized online learning for both players in the two settings. Our main contribution is to derive $\\textit{last iterate}$ convergence and sample complexity results in both settings. Notably, we design a new manipulation strategy for the follower in the latter setting, and show that it has an intrinsic advantage against the best response strategy. Our theories are also supported by empirical results.",
        "_bibtex": "@inproceedings{\nyu2024decentralized,\ntitle={Decentralized Online Learning in General-Sum Stackelberg Games},\nauthor={Yaolong Yu and Haipeng Chen},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=GsFG0ImriA}\n}"
    },
    {
        "title": "Consistency Regularization for Domain Generalization with Logit Attribution Matching",
        "authorids": [
            "~Han_Gao5",
            "~Kaican_Li1",
            "~Weiyan_Xie1",
            "~Zhi_LIN1",
            "~Yongxiang_Huang1",
            "~Luning_Wang1",
            "~Caleb_Chen_Cao1",
            "~Nevin_L._Zhang1"
        ],
        "keywords": [
            "domain generalization",
            "consistency regularization",
            "causality"
        ],
        "abstract": "Domain generalization (DG) is about training models that generalize well under domain shift. Previous research on DG has been conducted mostly in single-source or multi-source settings. In this paper, we consider a third lesser-known setting where a training domain is endowed with a collection of pairs of examples that share the same semantic information. Such semantic sharing (SS) pairs can be created via data augmentation and then utilized for consistency regularization (CR). We present a theory showing CR is conducive to DG and propose a novel CR method called Logit Attribution Matching (LAM). We conduct experiments on five DG benchmarks and four pretrained models with SS pairs created by both generic and targeted data augmentation methods. LAM outperforms representative single/multi-source DG methods and various CR methods that leverage SS pairs. The code and data of this project are available at https://github.com/Gaohan123/LAM.",
        "_bibtex": "@inproceedings{\ngao2024consistency,\ntitle={Consistency Regularization for Domain Generalization with Logit Attribution Matching},\nauthor={Han Gao and Kaican Li and Weiyan Xie and Zhi LIN and Yongxiang Huang and Luning Wang and Caleb Chen Cao and Nevin L. Zhang},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=WNy1ooHYHx}\n}"
    },
    {
        "title": "DistriBlock: Identifying adversarial audio samples by leveraging characteristics of the output distribution",
        "authorids": [
            "~Matias_Patricio_Pizarro_Bustamante1",
            "~Dorothea_Kolossa1",
            "~Asja_Fischer1"
        ],
        "keywords": [
            "Audio adversarial examples",
            "ASR",
            "Machine Learning",
            "Uncertainty"
        ],
        "abstract": "Adversarial attacks can mislead automatic speech recognition (ASR) systems into predicting an arbitrary target text, thus posing a clear security threat.\nTo prevent such attacks, we propose DistriBlock, an efficient detection strategy applicable to any ASR system that predicts a probability distribution over output tokens in each time step.\nWe measure a set of characteristAdversarial attacks can mislead automatic speech recognition (ASR) systems into predicting an arbitrary target text, thus posing a clear security threat.\nTo prevent such attacks, we propose DistriBlock, an efficient detection strategy applicable to any ASR system that predicts a probability distribution over output tokens in each time step.\nWe measure a set of characteristics of this distribution: the median, maximum, and minimum over the output probabilities, the entropy of the distribution, as well as the Kullback-Leibler and the Jensen-Shannon divergence with respect to the distributions of the subsequent time step. \nThen, by leveraging the characteristics observed for both benign and adversarial data, we apply binary classifiers, including simple threshold-based classification, ensembles of such classifiers, and neural networks. \nThrough extensive analysis across different state-of-the-art ASR systems and language data sets, we demonstrate the supreme performance of this approach, with a mean area under the receiver operating characteristic curve for distinguishing target adversarial examples against clean and noisy data of 99\\% and 97\\%, respectively. \nTo assess the robustness of our method, we show that adaptive adversarial examples that can circumvent DistriBlock are much noisier, which makes them easier to detect through filtering and creates another avenue for preserving the system's robustness.ics of this distribution: the median, maximum, and minimum over the output probabilities, the entropy of the distribution, as well as the Kullback-Leibler and the Jensen-Shannon divergence with respect to the distributions of the subsequent time step. \nThen, by leveraging the characteristics observed for both benign and adversarial data, we apply binary classifiers, including simple threshold-based classification, ensembles of such classifiers, and neural networks. \nThrough extensive analysis across different state-of-the-art ASR systems and language data sets, we demonstrate the supreme performance of this approach, with a mean area under the receiver operating characteristic for distinguishing target adversarial examples against clean and noisy data of 99\\% and 97\\%, respectively. \nTo assess the robustness of our method, we show that adaptive adversarial examples that can circumvent DistriBlock are much noisier, which makes them easier to detect through filtering and creates another avenue for preserving the system's robustness.",
        "_bibtex": "@inproceedings{\nbustamante2024distriblock,\ntitle={DistriBlock: Identifying adversarial audio samples by leveraging characteristics of the output distribution},\nauthor={Matias Patricio Pizarro Bustamante and Dorothea Kolossa and Asja Fischer},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=atIzXpLlKA}\n}"
    },
    {
        "title": "RE-SORT: Removing Spurious Correlation in Multilevel Interaction for CTR Prediction",
        "authorids": [
            "~Songli_Wu1",
            "~Liang_Du4",
            "~Jia-Qi_Yang1",
            "~Yuai_Wang1",
            "~De-Chuan_Zhan1",
            "~SHUANG_ZHAO2",
            "~Zixun_Sun1"
        ],
        "keywords": [
            "click through rate",
            "recommendation system",
            "spurious correlation",
            "stacked recurrent network"
        ],
        "abstract": "Click-through rate (CTR) prediction is a critical task in recommendation systems, serving as the ultimate filtering step to sort items for a user. Most recent cutting-edge methods primarily focus on investigating complex implicit and explicit feature interactions; however, these methods neglect the spurious correlation issue caused by confounding factors, thereby diminishing the model's generalization ability. We propose a CTR prediction framework that REmoves Spurious cORrelations in mulTilevel feature interactions, termed RE-SORT, which has two key components. I. A multilevel stacked recurrent (MSR) structure enables the model to efficiently capture diverse nonlinear interactions from feature spaces at different levels. II. A spurious correlation elimination (SCE) module further leverages Laplacian kernel mapping and sample reweighting methods to eliminate the spurious correlations concealed within the multilevel features, allowing the model to focus on the true causal features. Extensive experiments conducted on four challenging CTR datasets, our production dataset, and an online A/B test demonstrate that the proposed method achieves state-of-the-art performance in both accuracy and speed. The utilized codes, models, and dataset will be released at https://github.com/RE-SORT.",
        "_bibtex": "@inproceedings{\nwu2024resort,\ntitle={{RE}-{SORT}: Removing Spurious Correlation in Multilevel Interaction for {CTR} Prediction},\nauthor={Songli Wu and Liang Du and Jia-Qi Yang and Yuai Wang and De-Chuan Zhan and SHUANG ZHAO and Zixun Sun},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=5UpB1mboRB}\n}"
    },
    {
        "title": "Learning Topological Representations with Bidirectional Graph Attention Network for Solving Job Shop Scheduling Problem",
        "authorids": [
            "~Cong_Zhang3",
            "~Zhiguang_Cao1",
            "~Yaoxin_Wu2",
            "~Wen_Song1",
            "~Jing_Sun6"
        ],
        "keywords": [
            "Data-driven optimization",
            "deep reinforcement learning",
            "job shop scheduling",
            "graph neural network",
            "neural heuristics"
        ],
        "abstract": "Existing learning-based methods for solving job shop scheduling problems (JSSP) usually use off-the-shelf GNN models tailored to undirected graphs and neglect the rich and meaningful topological structures of disjunctive graphs (DGs). This paper proposes the topology-aware bidirectional graph attention network (TBGAT), a novel GNN architecture based on the attention mechanism, to embed the DG for solving JSSP in a local search framework. Specifically, TBGAT embeds the DG from a forward and a backward view, respectively, where the messages are propagated by following the different topologies of the views and aggregated via graph attention. Then, we propose a novel operator based on the message-passing mechanism to calculate the forward and backward topological sorts of the DG, which are the features for characterizing the topological structures and exploited by our model. In addition, we theoretically and experimentally show that TBGAT has linear computational complexity to the number of jobs and machines, respectively, strengthening our method's practical value. Besides, extensive experiments on five synthetic datasets and seven classic benchmarks show that TBGAT achieves new SOTA results by outperforming a wide range of neural methods by a large margin. All the code and data are publicly available online at https://github.com/zcaicaros/TBGAT.",
        "_bibtex": "@inproceedings{\nzhang2024learning,\ntitle={Learning Topological Representations with Bidirectional Graph Attention Network for Solving Job Shop Scheduling Problem},\nauthor={Cong Zhang and Zhiguang Cao and Yaoxin Wu and Wen Song and Jing Sun},\nbooktitle={The 40th Conference on Uncertainty in Artificial Intelligence},\nyear={2024},\nurl={https://openreview.net/forum?id=DJ44RJa9Gf}\n}"
    }
]